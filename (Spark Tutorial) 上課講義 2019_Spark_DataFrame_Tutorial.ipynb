{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "2019 Spark DataFrame Tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "9FisGymJZ2BM",
        "ZQTLug-b4zwB",
        "mkGg9xJWZ2Bb",
        "xIOULjFJZ2Be",
        "CEcSbB_ZZ2BZ",
        "ycnCvcAVZ2Bi",
        "pMpQ-zrB11pp",
        "0WbM7XshZ2B2"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UDICatNCHU/SparkTutorial/blob/master/(Spark%20Tutorial)%20%E4%B8%8A%E8%AA%B2%E8%AC%9B%E7%BE%A9%202019_Spark_DataFrame_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFD3Nh5xZ2A-",
        "colab_type": "code",
        "outputId": "1461dcee-d647-4390-9a57-0ffbe93930ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "! wget -O init_env.sh https://www.dropbox.com/s/6bnwn8u2hz19s59/init_env.sh && \\\n",
        "bash init_env.sh"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-04 01:54:42--  https://www.dropbox.com/s/6bnwn8u2hz19s59/init_env.sh\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.9.1, 2620:100:601f:1::a27d:901\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.9.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/6bnwn8u2hz19s59/init_env.sh [following]\n",
            "--2019-10-04 01:54:47--  https://www.dropbox.com/s/raw/6bnwn8u2hz19s59/init_env.sh\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc0e203ed8e0ece45f30bbbc0839.dl.dropboxusercontent.com/cd/0/inline/ApxJa_V1NGILOgTGAFMDZWO9IMbyUf4gQu28Ku_RGBSFOSfOTa5mQ1E9X49hFwFCClNO4ivxa-9O43xEvcupjd0P5w8M0s0Lg2Rz1lwfxQUmVg/file# [following]\n",
            "--2019-10-04 01:54:47--  https://uc0e203ed8e0ece45f30bbbc0839.dl.dropboxusercontent.com/cd/0/inline/ApxJa_V1NGILOgTGAFMDZWO9IMbyUf4gQu28Ku_RGBSFOSfOTa5mQ1E9X49hFwFCClNO4ivxa-9O43xEvcupjd0P5w8M0s0Lg2Rz1lwfxQUmVg/file\n",
            "Resolving uc0e203ed8e0ece45f30bbbc0839.dl.dropboxusercontent.com (uc0e203ed8e0ece45f30bbbc0839.dl.dropboxusercontent.com)... 162.125.9.6, 2620:100:601f:6::a27d:906\n",
            "Connecting to uc0e203ed8e0ece45f30bbbc0839.dl.dropboxusercontent.com (uc0e203ed8e0ece45f30bbbc0839.dl.dropboxusercontent.com)|162.125.9.6|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 336 [text/plain]\n",
            "Saving to: ‘init_env.sh’\n",
            "\n",
            "init_env.sh         100%[===================>]     336  --.-KB/s    in 0s      \n",
            "\n",
            "2019-10-04 01:54:48 (60.0 MB/s) - ‘init_env.sh’ saved [336/336]\n",
            "\n",
            "--2019-10-04 01:54:48--  https://d3kbcqa49mib13.cloudfront.net/spark-2.2.0-bin-hadoop2.7.tgz\n",
            "Resolving d3kbcqa49mib13.cloudfront.net (d3kbcqa49mib13.cloudfront.net)... 52.85.107.145, 52.85.107.163, 52.85.107.17, ...\n",
            "Connecting to d3kbcqa49mib13.cloudfront.net (d3kbcqa49mib13.cloudfront.net)|52.85.107.145|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 203728858 (194M) [application/x-tar]\n",
            "Saving to: ‘spark-2.2.0-bin-hadoop2.7.tgz’\n",
            "\n",
            "spark-2.2.0-bin-had 100%[===================>] 194.29M  86.6MB/s    in 2.2s    \n",
            "\n",
            "2019-10-04 01:54:50 (86.6 MB/s) - ‘spark-2.2.0-bin-hadoop2.7.tgz’ saved [203728858/203728858]\n",
            "\n",
            "spark-2.2.0-bin-hadoop2.7/\n",
            "spark-2.2.0-bin-hadoop2.7/NOTICE\n",
            "spark-2.2.0-bin-hadoop2.7/jars/\n",
            "spark-2.2.0-bin-hadoop2.7/jars/parquet-common-1.8.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/bonecp-0.8.0.RELEASE.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-net-2.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/javax.servlet-api-3.1.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-annotations-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-hdfs-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/oro-2.0.8.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/xercesImpl-2.9.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/antlr-runtime-3.4.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/machinist_2.11-0.6.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spire_2.11-0.13.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/parquet-hadoop-1.8.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-sketch_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/stream-2.7.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/kryo-shaded-3.0.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/metrics-jvm-3.1.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-mapreduce-client-common-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/RoaringBitmap-0.5.11.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-auth-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-common-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/activation-1.1.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jta-1.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/datanucleus-core-3.2.10.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jersey-container-servlet-2.22.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jersey-guava-2.22.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jets3t-0.9.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jetty-util-6.1.26.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-compress-1.4.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jersey-container-servlet-core-2.22.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-beanutils-1.7.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hk2-utils-2.4.0-b34.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/parquet-format-2.3.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/avro-1.7.7.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/datanucleus-api-jdo-3.2.6.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jline-2.12.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/metrics-core-3.1.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/java-xmlbuilder-1.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/stax-api-1.0-2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hk2-locator-2.4.0-b34.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/parquet-hadoop-bundle-1.6.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jsp-api-2.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/xmlenc-0.52.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/xbean-asm5-shaded-4.4.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jackson-core-asl-1.9.13.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/shapeless_2.11-2.3.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-collections-3.2.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/javax.inject-1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-sql_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/json4s-jackson_2.11-3.2.11.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/json4s-ast_2.11-3.2.11.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-codec-1.10.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/leveldbjni-all-1.8.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-httpclient-3.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/aopalliance-repackaged-2.4.0-b34.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-yarn-server-common-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/minlog-1.3.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/javolution-5.5.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/datanucleus-rdbms-3.2.9.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jersey-common-2.22.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-graphx_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/api-asn1-api-1.0.0-M20.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/apacheds-i18n-2.0.0-M15.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/validation-api-1.1.0.Final.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-dbcp-1.4.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/slf4j-log4j12-1.7.16.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-pool-1.5.4.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-network-common_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/pmml-schema-1.2.15.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-mapreduce-client-shuffle-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/joda-time-2.9.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-hive-thriftserver_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spire-macros_2.11-0.13.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/curator-recipes-2.6.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jersey-server-2.22.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/htrace-core-3.1.0-incubating.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/httpclient-4.5.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-mllib-local_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/snappy-0.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/breeze-macros_2.11-0.13.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jackson-jaxrs-1.9.13.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/eigenbase-properties-1.1.5.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-tags_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jackson-databind-2.6.5.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/curator-client-2.6.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-catalyst_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/paranamer-2.6.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/opencsv-2.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/json4s-core_2.11-3.2.11.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hive-metastore-1.2.1.spark2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-digester-1.8.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jsr305-1.3.9.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-repl_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jetty-6.1.26.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/pyrolite-4.13.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/log4j-1.2.17.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-yarn-server-web-proxy-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/calcite-avatica-1.2.0-incubating.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/scala-xml_2.11-1.0.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-network-shuffle_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/calcite-linq4j-1.2.0-incubating.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/compress-lzf-1.0.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jtransforms-2.4.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/apache-log4j-extras-1.2.17.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jcl-over-slf4j-1.7.16.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/guice-3.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/gson-2.2.4.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/httpcore-4.4.4.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/protobuf-java-2.5.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-mapreduce-client-core-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-yarn_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-hive_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-lang-2.6.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/stax-api-1.0.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/javax.annotation-api-1.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/netty-all-4.0.43.Final.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/curator-framework-2.6.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-io-2.4.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/avro-ipc-1.7.7.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hive-beeline-1.2.1.spark2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/mesos-1.0.0-shaded-protobuf.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jackson-annotations-2.6.5.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jersey-media-jaxb-2.22.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-yarn-common-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jackson-mapper-asl-1.9.13.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/parquet-jackson-1.8.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-client-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jackson-module-paranamer-2.6.5.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/aopalliance-1.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/super-csv-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/janino-3.0.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/antlr4-runtime-4.5.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jpam-1.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-lang3-3.5.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/javassist-3.18.1-GA.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/bcprov-jdk15on-1.51.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-launcher_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/javax.ws.rs-api-2.0.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-mapreduce-client-jobclient-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/metrics-graphite-3.1.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jackson-xc-1.9.13.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/lz4-1.3.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/core-1.1.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-mesos_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/antlr-2.7.7.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-yarn-client-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/mx4j-3.0.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-logging-1.1.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-beanutils-core-1.8.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/libfb303-0.9.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/libthrift-0.9.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jaxb-api-2.2.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hive-exec-1.2.1.spark2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/calcite-core-1.2.0-incubating.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/parquet-encoding-1.8.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-mllib_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/api-util-1.0.0-M20.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/apacheds-kerberos-codec-2.0.0-M15.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/mail-1.4.7.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/stringtemplate-3.2.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/guice-servlet-3.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-crypto-1.0.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/JavaEWAH-0.3.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/metrics-json-3.1.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jdo-api-3.0.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/scalap-2.11.8.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hive-cli-1.2.1.spark2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/zookeeper-3.4.6.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jackson-module-scala_2.11-2.6.5.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/ivy-2.4.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/py4j-0.10.4.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/arpack_combined_all-0.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-unsafe_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/macro-compat_2.11-1.1.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jul-to-slf4j-1.7.16.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/pmml-model-1.2.15.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/parquet-column-1.8.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/javax.inject-2.4.0-b34.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/breeze_2.11-0.13.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-cli-1.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/chill-java-0.8.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/avro-mapred-1.7.7-hadoop2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/snappy-java-1.1.2.6.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/base64-2.3.8.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-compiler-3.0.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/slf4j-api-1.7.16.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/derby-10.12.1.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-mapreduce-client-app-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/objenesis-2.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jodd-core-3.5.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jersey-client-2.22.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/ST4-4.0.4.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/univocity-parsers-2.2.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/scala-parser-combinators_2.11-1.0.4.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jackson-core-2.6.5.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-math3-3.4.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/xz-1.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/scala-reflect-2.11.8.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-configuration-1.6.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/scala-compiler-2.11.8.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-yarn-api-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hk2-api-2.4.0-b34.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-core_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/guava-14.0.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/netty-3.9.9.Final.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-streaming_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/chill_2.11-0.8.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/scala-library-2.11.8.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/osgi-resource-locator-1.0.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/python/\n",
            "spark-2.2.0-bin-hadoop2.7/python/run-tests.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/hello/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/hello/sub_hello/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/hello/sub_hello/sub_hello.txt\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/hello/hello.txt\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/userlibrary.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/userlib-0.1.zip\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/people.json\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/people1.json\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/c=1/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/c=1/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/c=1/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/_SUCCESS\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/c=0/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/c=0/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/c=0/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/streaming/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/streaming/text-test.txt\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/text-test.txt\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/people_array.json\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/.part-r-00008.gz.parquet.crc\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/part-r-00008.gz.parquet\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/_SUCCESS\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/part-r-00005.gz.parquet\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/.part-r-00005.gz.parquet.crc\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/part-r-00002.gz.parquet\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/.part-r-00002.gz.parquet.crc\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/part-r-00004.gz.parquet\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/.part-r-00004.gz.parquet.crc\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/.part-r-00007.gz.parquet.crc\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/part-r-00007.gz.parquet\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/_metadata\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/_common_metadata\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/ages_newlines.csv\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/ages.csv\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/SimpleHTTPServer.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pylintrc\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/pyspark.ml.rst\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/pyspark.streaming.rst\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/conf.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/_templates/\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/_templates/layout.html\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/pyspark.rst\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/make.bat\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/epytext.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/make2.bat\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/index.rst\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/_static/\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/_static/pyspark.js\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/_static/pyspark.css\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/pyspark.sql.rst\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/pyspark.mllib.rst\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/Makefile\n",
            "spark-2.2.0-bin-hadoop2.7/python/.gitignore\n",
            "spark-2.2.0-bin-hadoop2.7/python/MANIFEST.in\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/status.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/version.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/conf.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/base.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/evaluation.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/util.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/classification.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/regression.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/tests.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/tuning.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/common.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/stat.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/linalg/\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/linalg/__init__.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/pipeline.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/feature.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/clustering.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/recommendation.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/__init__.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/wrapper.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/param/\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/param/_shared_params_code_gen.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/param/shared.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/param/__init__.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/fpm.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/statcounter.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/profiler.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/serializers.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/traceback_utils.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/shell.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/conf.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/session.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/window.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/tests.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/utils.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/group.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/types.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/catalog.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/context.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/dataframe.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/column.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/streaming.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/__init__.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/readwriter.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/functions.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/taskcontext.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/util.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/python/\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/python/pyspark/\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/python/pyspark/shell.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/daemon.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/tests.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/resultiterable.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/heapq3.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/broadcast.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/shuffle.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/cloudpickle.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/accumulators.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/java_gateway.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/util.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/listener.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/tests.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/flume.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/kafka.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/kinesis.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/dstream.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/context.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/__init__.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/context.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/storagelevel.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/__init__.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/join.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/tree.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/evaluation.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/util.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/classification.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/regression.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/tests.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/common.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/linalg/\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/linalg/__init__.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/linalg/distributed.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/feature.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/clustering.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/recommendation.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/stat/\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/stat/__init__.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/stat/_statistics.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/stat/KernelDensity.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/stat/test.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/stat/distribution.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/random.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/__init__.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/fpm.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/rdd.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/rddsampler.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/worker.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/files.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/find_spark_home.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/setup.cfg\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark.egg-info/\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark.egg-info/SOURCES.txt\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark.egg-info/requires.txt\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark.egg-info/PKG-INFO\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark.egg-info/dependency_links.txt\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark.egg-info/top_level.txt\n",
            "spark-2.2.0-bin-hadoop2.7/python/run-tests\n",
            "spark-2.2.0-bin-hadoop2.7/python/dist/\n",
            "spark-2.2.0-bin-hadoop2.7/python/setup.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/lib/\n",
            "spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip\n",
            "spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip\n",
            "spark-2.2.0-bin-hadoop2.7/python/lib/PY4J_LICENSE.txt\n",
            "spark-2.2.0-bin-hadoop2.7/python/README.md\n",
            "spark-2.2.0-bin-hadoop2.7/RELEASE\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/start-mesos-shuffle-service.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/start-mesos-dispatcher.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/spark-daemon.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/stop-slaves.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/stop-thriftserver.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/stop-shuffle-service.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/stop-history-server.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/spark-config.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/start-history-server.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/start-thriftserver.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/start-shuffle-service.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/spark-daemons.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/start-all.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/stop-master.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/stop-mesos-dispatcher.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/stop-slave.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/start-slave.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/stop-mesos-shuffle-service.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/start-slaves.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/stop-all.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/slaves.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/start-master.sh\n",
            "spark-2.2.0-bin-hadoop2.7/examples/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/RSparkSQLExample.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/fpm.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/gbt.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/isoreg.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/randomForest.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/als.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/kstest.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/gaussianMixture.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/ml.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/survreg.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/glm.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/kmeans.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/naiveBayes.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/svmLinear.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/lda.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/bisectingKmeans.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/mlp.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/logit.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/data-manipulation.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/dataframe.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/streaming/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/streaming/structured_network_wordcount.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/status_api_demo.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/linearsvc.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/random_forest_regressor_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/stopwords_remover_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/min_hash_lsh_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/standard_scaler_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/multiclass_logistic_regression_with_elastic_net.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/fpgrowth_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/vector_assembler_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/max_abs_scaler_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/binarizer_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/imputer_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/bisecting_k_means_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/cross_validator.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/logistic_regression_summary_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/dataframe_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/naive_bayes_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/generalized_linear_regression_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/decision_tree_regression_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/count_vectorizer_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/one_vs_rest_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/linear_regression_with_elastic_net.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/index_to_string_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/sql_transformer.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/correlation_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/dct_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/gradient_boosted_tree_regressor_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/isotonic_regression_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/vector_slicer_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/multilayer_perceptron_classification.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/gradient_boosted_tree_classifier_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/kmeans_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/pca_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/lda_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/polynomial_expansion_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/onehot_encoder_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/aft_survival_regression.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/rformula_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/logistic_regression_with_elastic_net.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/decision_tree_classification_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/estimator_transformer_param_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/tokenizer_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/min_max_scaler_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/pipeline_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/random_forest_classifier_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/string_indexer_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/als_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/n_gram_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/vector_indexer_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/normalizer_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/quantile_discretizer_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/train_validation_split.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/chisq_selector_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/gaussian_mixture_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/bucketed_random_projection_lsh_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/bucketizer_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/elementwise_product_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/tf_idf_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/word2vec_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/chi_square_test_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/pagerank.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sql/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sql/hive.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sql/basic.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sql/datasource.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sql/streaming/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sql/streaming/structured_kafka_wordcount.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sql/streaming/structured_network_wordcount_windowed.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sql/streaming/structured_network_wordcount.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/wordcount.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/pi.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/logistic_regression.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/network_wordjoinsentiments.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/sql_network_wordcount.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/queue_stream.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/network_wordcount.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/kafka_wordcount.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/stateful_network_wordcount.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/direct_kafka_wordcount.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/hdfs_wordcount.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/recoverable_network_wordcount.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/flume_wordcount.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/transitive_closure.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/kmeans.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/avro_inputformat.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/linear_regression_with_sgd_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/svd_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/recommendation_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/regression_metrics_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/standard_scaler_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/kernel_density_estimation_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/random_forest_regression_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/sampled_rdds.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/fpgrowth_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/streaming_k_means_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/latent_dirichlet_allocation_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/bisecting_k_means_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/gradient_boosting_classification_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/multi_class_metrics_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/correlations_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/pca_rowmatrix_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/naive_bayes_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/decision_tree_regression_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/ranking_metrics_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/gaussian_mixture_model.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/logistic_regression_with_lbfgs_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/gradient_boosting_regression_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/power_iteration_clustering_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/binary_classification_metrics_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/logistic_regression.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/hypothesis_testing_kolmogorov_smirnov_test_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/isotonic_regression_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/word2vec.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/summary_statistics_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/multi_label_metrics_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/hypothesis_testing_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/kmeans.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/decision_tree_classification_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/random_rdd_generation.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/svm_with_sgd_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/streaming_linear_regression_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/stratified_sampling_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/normalizer_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/random_forest_classification_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/gaussian_mixture_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/elementwise_product_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/tf_idf_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/word2vec_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/correlations.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/k_means_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/parquet_inputformat.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/als.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sort.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPCAExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaCrossValidationExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaAFTSurvivalRegressionExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMaxAbsScalerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionSummaryExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionWithElasticNetExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearSVCExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorIndexerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaKMeansExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPipelineExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestRegressorExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaTfIdfExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorAssemblerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaInteractionExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketizerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeRegressionExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaSQLTransformerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDocument.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearRegressionWithElasticNetExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketedRandomProjectionLSHExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGeneralizedLinearRegressionExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLDAExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDCTExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaNaiveBayesExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLabeledDocument.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaIndexToStringExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaImputerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaOneVsRestExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaEstimatorTransformerParamExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBisectingKMeansExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGaussianMixtureExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaFPGrowthExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaStopWordsRemoverExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaCountVectorizerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRFormulaExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaOneHotEncoderExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaQuantileDiscretizerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestClassifierExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaCorrelationExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaIsotonicRegressionExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeClassifierExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaALSExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMultilayerPerceptronClassifierExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMulticlassLogisticRegressionWithElasticNetExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMinHashLSHExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaNGramExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaStandardScalerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeClassificationExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaNormalizerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPolynomialExpansionExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBinarizerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeRegressorExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaStringIndexerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaWord2VecExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorSlicerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSqSelectorExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMinMaxScalerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaElementwiseProductExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaTokenizerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSquareTestExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaTrainValidationSplitExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaSparkPi.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/hive/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/hive/JavaSparkHiveExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedTypedAggregation.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredKafkaWordCount.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCountWindowed.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCount.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredSessionization.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedUntypedAggregation.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaLogQuery.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaTC.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaStatusTrackerDemo.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecord.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaFlumeEventCount.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaDirectKafkaWordCount.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaNetworkWordCount.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaSqlNetworkWordCount.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecoverableNetworkWordCount.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaStatefulNetworkWordCount.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaCustomReceiver.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaQueueStream.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaKafkaWordCount.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaHdfsLR.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaPageRank.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaWordCount.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaPCAExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVDExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaKMeansExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLBFGSExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaKernelDensityEstimationExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaMultiLabelClassificationMetricsExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVMWithSGDExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaMulticlassClassificationMetricsExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaPrefixSpanExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLatentDirichletAllocationExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeRegressionExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaAssociationRulesExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaStreamingTestExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaBinaryClassificationMetricsExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestClassificationExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestRegressionExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaStratifiedSamplingExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaNaiveBayesExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingClassificationExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaPowerIterationClusteringExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRankingMetricsExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaALS.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRecommendationExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaBisectingKMeansExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaGaussianMixtureExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingRegressionExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaIsotonicRegressionExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSimpleFPGrowth.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRegressionMetricsExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeClassificationExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaCorrelationsExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingKolmogorovSmirnovTestExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLogisticRegressionWithLBFGSExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaChiSqSelectorExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaElementwiseProductExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSummaryStatisticsExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLinearRegressionWithSGDExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/DFSReadWriteTest.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/GroupByTest.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LinearSVCExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/InteractionExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MaxAbsScalerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/SQLTransformerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/StandardScalerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BisectingKMeansExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ElementwiseProductExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/IndexToStringExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionSummaryExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ALSExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/CorrelationExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BucketizerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorIndexerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MinHashLSHExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/StringIndexerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/TokenizerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/UnaryTransformerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PCAExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/StopWordsRemoverExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MultilayerPerceptronClassifierExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/IsotonicRegressionExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GBTExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ImputerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestRegressorExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ChiSquareTestExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DeveloperApiExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ChiSqSelectorExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/TfIdfExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BucketedRandomProjectionLSHExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BinarizerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeClassifierExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MinMaxScalerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/OneVsRestExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/OneHotEncoderExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorAssemblerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/Word2VecExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/NaiveBayesExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PipelineExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestClassifierExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DCTExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeRegressorExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionWithElasticNetExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DataFrameExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaCrossValidationExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MulticlassLogisticRegressionWithElasticNetExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeClassificationExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/KMeansExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/EstimatorTransformerParamExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaTrainValidationSplitExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/CountVectorizerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorSlicerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/NormalizerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/FPGrowthExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GaussianMixtureExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PolynomialExpansionExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionWithElasticNetExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeRegressionExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LDAExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GeneralizedLinearRegressionExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RFormulaExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/AFTSurvivalRegressionExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/QuantileDiscretizerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/NGramExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkKMeans.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/MultiBroadcastTest.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/hive/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/hive/SparkHiveExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedUntypedAggregation.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/RDDRelation.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedTypedAggregation.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCountWindowed.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredKafkaWordCount.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCount.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredSessionization.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/pythonconverters/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/pythonconverters/AvroConverters.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalLR.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkTC.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/BroadcastTest.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ExceptionHandlingTest.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalKMeans.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/AggregateMessagesExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/Analytics.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/SSSPExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/SynthBenchmark.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/ConnectedComponentsExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/ComprehensiveExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/PageRankExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/TriangleCountingExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/LiveJournalPageRank.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/HdfsTest.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SimpleSkewedGroupByTest.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkPageRank.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/StatefulNetworkWordCount.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/HdfsWordCount.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKafkaWordCount.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/QueueStream.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/FlumePollingEventCount.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/SqlNetworkWordCount.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/FlumeEventCount.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/RecoverableNetworkWordCount.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewStream.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewGenerator.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/StreamingExamples.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/NetworkWordCount.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/CustomReceiver.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/KafkaWordCount.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/RawNetworkGrep.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkPi.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkALS.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalFileLR.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/DriverSubmissionTest.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LogQuery.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SVMWithSGDExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LatentDirichletAllocationExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLinearRegressionExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MultiLabelMetricsExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RankingMetricsExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/AbstractParams.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StandardScalerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StratifiedSamplingExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/BisectingKMeansExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingKMeansExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/ElementwiseProductExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingClassificationExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestClassificationExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingTestExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/KernelDensityEstimationExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/TFIDFExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingRegressionExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnyPCA.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PMMLModelExportExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LogisticRegressionWithLBFGSExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PCAExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/AssociationRulesExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRunner.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnySVD.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/IsotonicRegressionExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassificationMetricsExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingKolmogorovSmirnovTestExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LBFGSExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PrefixSpanExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/ChiSqSelectorExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/Correlations.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RecommendationExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SVDExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MovieLensALS.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/CosineSimilarity.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MulticlassMetricsExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnSourceVectorExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/Word2VecExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLogisticRegression.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/NaiveBayesExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RegressionMetricsExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SampledRDDs.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LinearRegressionWithSGDExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeClassificationExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RandomRDDGeneration.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/KMeansExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SparseNaiveBayes.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassification.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PowerIterationClusteringExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DenseKMeans.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MultivariateSummarizer.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnRowMatrixExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/NormalizerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestRegressionExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/FPGrowthExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GaussianMixtureExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SummaryStatisticsExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/CorrelationsExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostedTreesRunner.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRegressionExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LinearRegression.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SimpleFPGrowth.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LDAExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkLR.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalPi.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkHdfsLR.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SkewedGroupByTest.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalALS.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/people.json\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/employees.json\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/people.txt\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/full_user.avsc\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/kv1.txt\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/users.parquet\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/user.avsc\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/users.avro\n",
            "spark-2.2.0-bin-hadoop2.7/examples/jars/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/jars/scopt_2.11-3.3.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/data/\n",
            "spark-2.2.0-bin-hadoop2.7/data/graphx/\n",
            "spark-2.2.0-bin-hadoop2.7/data/graphx/followers.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/graphx/users.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/streaming/\n",
            "spark-2.2.0-bin-hadoop2.7/data/streaming/AFINN-111.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/pagerank_data.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/kmeans_data.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/streaming_kmeans_data_test.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/sample_lda_libsvm_data.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/sample_kmeans_data.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/pic_data.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/sample_isotonic_regression_libsvm_data.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/als/\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/als/test.data\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/als/sample_movielens_ratings.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/sample_fpgrowth.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/sample_libsvm_data.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/ridge-data/\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/ridge-data/lpsa.data\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/sample_multiclass_classification_data.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/sample_linear_regression_data.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/sample_binary_classification_data.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/sample_lda_data.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/sample_movielens_data.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/sample_svm_data.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/gmm_data.txt\n",
            "spark-2.2.0-bin-hadoop2.7/R/\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/sparkr.zip\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/groupBy.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/covar_pop.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sampleBy.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sql.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/year.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/tan.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/GBTRegressionModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/last_day.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sign.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/hint.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/randn.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/orderBy.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/otherwise.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/AFTSurvivalRegressionModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/hashCode.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/bin.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dim.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.svmLinear.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/minute.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/createExternalTable-deprecated.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/distinct.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.conf.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/print.jobj.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/md5.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cbrt.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/write.ml.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dapplyCollect.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/acos.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/tables.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/NaiveBayesModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sum.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/structType.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/hex.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/isLocal.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/write.jdbc.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/from_utc_timestamp.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/tableNames.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/createDataFrame.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/isStreaming.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/toJSON.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.getSparkFiles.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/except.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/LDAModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/months_between.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark_partition_id.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/write.parquet.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sumDistinct.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/awaitTermination.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/BisectingKMeansModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sha2.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/abs.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/date_format.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/withColumn.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dayofyear.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sort_array.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/storageLevel.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/setCurrentDatabase.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/ceil.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/floor.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/stddev_pop.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sd.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/print.structType.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.survreg.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/predict.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/count.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/unhex.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/mean.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/instr.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/from_unixtime.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/saveAsTable.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/ltrim.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkRHive.init-deprecated.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/read.parquet.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/match.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/is.nan.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/read.ml.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/lag.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/write.json.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/unpersist.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/corr.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.callJStatic.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rint.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/LinearSVCModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.gbt.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/RandomForestClassificationModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/persist.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/var_pop.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/selectExpr.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/crc32.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/expr.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.callJMethod.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/with.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/generateAliasesForIntersectedCols.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/GeneralizedLinearRegressionModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/IsotonicRegressionModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/setLogLevel.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/shiftRightUnsigned.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/base64.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/array_contains.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/expm1.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/write.orc.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.version.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/insertInto.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/SparkDataFrame.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/merge.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dayofmonth.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/listDatabases.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/summarize.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/format_number.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/from_json.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dropDuplicates.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cache.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/write.text.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/approxCountDistinct.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkRSQL.init-deprecated.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/LogisticRegressionModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/stddev_samp.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/pivot.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/showDF.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/between.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/struct.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/subset.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/posexplode.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/lit.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/shiftLeft.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/glm.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/hypot.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/recoverPartitions.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.session.stop.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/translate.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/drop.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/GBTClassificationModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/shiftRight.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/regexp_replace.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/randomSplit.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/length.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rowsBetween.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/read.jdbc.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/schema.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/toRadians.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/filter.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/bround.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/createOrReplaceTempView.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cancelJobGroup.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/second.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/upper.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/head.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/limit.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/concat_ws.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/when.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/FPGrowthModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/install.spark.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.newJObject.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dropTempView.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/unbase64.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/soundex.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/structField.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.addFile.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.bisectingKmeans.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cacheTable.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cosh.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.mlp.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/ntile.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/atan2.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.kstest.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dtypes.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/reverse.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sinh.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.lda.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/createTable.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/negate.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/asin.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/hash.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rank.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/toDegrees.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/columns.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/columnfunctions.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/substring_index.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/to_timestamp.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.naiveBayes.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/atan.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.isoreg.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/factorial.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/countDistinct.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/quarter.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/setCheckpointDir.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/least.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/read.text.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/windowOrderBy.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dapply.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/coalesce.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/refreshByPath.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cume_dist.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dense_rank.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/freqItems.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/getNumPartitions.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/KMeansModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.session.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/arrange.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/write.stream.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/encode.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.glm.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/isActive.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/crossJoin.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rpad.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/uncacheTable.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/size.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/conv.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/log10.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/collect.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/read.stream.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/format_string.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/windowPartitionBy.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/union.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/stopQuery.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dropTempTable-deprecated.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/endsWith.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/startsWith.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/nanvl.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/mutate.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/explain.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/R.css\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cov.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/var_samp.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/log2.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/registerTempTable-deprecated.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/lastProgress.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/attach.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/min.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/ncol.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/month.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/window.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/partitionBy.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/percent_rank.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/listFunctions.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/to_utc_timestamp.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/crosstab.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/take.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/exp.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/to_json.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/column.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/ifelse.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/GaussianMixtureModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.logit.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/show.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/setJobGroup.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/KSTest-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/clearJobGroup.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/last.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/printSchema.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rename.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rbind.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/over.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/MultilayerPerceptronClassificationModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/coltypes.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/datediff.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/lead.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/summary.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/WindowSpec.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/unix_timestamp.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/tanh.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/listColumns.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/to_date.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/00Index.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.uiWebUrl.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/max.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/var.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/print.structField.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.als.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/alias.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sha1.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/status.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/read.orc.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/currentDatabase.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/write.df.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/round.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.init-deprecated.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/refreshTable.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/nrow.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/pmod.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/intersect.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rtrim.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/substr.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/log.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/levenshtein.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/monotonically_increasing_id.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/checkpoint.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/decode.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.lapply.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/read.json.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/RandomForestRegressionModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/trim.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/select.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/regexp_extract.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/as.data.frame.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rangeBetween.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/queryName.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sample.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/lower.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/repartition.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/concat.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cast.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/date_add.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.fpGrowth.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/hour.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/initcap.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/explode.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/add_months.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/bitwiseNOT.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/approxQuantile.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/kurtosis.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/greatest.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/first.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/read.df.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rand.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/next_day.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/clearCache.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/nafunctions.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/row_number.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/lpad.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/skewness.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/gapplyCollect.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/locate.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/avg.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sqrt.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cos.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/log1p.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/str.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/StreamingQuery.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/ALSModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/ascii.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/listTables.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sin.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/histogram.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/weekofyear.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/GroupedData.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/fitted.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/date_sub.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/tableToDF.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.getSparkFilesRootDirectory.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/join.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/gapply.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.randomForest.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.kmeans.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.gaussianMixture.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/INDEX\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/R/\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/R/SparkR.rdx\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/R/SparkR\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/R/SparkR.rdb\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/help/\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/help/aliases.rds\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/help/paths.rds\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/help/SparkR.rdx\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/help/AnIndex\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/help/SparkR.rdb\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/DESCRIPTION\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/Meta/\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/Meta/hsearch.rds\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/Meta/nsInfo.rds\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/Meta/package.rds\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/Meta/links.rds\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/Meta/Rd.rds\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/profile/\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/profile/general.R\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/profile/shell.R\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/worker/\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/worker/daemon.R\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/worker/worker.R\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/NAMESPACE\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/tests/\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/tests/testthat/\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/tests/testthat/test_basic.R\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-slf4j.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-scalacheck.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-py4j.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-paranamer.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-netlib.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-Mockito.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-minlog.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-graphlib-dot.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-d3.min.js.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-f2j.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-junit-interface.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-boto.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-spire.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-pyrolite.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-xmlenc.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-scala.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-SnapTree.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-AnchorJS.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-jline.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-heapq.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-cloudpickle.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-reflectasm.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-jpmml-model.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-modernizr.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-javolution.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-jbcrypt.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-sbt-launch-lib.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-postgresql.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-kryo.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-DPark.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-scopt.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-sorttable.js.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-dagre-d3.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-jquery.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-protobuf.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-antlr.txt\n",
            "spark-2.2.0-bin-hadoop2.7/conf/\n",
            "spark-2.2.0-bin-hadoop2.7/conf/fairscheduler.xml.template\n",
            "spark-2.2.0-bin-hadoop2.7/conf/metrics.properties.template\n",
            "spark-2.2.0-bin-hadoop2.7/conf/spark-env.sh.template\n",
            "spark-2.2.0-bin-hadoop2.7/conf/log4j.properties.template\n",
            "spark-2.2.0-bin-hadoop2.7/conf/docker.properties.template\n",
            "spark-2.2.0-bin-hadoop2.7/conf/slaves.template\n",
            "spark-2.2.0-bin-hadoop2.7/conf/spark-defaults.conf.template\n",
            "spark-2.2.0-bin-hadoop2.7/LICENSE\n",
            "spark-2.2.0-bin-hadoop2.7/bin/\n",
            "spark-2.2.0-bin-hadoop2.7/bin/spark-shell\n",
            "spark-2.2.0-bin-hadoop2.7/bin/spark-submit.cmd\n",
            "spark-2.2.0-bin-hadoop2.7/bin/spark-shell2.cmd\n",
            "spark-2.2.0-bin-hadoop2.7/bin/pyspark\n",
            "spark-2.2.0-bin-hadoop2.7/bin/sparkR.cmd\n",
            "spark-2.2.0-bin-hadoop2.7/bin/spark-class2.cmd\n",
            "spark-2.2.0-bin-hadoop2.7/bin/run-example.cmd\n",
            "spark-2.2.0-bin-hadoop2.7/bin/spark-submit2.cmd\n",
            "spark-2.2.0-bin-hadoop2.7/bin/spark-class\n",
            "spark-2.2.0-bin-hadoop2.7/bin/spark-submit\n",
            "spark-2.2.0-bin-hadoop2.7/bin/spark-sql\n",
            "spark-2.2.0-bin-hadoop2.7/bin/find-spark-home\n",
            "spark-2.2.0-bin-hadoop2.7/bin/run-example\n",
            "spark-2.2.0-bin-hadoop2.7/bin/beeline\n",
            "spark-2.2.0-bin-hadoop2.7/bin/pyspark2.cmd\n",
            "spark-2.2.0-bin-hadoop2.7/bin/spark-shell.cmd\n",
            "spark-2.2.0-bin-hadoop2.7/bin/spark-class.cmd\n",
            "spark-2.2.0-bin-hadoop2.7/bin/pyspark.cmd\n",
            "spark-2.2.0-bin-hadoop2.7/bin/sparkR\n",
            "spark-2.2.0-bin-hadoop2.7/bin/beeline.cmd\n",
            "spark-2.2.0-bin-hadoop2.7/bin/sparkR2.cmd\n",
            "spark-2.2.0-bin-hadoop2.7/bin/load-spark-env.sh\n",
            "spark-2.2.0-bin-hadoop2.7/bin/load-spark-env.cmd\n",
            "spark-2.2.0-bin-hadoop2.7/yarn/\n",
            "spark-2.2.0-bin-hadoop2.7/yarn/spark-2.2.0-yarn-shuffle.jar\n",
            "spark-2.2.0-bin-hadoop2.7/README.md\n",
            "環境初始化完畢\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u314rKd1Z2BC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, sys\n",
        "os.environ['SPARK_HOME'] = \"/usr/local/spark\"\n",
        "os.environ['PYSPARK_PYTHON'] = \"/usr/local/bin/python\"\n",
        "sys.path.append(\"/usr/local/spark/python/\")\n",
        "sys.path.append(\"/usr/local/spark/python/lib/pyspark.zip\")\n",
        "sys.path.append(\"/usr/local/spark/python/lib/py4j-0.10.4-src.zip\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dU1UwvjtaaSk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark import SparkConf\n",
        "sc =SparkContext()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEFwbXSgauLA",
        "colab_type": "code",
        "outputId": "41782a83-bc5a-446c-aa05-654a3f64ae2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        }
      },
      "source": [
        "!wget -O pm25.csv \"https://www.dropbox.com/s/zkn3ba7pitv83el/pm2.5Taiwan.csv?dl=0\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-04 01:55:00--  https://www.dropbox.com/s/zkn3ba7pitv83el/pm2.5Taiwan.csv?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.9.1, 2620:100:601f:1::a27d:901\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.9.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/zkn3ba7pitv83el/pm2.5Taiwan.csv [following]\n",
            "--2019-10-04 01:55:00--  https://www.dropbox.com/s/raw/zkn3ba7pitv83el/pm2.5Taiwan.csv\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc37f854d140868ba1fb51bb7cd6.dl.dropboxusercontent.com/cd/0/inline/ApyyQocXSZ-fGbliprdg0Nj0ykWqc1APB4Rya5SGfBlZq8ipDEej5J0Cd3CVhChssnt5uNQPWyXo2L-zhY4xJAxxfBEXVr_CdK1sFACeoB2-GA/file# [following]\n",
            "--2019-10-04 01:55:00--  https://uc37f854d140868ba1fb51bb7cd6.dl.dropboxusercontent.com/cd/0/inline/ApyyQocXSZ-fGbliprdg0Nj0ykWqc1APB4Rya5SGfBlZq8ipDEej5J0Cd3CVhChssnt5uNQPWyXo2L-zhY4xJAxxfBEXVr_CdK1sFACeoB2-GA/file\n",
            "Resolving uc37f854d140868ba1fb51bb7cd6.dl.dropboxusercontent.com (uc37f854d140868ba1fb51bb7cd6.dl.dropboxusercontent.com)... 162.125.9.6, 2620:100:601f:6::a27d:906\n",
            "Connecting to uc37f854d140868ba1fb51bb7cd6.dl.dropboxusercontent.com (uc37f854d140868ba1fb51bb7cd6.dl.dropboxusercontent.com)|162.125.9.6|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 50453822 (48M) [text/plain]\n",
            "Saving to: ‘pm25.csv’\n",
            "\n",
            "pm25.csv            100%[===================>]  48.12M  61.9MB/s    in 0.8s    \n",
            "\n",
            "2019-10-04 01:55:02 (61.9 MB/s) - ‘pm25.csv’ saved [50453822/50453822]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjbkqXx0ayTw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weather = sc.textFile(\"./pm25.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9jzWplxZ2BE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weather_data_rdd = weather.map(lambda line : line.split(\",\"))\n",
        "pm25schema = weather_data_rdd.first()\n",
        "# print(pm25schema)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FisGymJZ2BM",
        "colab_type": "text"
      },
      "source": [
        "# 回想如何使用RDD計算求取2015年，大里每小時的平均pm25數值。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RPMxMS1f11qI",
        "colab": {}
      },
      "source": [
        "clean_weather_data = weather_data_rdd\\\n",
        "                    .filter(lambda x: x!=pm25schema)\\\n",
        "                    .filter(remove_row_with_noise)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KGoSn_c3viJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dalipm25 = clean_weather_data.filter(lambda x: x[1] == '大里' and x[2]== \"PM2.5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzMR32zDZ2BQ",
        "colab_type": "code",
        "outputId": "28ef79c5-7e85-447b-86ba-f0e1cf94ddb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import math\n",
        "def remove_row_with_noise (x):\n",
        "    for i in range(3, len(x)):\n",
        "        if not x[i].isdecimal():\n",
        "            return False\n",
        "    return True \n",
        "\n",
        "def hourKeyGen(x):\n",
        "    hourkeypair = []\n",
        "    x=x[3:]\n",
        "    for i, value in enumerate(x):\n",
        "      print(i, value)\n",
        "      hourkeypair.append((i, float(value)))\n",
        "    return hourkeypair\n",
        "\n",
        "count = dalipm25.count()\n",
        "HourSum = dalipm25\\\n",
        "            .flatMap(hourKeyGen)\\\n",
        "            .reduceByKey(lambda x,y: x+y)\\\n",
        "            .mapValues(lambda x: x/count)\\\n",
        "            .map(lambda x: (x[1],x[0])).top(24)\n",
        "\n",
        "print(HourSum)"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(35.43939393939394, 11), (35.38383838383838, 12), (34.92424242424242, 10), (34.59090909090909, 13), (33.43434343434343, 14), (32.80808080808081, 9), (32.63636363636363, 21), (32.505050505050505, 20), (32.474747474747474, 15), (32.18181818181818, 22), (31.747474747474747, 19), (31.166666666666668, 16), (30.939393939393938, 23), (30.818181818181817, 17), (30.63131313131313, 18), (30.04040404040404, 0), (29.636363636363637, 8), (29.1010101010101, 1), (28.1010101010101, 2), (27.065656565656564, 3), (26.51010101010101, 7), (25.853535353535353, 4), (25.055555555555557, 5), (24.883838383838384, 6)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOldTm0PZ2BR",
        "colab_type": "text"
      },
      "source": [
        "# 使用DataFrame 來計算每小時平均值"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Peojix3wauLF",
        "colab_type": "code",
        "outputId": "a9bb2a03-4ead-46fd-b69c-4519b9203e5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(dalipm25.first())"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['2015/01/01', '大里', 'PM2.5', '53', '55', '58', '53', '43', '36', '35', '42', '55', '64', '65', '59', '52', '44', '47', '41', '43', '40', '42', '35', '28', '20', '18', '16']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLNcEsygc6oX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql import SQLContext\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3TQFM4tixp6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3pl9EZixkNiT",
        "outputId": "69c051a4-7560-45ef-d1f1-60cb15ec0856",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from pyspark.sql import SQLContext\n",
        "from pyspark.sql import Row\n",
        "\n",
        "dalipm25row = dalipm25.map(lambda p:\n",
        "        Row(\n",
        "        date = p[0],\n",
        "        location = p[1],\n",
        "        measure = p[2],\n",
        "        hr_01 = float(p[3]), hr_02 = float(p[4]),hr_03 = float(p[5]),hr_04 = float(p[6]),hr_05 = float(p[7]),\n",
        "        hr_06 = float(p[8]),hr_07 = float(p[9]),hr_08 = float(p[10]),hr_09 = float(p[11]),hr_10 = float(p[12]),\n",
        "        hr_11 = float(p[13]),hr_12 = float(p[14]),hr_13 = float(p[15]),hr_14 = float(p[16]),hr_15 = float(p[17]),\n",
        "        hr_16 = float(p[18]),hr_17 = float(p[19]),hr_18 = float(p[20]),hr_19 = float(p[21]),hr_20 = float(p[22]),\n",
        "        hr_21 = float(p[23]),hr_22 = float(p[24]),hr_23 = float(p[25]),hr_24 = float(p[26]),\n",
        "    )\n",
        ")\n",
        "\n",
        "dalipm25row.take(5)"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(date='2015/01/01', hr_01=53.0, hr_02=55.0, hr_03=58.0, hr_04=53.0, hr_05=43.0, hr_06=36.0, hr_07=35.0, hr_08=42.0, hr_09=55.0, hr_10=64.0, hr_11=65.0, hr_12=59.0, hr_13=52.0, hr_14=44.0, hr_15=47.0, hr_16=41.0, hr_17=43.0, hr_18=40.0, hr_19=42.0, hr_20=35.0, hr_21=28.0, hr_22=20.0, hr_23=18.0, hr_24=16.0, location='大里', measure='PM2.5'),\n",
              " Row(date='2015/01/02', hr_01=21.0, hr_02=22.0, hr_03=26.0, hr_04=23.0, hr_05=20.0, hr_06=18.0, hr_07=15.0, hr_08=21.0, hr_09=21.0, hr_10=25.0, hr_11=29.0, hr_12=32.0, hr_13=34.0, hr_14=29.0, hr_15=32.0, hr_16=39.0, hr_17=51.0, hr_18=51.0, hr_19=47.0, hr_20=43.0, hr_21=43.0, hr_22=48.0, hr_23=47.0, hr_24=53.0, location='大里', measure='PM2.5'),\n",
              " Row(date='2015/01/03', hr_01=48.0, hr_02=48.0, hr_03=43.0, hr_04=38.0, hr_05=37.0, hr_06=36.0, hr_07=37.0, hr_08=34.0, hr_09=37.0, hr_10=46.0, hr_11=64.0, hr_12=77.0, hr_13=83.0, hr_14=75.0, hr_15=68.0, hr_16=69.0, hr_17=64.0, hr_18=65.0, hr_19=59.0, hr_20=66.0, hr_21=71.0, hr_22=66.0, hr_23=57.0, hr_24=48.0, location='大里', measure='PM2.5'),\n",
              " Row(date='2015/01/04', hr_01=60.0, hr_02=56.0, hr_03=53.0, hr_04=43.0, hr_05=53.0, hr_06=53.0, hr_07=52.0, hr_08=44.0, hr_09=44.0, hr_10=50.0, hr_11=49.0, hr_12=51.0, hr_13=45.0, hr_14=42.0, hr_15=40.0, hr_16=38.0, hr_17=36.0, hr_18=43.0, hr_19=51.0, hr_20=63.0, hr_21=68.0, hr_22=72.0, hr_23=66.0, hr_24=58.0, location='大里', measure='PM2.5'),\n",
              " Row(date='2015/01/05', hr_01=48.0, hr_02=42.0, hr_03=42.0, hr_04=34.0, hr_05=34.0, hr_06=28.0, hr_07=34.0, hr_08=35.0, hr_09=45.0, hr_10=47.0, hr_11=54.0, hr_12=46.0, hr_13=35.0, hr_14=19.0, hr_15=16.0, hr_16=21.0, hr_17=24.0, hr_18=28.0, hr_19=37.0, hr_20=52.0, hr_21=60.0, hr_22=62.0, hr_23=64.0, hr_24=61.0, location='大里', measure='PM2.5')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kF182dzHZ2BU",
        "colab_type": "code",
        "outputId": "e1eb45e7-1e59-4674-d525-c6c6b9507efe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 494
        }
      },
      "source": [
        "df = spark.createDataFrame(dalipm25row)\n",
        "df.show()"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+--------+-------+\n",
            "|      date|hr_01|hr_02|hr_03|hr_04|hr_05|hr_06|hr_07|hr_08|hr_09|hr_10|hr_11|hr_12|hr_13|hr_14|hr_15|hr_16|hr_17|hr_18|hr_19|hr_20|hr_21|hr_22|hr_23|hr_24|location|measure|\n",
            "+----------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+--------+-------+\n",
            "|2015/01/01| 53.0| 55.0| 58.0| 53.0| 43.0| 36.0| 35.0| 42.0| 55.0| 64.0| 65.0| 59.0| 52.0| 44.0| 47.0| 41.0| 43.0| 40.0| 42.0| 35.0| 28.0| 20.0| 18.0| 16.0|      大里|  PM2.5|\n",
            "|2015/01/02| 21.0| 22.0| 26.0| 23.0| 20.0| 18.0| 15.0| 21.0| 21.0| 25.0| 29.0| 32.0| 34.0| 29.0| 32.0| 39.0| 51.0| 51.0| 47.0| 43.0| 43.0| 48.0| 47.0| 53.0|      大里|  PM2.5|\n",
            "|2015/01/03| 48.0| 48.0| 43.0| 38.0| 37.0| 36.0| 37.0| 34.0| 37.0| 46.0| 64.0| 77.0| 83.0| 75.0| 68.0| 69.0| 64.0| 65.0| 59.0| 66.0| 71.0| 66.0| 57.0| 48.0|      大里|  PM2.5|\n",
            "|2015/01/04| 60.0| 56.0| 53.0| 43.0| 53.0| 53.0| 52.0| 44.0| 44.0| 50.0| 49.0| 51.0| 45.0| 42.0| 40.0| 38.0| 36.0| 43.0| 51.0| 63.0| 68.0| 72.0| 66.0| 58.0|      大里|  PM2.5|\n",
            "|2015/01/05| 48.0| 42.0| 42.0| 34.0| 34.0| 28.0| 34.0| 35.0| 45.0| 47.0| 54.0| 46.0| 35.0| 19.0| 16.0| 21.0| 24.0| 28.0| 37.0| 52.0| 60.0| 62.0| 64.0| 61.0|      大里|  PM2.5|\n",
            "|2015/01/06| 59.0| 40.0| 34.0| 25.0| 27.0| 29.0| 26.0| 33.0| 42.0| 47.0| 38.0| 24.0| 14.0|  8.0| 17.0| 30.0| 51.0| 62.0| 68.0| 83.0| 83.0| 96.0|103.0|110.0|      大里|  PM2.5|\n",
            "|2015/01/08|  7.0|  9.0| 13.0| 18.0| 11.0| 12.0| 17.0| 29.0| 34.0| 39.0| 41.0| 46.0| 46.0| 44.0| 43.0| 39.0| 41.0| 46.0| 47.0| 48.0| 47.0| 47.0| 43.0| 33.0|      大里|  PM2.5|\n",
            "|2015/01/09| 35.0| 34.0| 37.0| 30.0| 25.0| 25.0| 22.0| 21.0| 18.0| 20.0| 14.0| 12.0| 21.0| 31.0| 44.0| 46.0| 52.0| 44.0| 39.0| 37.0| 43.0| 43.0| 42.0| 39.0|      大里|  PM2.5|\n",
            "|2015/01/10| 38.0| 33.0| 31.0| 24.0| 20.0| 19.0| 22.0| 31.0| 31.0| 45.0| 48.0| 49.0| 38.0| 39.0| 43.0| 46.0| 43.0| 36.0| 33.0| 29.0| 37.0| 34.0| 39.0| 33.0|      大里|  PM2.5|\n",
            "|2015/01/11| 37.0| 41.0| 43.0| 43.0| 27.0| 22.0| 26.0| 34.0| 39.0| 37.0| 51.0| 53.0| 61.0| 56.0| 48.0| 43.0| 37.0| 43.0| 43.0| 48.0| 54.0| 51.0| 46.0| 35.0|      大里|  PM2.5|\n",
            "|2015/01/12| 36.0| 40.0| 33.0| 32.0| 33.0| 40.0| 37.0| 34.0| 39.0| 53.0| 60.0| 65.0| 57.0| 50.0| 52.0| 51.0| 43.0| 24.0| 20.0| 28.0| 35.0| 40.0| 30.0| 36.0|      大里|  PM2.5|\n",
            "|2015/01/13| 36.0| 36.0| 32.0| 33.0| 38.0| 45.0| 38.0| 45.0| 45.0| 76.0| 84.0| 96.0| 92.0| 87.0| 64.0| 33.0| 21.0| 22.0| 20.0| 15.0|  7.0| 12.0|  9.0| 11.0|      大里|  PM2.5|\n",
            "|2015/01/14| 10.0|  7.0|  3.0|  0.0|  3.0|  7.0|  5.0|  1.0|  0.0|  0.0|  0.0|  0.0|  0.0|  0.0|  4.0| 12.0| 13.0| 10.0| 12.0| 14.0| 21.0| 19.0| 15.0|  8.0|      大里|  PM2.5|\n",
            "|2015/01/15|  2.0|  3.0|  7.0|  3.0|  7.0|  5.0| 10.0|  7.0| 13.0| 16.0| 14.0|  8.0|  5.0| 13.0| 20.0| 30.0| 30.0| 33.0| 28.0| 29.0| 33.0| 26.0| 23.0| 12.0|      大里|  PM2.5|\n",
            "|2015/01/17| 42.0| 33.0| 25.0| 18.0| 13.0|  9.0| 12.0| 20.0| 28.0| 33.0| 33.0| 43.0| 52.0| 55.0| 57.0| 60.0| 66.0| 77.0| 76.0| 76.0| 77.0| 74.0| 82.0| 75.0|      大里|  PM2.5|\n",
            "|2015/01/18| 77.0| 66.0| 61.0| 62.0| 70.0| 71.0| 69.0| 71.0| 75.0| 82.0| 90.0| 94.0| 88.0| 75.0| 57.0| 47.0| 33.0| 31.0| 25.0| 21.0| 16.0| 16.0| 18.0| 16.0|      大里|  PM2.5|\n",
            "|2015/01/19| 14.0| 14.0| 19.0| 18.0| 16.0|  7.0|  6.0| 12.0| 18.0| 30.0| 26.0| 28.0| 31.0| 32.0| 35.0| 37.0| 41.0| 43.0| 42.0| 50.0| 53.0| 54.0| 57.0| 62.0|      大里|  PM2.5|\n",
            "|2015/01/20| 62.0| 57.0| 52.0| 53.0| 61.0| 62.0| 62.0| 65.0| 73.0| 82.0| 83.0| 88.0| 83.0| 81.0| 73.0| 71.0| 69.0| 71.0| 66.0| 55.0| 49.0| 42.0| 49.0| 43.0|      大里|  PM2.5|\n",
            "|2015/01/21| 46.0| 37.0| 30.0| 31.0| 31.0| 38.0| 42.0| 46.0| 46.0| 36.0| 33.0| 33.0| 37.0| 34.0| 33.0| 33.0| 34.0| 39.0| 48.0| 50.0| 47.0| 45.0| 43.0| 39.0|      大里|  PM2.5|\n",
            "|2015/01/23| 61.0| 47.0| 46.0| 55.0| 49.0| 39.0| 29.0| 31.0| 29.0| 19.0| 12.0| 14.0| 30.0| 49.0| 57.0| 55.0| 57.0| 56.0| 64.0| 72.0| 73.0| 75.0| 68.0| 68.0|      大里|  PM2.5|\n",
            "+----------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+--------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQTLug-b4zwB",
        "colab_type": "text"
      },
      "source": [
        "# 直接透過rdd生成data frame. toDF()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpycWwEed5PR",
        "colab_type": "code",
        "outputId": "63254b9c-d79b-4bcd-83f9-d62f2681e939",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        }
      },
      "source": [
        "dalipm25.toDF().show()"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+---+-----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
            "|        _1| _2|   _3| _4| _5| _6| _7| _8| _9|_10|_11|_12|_13|_14|_15|_16|_17|_18|_19|_20|_21|_22|_23|_24|_25|_26|_27|\n",
            "+----------+---+-----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
            "|2015/01/01| 大里|PM2.5| 53| 55| 58| 53| 43| 36| 35| 42| 55| 64| 65| 59| 52| 44| 47| 41| 43| 40| 42| 35| 28| 20| 18| 16|\n",
            "|2015/01/02| 大里|PM2.5| 21| 22| 26| 23| 20| 18| 15| 21| 21| 25| 29| 32| 34| 29| 32| 39| 51| 51| 47| 43| 43| 48| 47| 53|\n",
            "|2015/01/03| 大里|PM2.5| 48| 48| 43| 38| 37| 36| 37| 34| 37| 46| 64| 77| 83| 75| 68| 69| 64| 65| 59| 66| 71| 66| 57| 48|\n",
            "|2015/01/04| 大里|PM2.5| 60| 56| 53| 43| 53| 53| 52| 44| 44| 50| 49| 51| 45| 42| 40| 38| 36| 43| 51| 63| 68| 72| 66| 58|\n",
            "|2015/01/05| 大里|PM2.5| 48| 42| 42| 34| 34| 28| 34| 35| 45| 47| 54| 46| 35| 19| 16| 21| 24| 28| 37| 52| 60| 62| 64| 61|\n",
            "|2015/01/06| 大里|PM2.5| 59| 40| 34| 25| 27| 29| 26| 33| 42| 47| 38| 24| 14|  8| 17| 30| 51| 62| 68| 83| 83| 96|103|110|\n",
            "|2015/01/08| 大里|PM2.5|  7|  9| 13| 18| 11| 12| 17| 29| 34| 39| 41| 46| 46| 44| 43| 39| 41| 46| 47| 48| 47| 47| 43| 33|\n",
            "|2015/01/09| 大里|PM2.5| 35| 34| 37| 30| 25| 25| 22| 21| 18| 20| 14| 12| 21| 31| 44| 46| 52| 44| 39| 37| 43| 43| 42| 39|\n",
            "|2015/01/10| 大里|PM2.5| 38| 33| 31| 24| 20| 19| 22| 31| 31| 45| 48| 49| 38| 39| 43| 46| 43| 36| 33| 29| 37| 34| 39| 33|\n",
            "|2015/01/11| 大里|PM2.5| 37| 41| 43| 43| 27| 22| 26| 34| 39| 37| 51| 53| 61| 56| 48| 43| 37| 43| 43| 48| 54| 51| 46| 35|\n",
            "|2015/01/12| 大里|PM2.5| 36| 40| 33| 32| 33| 40| 37| 34| 39| 53| 60| 65| 57| 50| 52| 51| 43| 24| 20| 28| 35| 40| 30| 36|\n",
            "|2015/01/13| 大里|PM2.5| 36| 36| 32| 33| 38| 45| 38| 45| 45| 76| 84| 96| 92| 87| 64| 33| 21| 22| 20| 15|  7| 12|  9| 11|\n",
            "|2015/01/14| 大里|PM2.5| 10|  7|  3|  0|  3|  7|  5|  1|  0|  0|  0|  0|  0|  0|  4| 12| 13| 10| 12| 14| 21| 19| 15|  8|\n",
            "|2015/01/15| 大里|PM2.5|  2|  3|  7|  3|  7|  5| 10|  7| 13| 16| 14|  8|  5| 13| 20| 30| 30| 33| 28| 29| 33| 26| 23| 12|\n",
            "|2015/01/17| 大里|PM2.5| 42| 33| 25| 18| 13|  9| 12| 20| 28| 33| 33| 43| 52| 55| 57| 60| 66| 77| 76| 76| 77| 74| 82| 75|\n",
            "|2015/01/18| 大里|PM2.5| 77| 66| 61| 62| 70| 71| 69| 71| 75| 82| 90| 94| 88| 75| 57| 47| 33| 31| 25| 21| 16| 16| 18| 16|\n",
            "|2015/01/19| 大里|PM2.5| 14| 14| 19| 18| 16|  7|  6| 12| 18| 30| 26| 28| 31| 32| 35| 37| 41| 43| 42| 50| 53| 54| 57| 62|\n",
            "|2015/01/20| 大里|PM2.5| 62| 57| 52| 53| 61| 62| 62| 65| 73| 82| 83| 88| 83| 81| 73| 71| 69| 71| 66| 55| 49| 42| 49| 43|\n",
            "|2015/01/21| 大里|PM2.5| 46| 37| 30| 31| 31| 38| 42| 46| 46| 36| 33| 33| 37| 34| 33| 33| 34| 39| 48| 50| 47| 45| 43| 39|\n",
            "|2015/01/23| 大里|PM2.5| 61| 47| 46| 55| 49| 39| 29| 31| 29| 19| 12| 14| 30| 49| 57| 55| 57| 56| 64| 72| 73| 75| 68| 68|\n",
            "+----------+---+-----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Bku4vR-hfcT",
        "colab_type": "code",
        "outputId": "4d0ddc85-a1ff-4334-dfd8-e49bb30f9703",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        }
      },
      "source": [
        "pm25schema = weather_data_rdd.first()\n",
        "dalipm25.toDF(pm25schema).show()"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+---+-----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
            "|        日期| 測站|   測項| 00| 01| 02| 03| 04| 05| 06| 07| 08| 09| 10| 11| 12| 13| 14| 15| 16| 17| 18| 19| 20| 21| 22| 23|\n",
            "+----------+---+-----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
            "|2015/01/01| 大里|PM2.5| 53| 55| 58| 53| 43| 36| 35| 42| 55| 64| 65| 59| 52| 44| 47| 41| 43| 40| 42| 35| 28| 20| 18| 16|\n",
            "|2015/01/02| 大里|PM2.5| 21| 22| 26| 23| 20| 18| 15| 21| 21| 25| 29| 32| 34| 29| 32| 39| 51| 51| 47| 43| 43| 48| 47| 53|\n",
            "|2015/01/03| 大里|PM2.5| 48| 48| 43| 38| 37| 36| 37| 34| 37| 46| 64| 77| 83| 75| 68| 69| 64| 65| 59| 66| 71| 66| 57| 48|\n",
            "|2015/01/04| 大里|PM2.5| 60| 56| 53| 43| 53| 53| 52| 44| 44| 50| 49| 51| 45| 42| 40| 38| 36| 43| 51| 63| 68| 72| 66| 58|\n",
            "|2015/01/05| 大里|PM2.5| 48| 42| 42| 34| 34| 28| 34| 35| 45| 47| 54| 46| 35| 19| 16| 21| 24| 28| 37| 52| 60| 62| 64| 61|\n",
            "|2015/01/06| 大里|PM2.5| 59| 40| 34| 25| 27| 29| 26| 33| 42| 47| 38| 24| 14|  8| 17| 30| 51| 62| 68| 83| 83| 96|103|110|\n",
            "|2015/01/08| 大里|PM2.5|  7|  9| 13| 18| 11| 12| 17| 29| 34| 39| 41| 46| 46| 44| 43| 39| 41| 46| 47| 48| 47| 47| 43| 33|\n",
            "|2015/01/09| 大里|PM2.5| 35| 34| 37| 30| 25| 25| 22| 21| 18| 20| 14| 12| 21| 31| 44| 46| 52| 44| 39| 37| 43| 43| 42| 39|\n",
            "|2015/01/10| 大里|PM2.5| 38| 33| 31| 24| 20| 19| 22| 31| 31| 45| 48| 49| 38| 39| 43| 46| 43| 36| 33| 29| 37| 34| 39| 33|\n",
            "|2015/01/11| 大里|PM2.5| 37| 41| 43| 43| 27| 22| 26| 34| 39| 37| 51| 53| 61| 56| 48| 43| 37| 43| 43| 48| 54| 51| 46| 35|\n",
            "|2015/01/12| 大里|PM2.5| 36| 40| 33| 32| 33| 40| 37| 34| 39| 53| 60| 65| 57| 50| 52| 51| 43| 24| 20| 28| 35| 40| 30| 36|\n",
            "|2015/01/13| 大里|PM2.5| 36| 36| 32| 33| 38| 45| 38| 45| 45| 76| 84| 96| 92| 87| 64| 33| 21| 22| 20| 15|  7| 12|  9| 11|\n",
            "|2015/01/14| 大里|PM2.5| 10|  7|  3|  0|  3|  7|  5|  1|  0|  0|  0|  0|  0|  0|  4| 12| 13| 10| 12| 14| 21| 19| 15|  8|\n",
            "|2015/01/15| 大里|PM2.5|  2|  3|  7|  3|  7|  5| 10|  7| 13| 16| 14|  8|  5| 13| 20| 30| 30| 33| 28| 29| 33| 26| 23| 12|\n",
            "|2015/01/17| 大里|PM2.5| 42| 33| 25| 18| 13|  9| 12| 20| 28| 33| 33| 43| 52| 55| 57| 60| 66| 77| 76| 76| 77| 74| 82| 75|\n",
            "|2015/01/18| 大里|PM2.5| 77| 66| 61| 62| 70| 71| 69| 71| 75| 82| 90| 94| 88| 75| 57| 47| 33| 31| 25| 21| 16| 16| 18| 16|\n",
            "|2015/01/19| 大里|PM2.5| 14| 14| 19| 18| 16|  7|  6| 12| 18| 30| 26| 28| 31| 32| 35| 37| 41| 43| 42| 50| 53| 54| 57| 62|\n",
            "|2015/01/20| 大里|PM2.5| 62| 57| 52| 53| 61| 62| 62| 65| 73| 82| 83| 88| 83| 81| 73| 71| 69| 71| 66| 55| 49| 42| 49| 43|\n",
            "|2015/01/21| 大里|PM2.5| 46| 37| 30| 31| 31| 38| 42| 46| 46| 36| 33| 33| 37| 34| 33| 33| 34| 39| 48| 50| 47| 45| 43| 39|\n",
            "|2015/01/23| 大里|PM2.5| 61| 47| 46| 55| 49| 39| 29| 31| 29| 19| 12| 14| 30| 49| 57| 55| 57| 56| 64| 72| 73| 75| 68| 68|\n",
            "+----------+---+-----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByodqKk3knMK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dfpm25 = dalipm25.toDF(pm25schema)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "np4IHhqFyJ8Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in dfpm25.columns[3:]:\n",
        "  dfpm25 = dfpm25.withColumn(i, dfpm25[i].cast(\"double\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYRCksyPll5S",
        "colab_type": "code",
        "outputId": "12702fea-1e1f-49e1-d9e4-1054b232627b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "dfpm25.printSchema()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- 日期: string (nullable = true)\n",
            " |-- 測站: string (nullable = true)\n",
            " |-- 測項: string (nullable = true)\n",
            " |-- 00: double (nullable = true)\n",
            " |-- 01: double (nullable = true)\n",
            " |-- 02: double (nullable = true)\n",
            " |-- 03: double (nullable = true)\n",
            " |-- 04: double (nullable = true)\n",
            " |-- 05: double (nullable = true)\n",
            " |-- 06: double (nullable = true)\n",
            " |-- 07: double (nullable = true)\n",
            " |-- 08: double (nullable = true)\n",
            " |-- 09: double (nullable = true)\n",
            " |-- 10: double (nullable = true)\n",
            " |-- 11: double (nullable = true)\n",
            " |-- 12: double (nullable = true)\n",
            " |-- 13: double (nullable = true)\n",
            " |-- 14: double (nullable = true)\n",
            " |-- 15: double (nullable = true)\n",
            " |-- 16: double (nullable = true)\n",
            " |-- 17: double (nullable = true)\n",
            " |-- 18: double (nullable = true)\n",
            " |-- 19: double (nullable = true)\n",
            " |-- 20: double (nullable = true)\n",
            " |-- 21: double (nullable = true)\n",
            " |-- 22: double (nullable = true)\n",
            " |-- 23: double (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkGg9xJWZ2Bb",
        "colab_type": "text"
      },
      "source": [
        "# 使用 DataFrame.filter() 來進行row資料之條件計算 http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vtr4GV5bZ2Ba",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "ad5a0d28-eeea-475a-9430-778a9b1f692a"
      },
      "source": [
        "df.filter(df.hr_01>30).show()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+--------+-------+\n",
            "|      date|hr_01|hr_02|hr_03|hr_04|hr_05|hr_06|hr_07|hr_08|hr_09|hr_10|hr_11|hr_12|hr_13|hr_14|hr_15|hr_16|hr_17|hr_18|hr_19|hr_20|hr_21|hr_22|hr_23|hr_24|location|measure|\n",
            "+----------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+--------+-------+\n",
            "|2015/01/01| 53.0| 55.0| 58.0| 53.0| 43.0| 36.0| 35.0| 42.0| 55.0| 64.0| 65.0| 59.0| 52.0| 44.0| 47.0| 41.0| 43.0| 40.0| 42.0| 35.0| 28.0| 20.0| 18.0| 16.0|      大里|  PM2.5|\n",
            "|2015/01/03| 48.0| 48.0| 43.0| 38.0| 37.0| 36.0| 37.0| 34.0| 37.0| 46.0| 64.0| 77.0| 83.0| 75.0| 68.0| 69.0| 64.0| 65.0| 59.0| 66.0| 71.0| 66.0| 57.0| 48.0|      大里|  PM2.5|\n",
            "|2015/01/04| 60.0| 56.0| 53.0| 43.0| 53.0| 53.0| 52.0| 44.0| 44.0| 50.0| 49.0| 51.0| 45.0| 42.0| 40.0| 38.0| 36.0| 43.0| 51.0| 63.0| 68.0| 72.0| 66.0| 58.0|      大里|  PM2.5|\n",
            "|2015/01/05| 48.0| 42.0| 42.0| 34.0| 34.0| 28.0| 34.0| 35.0| 45.0| 47.0| 54.0| 46.0| 35.0| 19.0| 16.0| 21.0| 24.0| 28.0| 37.0| 52.0| 60.0| 62.0| 64.0| 61.0|      大里|  PM2.5|\n",
            "|2015/01/06| 59.0| 40.0| 34.0| 25.0| 27.0| 29.0| 26.0| 33.0| 42.0| 47.0| 38.0| 24.0| 14.0|  8.0| 17.0| 30.0| 51.0| 62.0| 68.0| 83.0| 83.0| 96.0|103.0|110.0|      大里|  PM2.5|\n",
            "|2015/01/09| 35.0| 34.0| 37.0| 30.0| 25.0| 25.0| 22.0| 21.0| 18.0| 20.0| 14.0| 12.0| 21.0| 31.0| 44.0| 46.0| 52.0| 44.0| 39.0| 37.0| 43.0| 43.0| 42.0| 39.0|      大里|  PM2.5|\n",
            "|2015/01/10| 38.0| 33.0| 31.0| 24.0| 20.0| 19.0| 22.0| 31.0| 31.0| 45.0| 48.0| 49.0| 38.0| 39.0| 43.0| 46.0| 43.0| 36.0| 33.0| 29.0| 37.0| 34.0| 39.0| 33.0|      大里|  PM2.5|\n",
            "|2015/01/11| 37.0| 41.0| 43.0| 43.0| 27.0| 22.0| 26.0| 34.0| 39.0| 37.0| 51.0| 53.0| 61.0| 56.0| 48.0| 43.0| 37.0| 43.0| 43.0| 48.0| 54.0| 51.0| 46.0| 35.0|      大里|  PM2.5|\n",
            "|2015/01/12| 36.0| 40.0| 33.0| 32.0| 33.0| 40.0| 37.0| 34.0| 39.0| 53.0| 60.0| 65.0| 57.0| 50.0| 52.0| 51.0| 43.0| 24.0| 20.0| 28.0| 35.0| 40.0| 30.0| 36.0|      大里|  PM2.5|\n",
            "|2015/01/13| 36.0| 36.0| 32.0| 33.0| 38.0| 45.0| 38.0| 45.0| 45.0| 76.0| 84.0| 96.0| 92.0| 87.0| 64.0| 33.0| 21.0| 22.0| 20.0| 15.0|  7.0| 12.0|  9.0| 11.0|      大里|  PM2.5|\n",
            "|2015/01/17| 42.0| 33.0| 25.0| 18.0| 13.0|  9.0| 12.0| 20.0| 28.0| 33.0| 33.0| 43.0| 52.0| 55.0| 57.0| 60.0| 66.0| 77.0| 76.0| 76.0| 77.0| 74.0| 82.0| 75.0|      大里|  PM2.5|\n",
            "|2015/01/18| 77.0| 66.0| 61.0| 62.0| 70.0| 71.0| 69.0| 71.0| 75.0| 82.0| 90.0| 94.0| 88.0| 75.0| 57.0| 47.0| 33.0| 31.0| 25.0| 21.0| 16.0| 16.0| 18.0| 16.0|      大里|  PM2.5|\n",
            "|2015/01/20| 62.0| 57.0| 52.0| 53.0| 61.0| 62.0| 62.0| 65.0| 73.0| 82.0| 83.0| 88.0| 83.0| 81.0| 73.0| 71.0| 69.0| 71.0| 66.0| 55.0| 49.0| 42.0| 49.0| 43.0|      大里|  PM2.5|\n",
            "|2015/01/21| 46.0| 37.0| 30.0| 31.0| 31.0| 38.0| 42.0| 46.0| 46.0| 36.0| 33.0| 33.0| 37.0| 34.0| 33.0| 33.0| 34.0| 39.0| 48.0| 50.0| 47.0| 45.0| 43.0| 39.0|      大里|  PM2.5|\n",
            "|2015/01/23| 61.0| 47.0| 46.0| 55.0| 49.0| 39.0| 29.0| 31.0| 29.0| 19.0| 12.0| 14.0| 30.0| 49.0| 57.0| 55.0| 57.0| 56.0| 64.0| 72.0| 73.0| 75.0| 68.0| 68.0|      大里|  PM2.5|\n",
            "|2015/01/24| 63.0| 61.0| 59.0| 58.0| 56.0| 52.0| 39.0| 33.0| 34.0| 40.0| 30.0| 33.0| 49.0| 66.0| 76.0| 75.0| 73.0| 60.0| 58.0| 67.0| 83.0| 88.0| 87.0| 80.0|      大里|  PM2.5|\n",
            "|2015/01/25| 71.0| 68.0| 65.0| 63.0| 63.0| 62.0| 57.0| 55.0| 65.0| 70.0| 79.0| 75.0| 75.0| 64.0| 59.0| 68.0| 78.0| 86.0| 80.0| 74.0| 66.0| 60.0| 49.0| 44.0|      大里|  PM2.5|\n",
            "|2015/01/26| 50.0| 53.0| 48.0| 43.0| 38.0| 40.0| 38.0| 47.0| 49.0| 59.0| 55.0| 53.0| 39.0| 29.0| 28.0| 38.0| 51.0| 55.0| 53.0| 55.0| 54.0| 61.0| 55.0| 61.0|      大里|  PM2.5|\n",
            "|2015/01/27| 56.0| 52.0| 41.0| 46.0| 49.0| 52.0| 43.0| 43.0| 48.0| 48.0| 40.0| 30.0| 30.0| 33.0| 36.0| 37.0| 35.0| 36.0| 43.0| 46.0| 42.0| 30.0| 22.0| 17.0|      大里|  PM2.5|\n",
            "|2015/02/07| 32.0| 32.0| 38.0| 43.0| 49.0| 48.0| 47.0| 54.0| 56.0| 60.0| 62.0| 66.0| 70.0| 73.0| 60.0| 46.0| 23.0| 25.0| 23.0| 40.0| 36.0| 32.0| 25.0| 29.0|      大里|  PM2.5|\n",
            "+----------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+--------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIOULjFJZ2Be",
        "colab_type": "text"
      },
      "source": [
        "# 使用 DataFrame.select() 來進行資料之Projection http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCXgch-_Z2Bc",
        "colab_type": "code",
        "outputId": "4dd2dc5e-e342-4797-f7aa-401ddc610df1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "df.filter(df.hr_01>50).select(\"hr_01\",\"location\",\"measure\").show()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------+-------+\n",
            "|hr_01|location|measure|\n",
            "+-----+--------+-------+\n",
            "| 53.0|      大里|  PM2.5|\n",
            "| 60.0|      大里|  PM2.5|\n",
            "| 59.0|      大里|  PM2.5|\n",
            "| 77.0|      大里|  PM2.5|\n",
            "| 62.0|      大里|  PM2.5|\n",
            "| 61.0|      大里|  PM2.5|\n",
            "| 63.0|      大里|  PM2.5|\n",
            "| 71.0|      大里|  PM2.5|\n",
            "| 56.0|      大里|  PM2.5|\n",
            "| 51.0|      大里|  PM2.5|\n",
            "| 54.0|      大里|  PM2.5|\n",
            "| 66.0|      大里|  PM2.5|\n",
            "| 67.0|      大里|  PM2.5|\n",
            "| 58.0|      大里|  PM2.5|\n",
            "| 52.0|      大里|  PM2.5|\n",
            "| 70.0|      大里|  PM2.5|\n",
            "| 83.0|      大里|  PM2.5|\n",
            "| 52.0|      大里|  PM2.5|\n",
            "| 51.0|      大里|  PM2.5|\n",
            "| 69.0|      大里|  PM2.5|\n",
            "+-----+--------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PI6sInWJZ2Bg",
        "colab_type": "text"
      },
      "source": [
        "## 使用 DataFrame.describe() 來進行DataFrame or Column 資料之統計 http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-vy-Ms1Z2Bh",
        "colab_type": "code",
        "outputId": "84433339-d42b-4f15-fe2d-b50e7ea133da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "df.describe().show()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+----------+-----------------+-----------------+-----------------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-----------------+------------------+--------+-------+\n",
            "|summary|      date|            hr_01|            hr_02|            hr_03|             hr_04|             hr_05|             hr_06|             hr_07|            hr_08|             hr_09|             hr_10|            hr_11|             hr_12|             hr_13|             hr_14|             hr_15|             hr_16|             hr_17|             hr_18|             hr_19|             hr_20|             hr_21|             hr_22|            hr_23|             hr_24|location|measure|\n",
            "+-------+----------+-----------------+-----------------+-----------------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-----------------+------------------+--------+-------+\n",
            "|  count|       198|              198|              198|              198|               198|               198|               198|               198|              198|               198|               198|              198|               198|               198|               198|               198|               198|               198|               198|               198|               198|               198|               198|              198|               198|     198|    198|\n",
            "|   mean|      null|30.04040404040404| 29.1010101010101| 28.1010101010101|27.065656565656564|25.853535353535353|25.055555555555557|24.883838383838384|26.51010101010101|29.636363636363637| 32.80808080808081|34.92424242424242| 35.43939393939394| 35.38383838383838| 34.59090909090909| 33.43434343434343|32.474747474747474|31.166666666666668|30.818181818181817| 30.63131313131313|31.747474747474747|32.505050505050505| 32.63636363636363|32.18181818181818|30.939393939393938|    null|   null|\n",
            "| stddev|      null|20.06533449925237|19.65669565958899|19.77410240385312|19.417683403396772|19.423512953586293|18.827346577025022|17.742708273179396| 17.9863851214172|18.715858009304785|19.908509910497596|19.82561841058815|20.238021803881413|20.277119245283668|20.342481822089674|19.235997143765555|18.886432395137792| 17.98046034768634| 18.04948844775545|17.872090066443693|19.357624678266657| 20.29271767424397|21.098249522238337|21.75118347272709| 21.03336396373215|    null|   null|\n",
            "|    min|2015/01/01|              2.0|              0.0|              0.0|               0.0|               0.0|               0.0|               0.0|              0.0|               0.0|               0.0|              0.0|               0.0|               0.0|               0.0|               0.0|               0.0|               0.0|               0.0|               0.0|               0.0|               0.0|               0.0|              0.0|               0.0|      大里|  PM2.5|\n",
            "|    max|2015/12/29|            114.0|            109.0|            107.0|             102.0|              98.0|              88.0|              74.0|             77.0|              86.0|              90.0|             90.0|              99.0|             111.0|             101.0|              94.0|             104.0|              89.0|              86.0|              88.0|              99.0|             101.0|              98.0|            107.0|             110.0|      大里|  PM2.5|\n",
            "+-------+----------+-----------------+-----------------+-----------------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-----------------+------------------+--------+-------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEcSbB_ZZ2BZ",
        "colab_type": "text"
      },
      "source": [
        "# 使用 DataFrame.agg() 來進行column數值之統計計算 http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UP5lLypj1P8T",
        "colab_type": "code",
        "outputId": "f213f517-936e-4b05-e402-97536ed0a23b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "source": [
        "df.groupBy().avg().show()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------+----------------+----------------+------------------+------------------+------------------+------------------+-----------------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+------------------+------------------+------------------+-----------------+------------------+------------------+-----------------+-----------------+------------------+\n",
            "|       avg(hr_01)|      avg(hr_02)|      avg(hr_03)|        avg(hr_04)|        avg(hr_05)|        avg(hr_06)|        avg(hr_07)|       avg(hr_08)|        avg(hr_09)|       avg(hr_10)|       avg(hr_11)|       avg(hr_12)|       avg(hr_13)|       avg(hr_14)|       avg(hr_15)|        avg(hr_16)|        avg(hr_17)|        avg(hr_18)|       avg(hr_19)|        avg(hr_20)|        avg(hr_21)|       avg(hr_22)|       avg(hr_23)|        avg(hr_24)|\n",
            "+-----------------+----------------+----------------+------------------+------------------+------------------+------------------+-----------------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+------------------+------------------+------------------+-----------------+------------------+------------------+-----------------+-----------------+------------------+\n",
            "|30.04040404040404|29.1010101010101|28.1010101010101|27.065656565656564|25.853535353535353|25.055555555555557|24.883838383838384|26.51010101010101|29.636363636363637|32.80808080808081|34.92424242424242|35.43939393939394|35.38383838383838|34.59090909090909|33.43434343434343|32.474747474747474|31.166666666666668|30.818181818181817|30.63131313131313|31.747474747474747|32.505050505050505|32.63636363636363|32.18181818181818|30.939393939393938|\n",
            "+-----------------+----------------+----------------+------------------+------------------+------------------+------------------+-----------------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+------------------+------------------+------------------+-----------------+------------------+------------------+-----------------+-----------------+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-PTxlE0Z2Be",
        "colab_type": "code",
        "outputId": "4432ceeb-8496-41f7-88a5-604257ecbb94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from pyspark.sql import functions as F\n",
        "df.agg(F.mean(df.hr_01),F.mean(df.hr_02)).show()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------+----------------+\n",
            "|       avg(hr_01)|      avg(hr_02)|\n",
            "+-----------------+----------------+\n",
            "|30.04040404040404|29.1010101010101|\n",
            "+-----------------+----------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "D3jtfh2MQHTY"
      },
      "source": [
        "#使用 DataFrame.withColumn() 來進行column新增column http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gz_KzHSQQbtB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "6cc0dfe4-4888-4ca8-d3e0-e565be38bb9a"
      },
      "source": [
        "df.withColumn('hr_01', df.hr_01+df.hr_02)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[date: string, hr_01: double, hr_02: double, hr_03: double, hr_04: double, hr_05: double, hr_06: double, hr_07: double, hr_08: double, hr_09: double, hr_10: double, hr_11: double, hr_12: double, hr_13: double, hr_14: double, hr_15: double, hr_16: double, hr_17: double, hr_18: double, hr_19: double, hr_20: double, hr_21: double, hr_22: double, hr_23: double, hr_24: double, location: string, measure: string]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzyEnb1kZ2CD",
        "colab_type": "text"
      },
      "source": [
        "# 練習1: 請算算看2015全國哪個測站，紫爆天數最多？"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kti4CDbZ2CC",
        "colab_type": "text"
      },
      "source": [
        "# 練習2: 請使用SPARK SQL求取2015年，全國pm2.5最高的前十個工作站測點以及其日期。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doUg4ZNDN3n4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycnCvcAVZ2Bi",
        "colab_type": "text"
      },
      "source": [
        "# 使用 Spark SQL來下達SQL查詢"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfO7cTEUZ2Bj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.registerTempTable(\"DaliTable\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSsE-3AWZ2Bl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4af8a21d-4c3f-4490-895e-0e7eedfd1577"
      },
      "source": [
        "spark.sql(\"\"\"\n",
        "            select * from DaliTable where date ='2015/02/07'\n",
        "          \"\"\").show()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR:root:An unexpected error occurred while tokenizing input\n",
            "The following traceback may be corrupted or invalid\n",
            "The error message is: ('EOF in multi-line string', (1, 10))\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o109.sql.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`date`' given input columns: [_8, _11, _21, _14, _12, _23, _7, _4, _27, _1, _25, _9, _6, _22, _24, _2, _5, _3, _18, _15, _26, _10, _17, _13, _16, _19, _20]; line 2 pos 42;\n'Project [*]\n+- 'Filter ('date = 2015/02/07)\n   +- SubqueryAlias dalitable\n      +- Project [_1#2648, _2#2649, _3#2650, _4#2733, _5#2762, _6#2791, _7#2820, _8#2849, _9#2878, _10#2907, _11#2936, _12#2965, _13#2994, _14#3023, _15#3052, _16#3081, _17#3110, _18#3139, _19#3168, _20#3197, _21#3226, _22#3255, _23#3284, _24#3313, ... 3 more fields]\n         +- Project [_1#2648, _2#2649, _3#2650, _4#2733, _5#2762, _6#2791, _7#2820, _8#2849, _9#2878, _10#2907, _11#2936, _12#2965, _13#2994, _14#3023, _15#3052, _16#3081, _17#3110, _18#3139, _19#3168, _20#3197, _21#3226, _22#3255, _23#3284, _24#3313, ... 3 more fields]\n            +- Project [_1#2648, _2#2649, _3#2650, _4#2733, _5#2762, _6#2791, _7#2820, _8#2849, _9#2878, _10#2907, _11#2936, _12#2965, _13#2994, _14#3023, _15#3052, _16#3081, _17#3110, _18#3139, _19#3168, _20#3197, _21#3226, _22#3255, _23#3284, _24#3313, ... 3 more fields]\n               +- Project [_1#2648, _2#2649, _3#2650, _4#2733, _5#2762, _6#2791, _7#2820, _8#2849, _9#2878, _10#2907, _11#2936, _12#2965, _13#2994, _14#3023, _15#3052, _16#3081, _17#3110, _18#3139, _19#3168, _20#3197, _21#3226, _22#3255, _23#3284, cast(_24#2671 as double) AS _24#3313, ... 3 more fields]\n                  +- Project [_1#2648, _2#2649, _3#2650, _4#2733, _5#2762, _6#2791, _7#2820, _8#2849, _9#2878, _10#2907, _11#2936, _12#2965, _13#2994, _14#3023, _15#3052, _16#3081, _17#3110, _18#3139, _19#3168, _20#3197, _21#3226, _22#3255, cast(_23#2670 as double) AS _23#3284, _24#2671, ... 3 more fields]\n                     +- Project [_1#2648, _2#2649, _3#2650, _4#2733, _5#2762, _6#2791, _7#2820, _8#2849, _9#2878, _10#2907, _11#2936, _12#2965, _13#2994, _14#3023, _15#3052, _16#3081, _17#3110, _18#3139, _19#3168, _20#3197, _21#3226, cast(_22#2669 as double) AS _22#3255, _23#2670, _24#2671, ... 3 more fields]\n                        +- Project [_1#2648, _2#2649, _3#2650, _4#2733, _5#2762, _6#2791, _7#2820, _8#2849, _9#2878, _10#2907, _11#2936, _12#2965, _13#2994, _14#3023, _15#3052, _16#3081, _17#3110, _18#3139, _19#3168, _20#3197, cast(_21#2668 as double) AS _21#3226, _22#2669, _23#2670, _24#2671, ... 3 more fields]\n                           +- Project [_1#2648, _2#2649, _3#2650, _4#2733, _5#2762, _6#2791, _7#2820, _8#2849, _9#2878, _10#2907, _11#2936, _12#2965, _13#2994, _14#3023, _15#3052, _16#3081, _17#3110, _18#3139, _19#3168, cast(_20#2667 as double) AS _20#3197, _21#2668, _22#2669, _23#2670, _24#2671, ... 3 more fields]\n                              +- Project [_1#2648, _2#2649, _3#2650, _4#2733, _5#2762, _6#2791, _7#2820, _8#2849, _9#2878, _10#2907, _11#2936, _12#2965, _13#2994, _14#3023, _15#3052, _16#3081, _17#3110, _18#3139, cast(_19#2666 as double) AS _19#3168, _20#2667, _21#2668, _22#2669, _23#2670, _24#2671, ... 3 more fields]\n                                 +- Project [_1#2648, _2#2649, _3#2650, _4#2733, _5#2762, _6#2791, _7#2820, _8#2849, _9#2878, _10#2907, _11#2936, _12#2965, _13#2994, _14#3023, _15#3052, _16#3081, _17#3110, cast(_18#2665 as double) AS _18#3139, _19#2666, _20#2667, _21#2668, _22#2669, _23#2670, _24#2671, ... 3 more fields]\n                                    +- Project [_1#2648, _2#2649, _3#2650, _4#2733, _5#2762, _6#2791, _7#2820, _8#2849, _9#2878, _10#2907, _11#2936, _12#2965, _13#2994, _14#3023, _15#3052, _16#3081, cast(_17#2664 as double) AS _17#3110, _18#2665, _19#2666, _20#2667, _21#2668, _22#2669, _23#2670, _24#2671, ... 3 more fields]\n                                       +- Project [_1#2648, _2#2649, _3#2650, _4#2733, _5#2762, _6#2791, _7#2820, _8#2849, _9#2878, _10#2907, _11#2936, _12#2965, _13#2994, _14#3023, _15#3052, cast(_16#2663 as double) AS _16#3081, _17#2664, _18#2665, _19#2666, _20#2667, _21#2668, _22#2669, _23#2670, _24#2671, ... 3 more fields]\n                                          +- Project [_1#2648, _2#2649, _3#2650, _4#2733, _5#2762, _6#2791, _7#2820, _8#2849, _9#2878, _10#2907, _11#2936, _12#2965, _13#2994, _14#3023, cast(_15#2662 as double) AS _15#3052, _16#2663, _17#2664, _18#2665, _19#2666, _20#2667, _21#2668, _22#2669, _23#2670, _24#2671, ... 3 more fields]\n                                             +- Project [_1#2648, _2#2649, _3#2650, _4#2733, _5#2762, _6#2791, _7#2820, _8#2849, _9#2878, _10#2907, _11#2936, _12#2965, _13#2994, cast(_14#2661 as double) AS _14#3023, _15#2662, _16#2663, _17#2664, _18#2665, _19#2666, _20#2667, _21#2668, _22#2669, _23#2670, _24#2671, ... 3 more fields]\n                                                +- Project [_1#2648, _2#2649, _3#2650, _4#2733, _5#2762, _6#2791, _7#2820, _8#2849, _9#2878, _10#2907, _11#2936, _12#2965, cast(_13#2660 as double) AS _13#2994, _14#2661, _15#2662, _16#2663, _17#2664, _18#2665, _19#2666, _20#2667, _21#2668, _22#2669, _23#2670, _24#2671, ... 3 more fields]\n                                                   +- Project [_1#2648, _2#2649, _3#2650, _4#2733, _5#2762, _6#2791, _7#2820, _8#2849, _9#2878, _10#2907, _11#2936, cast(_12#2659 as double) AS _12#2965, _13#2660, _14#2661, _15#2662, _16#2663, _17#2664, _18#2665, _19#2666, _20#2667, _21#2668, _22#2669, _23#2670, _24#2671, ... 3 more fields]\n                                                      +- Project [_1#2648, _2#2649, _3#2650, _4#2733, _5#2762, _6#2791, _7#2820, _8#2849, _9#2878, _10#2907, cast(_11#2658 as double) AS _11#2936, _12#2659, _13#2660, _14#2661, _15#2662, _16#2663, _17#2664, _18#2665, _19#2666, _20#2667, _21#2668, _22#2669, _23#2670, _24#2671, ... 3 more fields]\n                                                         +- Project [_1#2648, _2#2649, _3#2650, _4#2733, _5#2762, _6#2791, _7#2820, _8#2849, _9#2878, cast(_10#2657 as double) AS _10#2907, _11#2658, _12#2659, _13#2660, _14#2661, _15#2662, _16#2663, _17#2664, _18#2665, _19#2666, _20#2667, _21#2668, _22#2669, _23#2670, _24#2671, ... 3 more fields]\n                                                            +- Project [_1#2648, _2#2649, _3#2650, _4#2733, _5#2762, _6#2791, _7#2820, _8#2849, cast(_9#2656 as double) AS _9#2878, _10#2657, _11#2658, _12#2659, _13#2660, _14#2661, _15#2662, _16#2663, _17#2664, _18#2665, _19#2666, _20#2667, _21#2668, _22#2669, _23#2670, _24#2671, ... 3 more fields]\n                                                               +- Project [_1#2648, _2#2649, _3#2650, _4#2733, _5#2762, _6#2791, _7#2820, cast(_8#2655 as double) AS _8#2849, _9#2656, _10#2657, _11#2658, _12#2659, _13#2660, _14#2661, _15#2662, _16#2663, _17#2664, _18#2665, _19#2666, _20#2667, _21#2668, _22#2669, _23#2670, _24#2671, ... 3 more fields]\n                                                                  +- Project [_1#2648, _2#2649, _3#2650, _4#2733, _5#2762, _6#2791, cast(_7#2654 as double) AS _7#2820, _8#2655, _9#2656, _10#2657, _11#2658, _12#2659, _13#2660, _14#2661, _15#2662, _16#2663, _17#2664, _18#2665, _19#2666, _20#2667, _21#2668, _22#2669, _23#2670, _24#2671, ... 3 more fields]\n                                                                     +- Project [_1#2648, _2#2649, _3#2650, _4#2733, _5#2762, cast(_6#2653 as double) AS _6#2791, _7#2654, _8#2655, _9#2656, _10#2657, _11#2658, _12#2659, _13#2660, _14#2661, _15#2662, _16#2663, _17#2664, _18#2665, _19#2666, _20#2667, _21#2668, _22#2669, _23#2670, _24#2671, ... 3 more fields]\n                                                                        +- Project [_1#2648, _2#2649, _3#2650, _4#2733, cast(_5#2652 as double) AS _5#2762, _6#2653, _7#2654, _8#2655, _9#2656, _10#2657, _11#2658, _12#2659, _13#2660, _14#2661, _15#2662, _16#2663, _17#2664, _18#2665, _19#2666, _20#2667, _21#2668, _22#2669, _23#2670, _24#2671, ... 3 more fields]\n                                                                           +- Project [_1#2648, _2#2649, _3#2650, cast(_4#2651 as double) AS _4#2733, _5#2652, _6#2653, _7#2654, _8#2655, _9#2656, _10#2657, _11#2658, _12#2659, _13#2660, _14#2661, _15#2662, _16#2663, _17#2664, _18#2665, _19#2666, _20#2667, _21#2668, _22#2669, _23#2670, _24#2671, ... 3 more fields]\n                                                                              +- LogicalRDD [_1#2648, _2#2649, _3#2650, _4#2651, _5#2652, _6#2653, _7#2654, _8#2655, _9#2656, _10#2657, _11#2658, _12#2659, _13#2660, _14#2661, _15#2662, _16#2663, _17#2664, _18#2665, _19#2666, _20#2667, _21#2668, _22#2669, _23#2670, _24#2671, ... 3 more fields]\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:88)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:279)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:289)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$6.apply(QueryPlan.scala:298)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:298)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:268)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:78)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:78)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:91)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:52)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:623)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-fbee181fab89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m spark.sql(\"\"\"\n\u001b[1;32m      2\u001b[0m             \u001b[0mselect\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mDaliTable\u001b[0m \u001b[0mwhere\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'2015/02/07'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m           \"\"\").show()\n\u001b[0m",
            "\u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \"\"\"\n\u001b[0;32m--> 556\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`date`' given input columns: [_8, _11, _21, _14, _12, _23, _7, _4, _27, _1, _25, _9, _6, _22, _24, _2, _5, _3, _18, _15, _26, _10, _17, _13, _16, _19, _20]; line 2 pos 42;\\n'Project [*]\\n+- 'Filter ('date = 2015/02/07)\\n   +- SubqueryAlias dalitable\\n      +- Project [_1#2648, _2#2649, _3#2650, _4#2733, _5#2762, _6#2791, _7#2820, _8#2849, _9#2878, _10#2907, _11#2936, _12#2965, _13#2994, _14#3023, _15#3052, _16#3081, _17#3110, _18#3139, _19#3168, _20#3197, _21#3226, _22#3255, _23#3284, _24#3313, ... 3 more fields]\\n         +- Project [_1#2648, _2#2649, _3#2650, _4#2733, _5#2762, _6#2791, _7#2820, _8#2849, _9#2878, _10#2907, _11#2936, _12#2965, _13#2994, _14#3023, _15#3052, _16#3081, _17#3110, _18#3139, _19#3168, _20#3197, _21#3226, _22#3255, _23#3284, _24#3313, ... 3 more fields]\\n            +- Project [_1#2648, _2#2649, _3#2650, _4#2733, _5#2762, _6#2791, _7#2820, _8#2849, _9#2878, _10#2907, _11#2936, _12#2965, _13#2994, _14#3023, _15#3052, _16#3081, _17#3110, _18#3139, _19#3168, _20#3197, _21#3226, _22#3255, _23#3284, _24#3313, ... 3 more fields]\\n               +- Project [_1#2648, _2#2649, _3#2650, _4#2733, _5#2762, _6#2791, _7#2820, _8#2849, _9#2878, _10#2907, _11#2936, _12#2965, _13#2994, _14#3023, _15#3052, _16#3081, _17#3110, _18#3139, _19#3168, _20#3197, _21#3226, _22#3255, _23#3284, cast(_24#2671 as double) AS _24#3313, ... 3 more fields]\\n                  +- Project [_1#2648, _2#2649, _3#2650, _4#2733, _5#2762, _6..."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66vVEpvUZ2Bo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spark.sql(\"\"\"\n",
        "                select count(*) count \n",
        "                from DaliTable \n",
        "                where hr_01 > 100\n",
        "               \"\"\").show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vxauml1-Z2Bt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spark.sql(\"\"\"\n",
        "                select AVG(hr_01) count \n",
        "                from DaliTable\n",
        "               \"\"\").show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jux8542WZ2Bv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spark.sql(\"\"\"\n",
        "                select date,location, hr_01, hr_02, hr_01-hr_02 as diff \n",
        "                from DaliTable\n",
        "          \"\"\").show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgoKjn6PZ2Bw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spark.sql(\"\"\"\n",
        "                select date,location, hr_01, hr_02, hr_01+hr_02 as diff \n",
        "                from DaliTable \n",
        "                order by diff DESC\n",
        "                \"\"\").show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dzr5S_ysZ2By",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spark.sql(\"\"\"\n",
        "                select date,location, hr_01+hr_02+hr_03+hr_04+hr_05+hr_06+hr_07+hr_08 as sum \n",
        "                from DaliTable \n",
        "                order by sum DESC\n",
        "                \"\"\").show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVcomNQfZ2B0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HryYFAWXZ2B1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pMpQ-zrB11pp"
      },
      "source": [
        "# 利用DataFrame 直接讀取CSV, Fillna, DropNa功能清洗資料"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jO_EEY1011pq",
        "colab": {}
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .getOrCreate()\n",
        "\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark.sql import Row"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9pDYpUS811ps",
        "colab": {}
      },
      "source": [
        "df = spark.read.load(\"./pm25.csv\", format=\"csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "4797fe85-1105-49ed-fcf0-59d11a462d96",
        "id": "cBn5I6iY11pt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+---+----------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
            "|       _c0|_c1|       _c2| _c3| _c4| _c5| _c6| _c7| _c8| _c9|_c10|_c11|_c12|_c13|_c14|_c15|_c16|_c17|_c18|_c19|_c20|_c21|_c22|_c23|_c24|_c25|_c26|\n",
            "+----------+---+----------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
            "|        日期| 測站|        測項|  00|  01|  02|  03|  04|  05|  06|  07|  08|  09|  10|  11|  12|  13|  14|  15|  16|  17|  18|  19|  20|  21|  22|  23|\n",
            "|2015/01/01| 龍潭|  AMB_TEMP|  14|  14|  14|  13|  13|  13|  12|  12|  13|  14|  14|  14|  14|  14|  13|  13|  12|  11|  11|  11|  11|  11|  11|  11|\n",
            "|2015/01/01| 龍潭|        CO|0.69|0.72|0.69|0.64|0.54|0.47|0.45|0.48|0.51|0.54|0.54| 0.5|0.47|0.38|0.36|0.35|0.34|0.37|0.34|0.29|0.26|0.22|0.19|0.18|\n",
            "|2015/01/01| 龍潭|        NO| 0.3| 0.1| 0.6|   2|   2| 1.9| 2.2| 3.1| 3.7| 4.3| 4.3| 4.5| 3.3| 4.1| 3.1| 3.6| 3.6| 2.8| 2.8| 2.5| 2.2| 1.4| 2.1|   2|\n",
            "|2015/01/01| 龍潭|       NO2|  11| 9.6| 8.7| 9.1| 9.6| 9.9|  11|  13|  11|  12|  12|  11|  11| 9.9| 9.9|  10|  11|  13|  11|  10| 8.2| 7.3| 6.5| 5.5|\n",
            "|2015/01/01| 龍潭|       NOx|  11| 9.7| 9.3|  11|  12|  12|  13|  17|  15|  16|  16|  16|  14|  14|  13|  14|  15|  16|  14|  13|  10| 8.7| 8.6| 7.5|\n",
            "|2015/01/01| 龍潭|        O3|  44|  43|  43|  44|  41|  39|  38|  34|  37|  37|  39|  44|  47|  49|  48|  44|  39|  37|  37|  39|  39|  38|  38|  39|\n",
            "|2015/01/01| 龍潭|      PM10| 106| 138| 152| 152| 143| 128| 115| 106| 102| 105| 108| 114| 108|  96|  82|  74|  72|  72|  70|  60|  50|  37|  38|  40|\n",
            "|2015/01/01| 龍潭|     PM2.5|  46|  71|  76|  74|  65|  62|  56|  50|  52|  56|  54|  47|  40|  36|  37|  27|  30|  25|  26|  24|  18|  16|  11|  14|\n",
            "|2015/01/01| 龍潭|  RAINFALL|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|\n",
            "|2015/01/01| 龍潭|        RH|  70|  69|  70|  70|  71|  72|  70|  69|  64|  60|  57|  53|  52|  52|  53|  56|  59|  60|  63|  62|  60|  62|  65|  64|\n",
            "|2015/01/01| 龍潭|       SO2| 8.6| 8.5| 7.6| 6.7| 7.4| 6.4| 6.2| 7.2| 6.7| 6.5| 5.5| 4.8| 4.1| 3.2| 2.8| 2.8| 3.1| 2.8| 2.5|   2| 1.6| 1.5| 1.4| 1.2|\n",
            "|2015/01/01| 龍潭|     WD_HR|  66|  70|  69|  68|  67|  72|  74|  72|  66|  66|  63|  60|  59|  62|  59|  63|  64|  62|  62|  62|  59|  59|  59|  61|\n",
            "|2015/01/01| 龍潭|WIND_DIREC|  67|  70|  68|  67|  66|  75|  75|  73|  68|  64|  59|  59|  65|  62|  60|  68|  60|  61|  62|  61|  59|  63|  65|  58|\n",
            "|2015/01/01| 龍潭|WIND_SPEED| 8.7| 7.5| 7.7| 7.2| 6.9| 5.3| 6.5| 6.4|   7| 7.1| 7.5| 7.3| 6.9| 7.4|   8| 8.3| 6.6| 7.3| 7.1|   7| 6.9| 7.5| 6.8| 5.8|\n",
            "|2015/01/01| 龍潭|     WS_HR|   8| 7.7| 7.2| 6.9|   7| 6.6| 6.3| 6.2| 7.1|   7| 7.1| 7.6| 7.5| 7.5| 7.9| 7.9| 7.2| 6.9| 7.2| 6.9| 6.7| 6.9| 6.9| 5.7|\n",
            "|2015/01/02| 龍潭|  AMB_TEMP|  11|  11|  11|  11|  11|  11|  11|  11|  13|  14|  15|  16|  16|  16|  16|  16|  15|  14|  13|  13|  13|  12|  12|  12|\n",
            "|2015/01/02| 龍潭|        CO|0.17|0.17|0.18|0.18|0.23|0.24|0.26|0.33|0.34|0.35|0.35|0.34|0.32|0.31| 0.3|0.32| 0.3|0.34|0.33| 0.3|0.28|0.28|0.27|0.26|\n",
            "|2015/01/02| 龍潭|        NO| 2.2| 1.6| 1.7| 1.8|   2| 1.9| 2.5| 3.4| 4.5|   5| 4.6| 4.6| 4.3|   4|   4| 3.6| 3.6| 3.1| 2.7| 2.6| 2.9| 2.3| 1.9| 2.2|\n",
            "|2015/01/02| 龍潭|       NO2|   5| 5.4| 5.1| 5.1| 5.6|   7| 7.5| 9.6| 9.7| 9.5| 7.8| 7.4| 7.1| 7.1| 7.9| 9.2|  11|  13|  12|  10| 9.2| 8.5| 8.8| 9.2|\n",
            "+----------+---+----------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P2Du-yrk11pw",
        "colab": {}
      },
      "source": [
        "df2=df.filter(df[\"_c0\"]!=\"日期\") "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "8cd77220-d96a-4487-e700-1caad78fbd85",
        "id": "4eRUGtKK11py",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df2.count()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "465317"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "9a74cfa8-3d4c-4815-cb83-555477c6b28e",
        "id": "zcL55Mg211p0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df2.dropna()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string, _c9: string, _c10: string, _c11: string, _c12: string, _c13: string, _c14: string, _c15: string, _c16: string, _c17: string, _c18: string, _c19: string, _c20: string, _c21: string, _c22: string, _c23: string, _c24: string, _c25: string, _c26: string]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ef4f82d1-8d77-4a83-8fd0-e6bf3e319165",
        "id": "9CN4CwP_11p2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df2.count()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "465317"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "1074ebd2-6695-47a3-a420-a76998f788ef",
        "id": "yOg8NZWA11p3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df2.filter(df2._c3.isNotNull()).show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+---+----------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
            "|       _c0|_c1|       _c2| _c3| _c4| _c5| _c6| _c7| _c8| _c9|_c10|_c11|_c12|_c13|_c14|_c15|_c16|_c17|_c18|_c19|_c20|_c21|_c22|_c23|_c24|_c25|_c26|\n",
            "+----------+---+----------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
            "|2015/01/01| 龍潭|  AMB_TEMP|  14|  14|  14|  13|  13|  13|  12|  12|  13|  14|  14|  14|  14|  14|  13|  13|  12|  11|  11|  11|  11|  11|  11|  11|\n",
            "|2015/01/01| 龍潭|        CO|0.69|0.72|0.69|0.64|0.54|0.47|0.45|0.48|0.51|0.54|0.54| 0.5|0.47|0.38|0.36|0.35|0.34|0.37|0.34|0.29|0.26|0.22|0.19|0.18|\n",
            "|2015/01/01| 龍潭|        NO| 0.3| 0.1| 0.6|   2|   2| 1.9| 2.2| 3.1| 3.7| 4.3| 4.3| 4.5| 3.3| 4.1| 3.1| 3.6| 3.6| 2.8| 2.8| 2.5| 2.2| 1.4| 2.1|   2|\n",
            "|2015/01/01| 龍潭|       NO2|  11| 9.6| 8.7| 9.1| 9.6| 9.9|  11|  13|  11|  12|  12|  11|  11| 9.9| 9.9|  10|  11|  13|  11|  10| 8.2| 7.3| 6.5| 5.5|\n",
            "|2015/01/01| 龍潭|       NOx|  11| 9.7| 9.3|  11|  12|  12|  13|  17|  15|  16|  16|  16|  14|  14|  13|  14|  15|  16|  14|  13|  10| 8.7| 8.6| 7.5|\n",
            "|2015/01/01| 龍潭|        O3|  44|  43|  43|  44|  41|  39|  38|  34|  37|  37|  39|  44|  47|  49|  48|  44|  39|  37|  37|  39|  39|  38|  38|  39|\n",
            "|2015/01/01| 龍潭|      PM10| 106| 138| 152| 152| 143| 128| 115| 106| 102| 105| 108| 114| 108|  96|  82|  74|  72|  72|  70|  60|  50|  37|  38|  40|\n",
            "|2015/01/01| 龍潭|     PM2.5|  46|  71|  76|  74|  65|  62|  56|  50|  52|  56|  54|  47|  40|  36|  37|  27|  30|  25|  26|  24|  18|  16|  11|  14|\n",
            "|2015/01/01| 龍潭|  RAINFALL|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|\n",
            "|2015/01/01| 龍潭|        RH|  70|  69|  70|  70|  71|  72|  70|  69|  64|  60|  57|  53|  52|  52|  53|  56|  59|  60|  63|  62|  60|  62|  65|  64|\n",
            "|2015/01/01| 龍潭|       SO2| 8.6| 8.5| 7.6| 6.7| 7.4| 6.4| 6.2| 7.2| 6.7| 6.5| 5.5| 4.8| 4.1| 3.2| 2.8| 2.8| 3.1| 2.8| 2.5|   2| 1.6| 1.5| 1.4| 1.2|\n",
            "|2015/01/01| 龍潭|     WD_HR|  66|  70|  69|  68|  67|  72|  74|  72|  66|  66|  63|  60|  59|  62|  59|  63|  64|  62|  62|  62|  59|  59|  59|  61|\n",
            "|2015/01/01| 龍潭|WIND_DIREC|  67|  70|  68|  67|  66|  75|  75|  73|  68|  64|  59|  59|  65|  62|  60|  68|  60|  61|  62|  61|  59|  63|  65|  58|\n",
            "|2015/01/01| 龍潭|WIND_SPEED| 8.7| 7.5| 7.7| 7.2| 6.9| 5.3| 6.5| 6.4|   7| 7.1| 7.5| 7.3| 6.9| 7.4|   8| 8.3| 6.6| 7.3| 7.1|   7| 6.9| 7.5| 6.8| 5.8|\n",
            "|2015/01/01| 龍潭|     WS_HR|   8| 7.7| 7.2| 6.9|   7| 6.6| 6.3| 6.2| 7.1|   7| 7.1| 7.6| 7.5| 7.5| 7.9| 7.9| 7.2| 6.9| 7.2| 6.9| 6.7| 6.9| 6.9| 5.7|\n",
            "|2015/01/02| 龍潭|  AMB_TEMP|  11|  11|  11|  11|  11|  11|  11|  11|  13|  14|  15|  16|  16|  16|  16|  16|  15|  14|  13|  13|  13|  12|  12|  12|\n",
            "|2015/01/02| 龍潭|        CO|0.17|0.17|0.18|0.18|0.23|0.24|0.26|0.33|0.34|0.35|0.35|0.34|0.32|0.31| 0.3|0.32| 0.3|0.34|0.33| 0.3|0.28|0.28|0.27|0.26|\n",
            "|2015/01/02| 龍潭|        NO| 2.2| 1.6| 1.7| 1.8|   2| 1.9| 2.5| 3.4| 4.5|   5| 4.6| 4.6| 4.3|   4|   4| 3.6| 3.6| 3.1| 2.7| 2.6| 2.9| 2.3| 1.9| 2.2|\n",
            "|2015/01/02| 龍潭|       NO2|   5| 5.4| 5.1| 5.1| 5.6|   7| 7.5| 9.6| 9.7| 9.5| 7.8| 7.4| 7.1| 7.1| 7.9| 9.2|  11|  13|  12|  10| 9.2| 8.5| 8.8| 9.2|\n",
            "|2015/01/02| 龍潭|       NOx| 7.2|   7| 6.8| 6.9| 7.6|   9|  10|  13|  14|  14|  12|  12|  11|  11|  12|  13|  14|  16|  15|  13|  12|  11|  11|  11|\n",
            "+----------+---+----------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xeL-EKam11p5",
        "colab": {}
      },
      "source": [
        "for i in df2.columns[3:]:\n",
        "  df2 = df2.withColumn(i, df2[i].cast(\"double\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "931e2706-f384-4e3d-e49b-36960d8fec74",
        "id": "TCw-zU4211p6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df2.dropna().count()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "366789"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y6GLXSxK11p7",
        "colab": {}
      },
      "source": [
        "df2 = df2.filter(df2[\"_c2\"]!=\"RAINFALL\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wUx3JyvT11p9",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "coU6Cj_S11p_",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WbM7XshZ2B2",
        "colab_type": "text"
      },
      "source": [
        "# Perform Pearson Correlation using DataFrame.corr( )\n",
        "# 練習: 計算大里區pm10, pm2.5 間之關聯度 http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame\n",
        "使用前面所建立的clean_weather_data rdd資料\n",
        "    \n",
        "    \n",
        "    corr(col1, col2, method=None)\n",
        "    Calculates the correlation of two columns of a DataFrame as a double value. Currently only supports the Pearson Correlation Coefficient. DataFrame.corr() and DataFrameStatFunctions.corr() are aliases of each other.\n",
        "\n",
        "    Parameters:\t\n",
        "    col1 – The name of the first column\n",
        "    col2 – The name of the second column\n",
        "    method – The correlation method. Currently only supports “pearson”\n",
        "    New in version 1.4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW5zsuLAZ2B3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Generated_Measurement(x):\n",
        "    date = x[0]\n",
        "    location = x[1]\n",
        "    measure = x[2]\n",
        "    measurements_of_a_day = []\n",
        "    for i, value in enumerate(x[3:]):\n",
        "        measurements_of_a_day.append((date, measure, \"hr_\"+str(i), value))\n",
        "    return measurements_of_a_day\n",
        "\n",
        "daliData = clean_weather_data.filter(lambda x: x[1]==\"大里\" and (x[2] == \"PM2.5\" or x[2] == \"PM10\" ))\n",
        "daliData.cache()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4y8sIyUD5sBg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "daliData.toDF().show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49larQ_8Z2B4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "daliDataRow = clean_weather_data.filter(lambda x: x[1]==\"大里\" and (x[2] == \"PM2.5\" or x[2] == \"PM10\" ))\\\n",
        "    .flatMap(Generated_Measurement)\\\n",
        "    .map(lambda x: ( (x[0], x[2]), x[1], x[3] ) )\\\n",
        "    .groupBy(lambda x: x[0])\\\n",
        "    .filter(lambda x: len(x[1])==2)\\\n",
        "    .mapValues(lambda x: list(x))\\\n",
        "    .mapValues(lambda x: [x[0][1], x[0][2], x[1][1], x[1][2]])\\\n",
        "    .map(lambda x:[ x[0][0], x[0][1], x[1][1], x[1][3]])\\\n",
        "    .map(lambda x: Row(\n",
        "            date = x[0],\n",
        "            time = x[1],\n",
        "            pm10 = float(x[2]),\n",
        "            pm25 = float(x[3])\n",
        "        ))\n",
        "    \n",
        "df = spark.createDataFrame(daliDataRow)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0vjzrg0Z2B6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.show(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7OPjFeNZ2B7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.groupBy().avg().show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOCH_rb-Z2B-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.corr(\"pm10\",\"pm25\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVaWYyQ_Z2CA",
        "colab_type": "code",
        "outputId": "ece851c9-d481-46b7-e4a3-450c92ed1f05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        }
      },
      "source": [
        "df.describe().show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-149-dde026cdb0e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mdescribe\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m    947\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m             \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    950\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1133\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
            "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FukeJJoY11Xq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}