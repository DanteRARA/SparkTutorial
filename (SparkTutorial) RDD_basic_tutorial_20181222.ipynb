{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RDD basic tutorial.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "cF0hz0LrunvS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 25850
        },
        "outputId": "331d249e-3143-47e8-cca3-d553291c011c"
      },
      "cell_type": "code",
      "source": [
        "# 環境初始化 (大約三至五分鐘)\n",
        "! wget -O init_env.sh https://www.dropbox.com/s/6bnwn8u2hz19s59/init_env.sh && \\\n",
        "bash init_env.sh"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-12-22 06:18:57--  https://www.dropbox.com/s/6bnwn8u2hz19s59/init_env.sh\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.9.1, 2620:100:601b:1::a27d:801\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.9.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/6bnwn8u2hz19s59/init_env.sh [following]\n",
            "--2018-12-22 06:18:57--  https://www.dropbox.com/s/raw/6bnwn8u2hz19s59/init_env.sh\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc5fb5d72594e29bb603e7bf6856.dl.dropboxusercontent.com/cd/0/inline/AX420C3NT94NtcQJ6ssGyVtwmKw20wdrV7U2UJ-cqg3wWBRY2wJd8nOPEkPcPHkpThT4jUJyuQyz39eYbRl1d6IV8DnTMboeKlnT2KNOIWmbhHZXjahKXPHnT3RHIdH-Azl05yBMFkv6EPSpkpvGwuHd7R3TSARYxRy1JAw2nZL4Nj5pblQSw1DuahTpjYnVpEE/file [following]\n",
            "--2018-12-22 06:18:57--  https://uc5fb5d72594e29bb603e7bf6856.dl.dropboxusercontent.com/cd/0/inline/AX420C3NT94NtcQJ6ssGyVtwmKw20wdrV7U2UJ-cqg3wWBRY2wJd8nOPEkPcPHkpThT4jUJyuQyz39eYbRl1d6IV8DnTMboeKlnT2KNOIWmbhHZXjahKXPHnT3RHIdH-Azl05yBMFkv6EPSpkpvGwuHd7R3TSARYxRy1JAw2nZL4Nj5pblQSw1DuahTpjYnVpEE/file\n",
            "Resolving uc5fb5d72594e29bb603e7bf6856.dl.dropboxusercontent.com (uc5fb5d72594e29bb603e7bf6856.dl.dropboxusercontent.com)... 162.125.8.6, 2620:100:601f:6::a27d:906\n",
            "Connecting to uc5fb5d72594e29bb603e7bf6856.dl.dropboxusercontent.com (uc5fb5d72594e29bb603e7bf6856.dl.dropboxusercontent.com)|162.125.8.6|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 336 [text/plain]\n",
            "Saving to: ‘init_env.sh’\n",
            "\n",
            "init_env.sh         100%[===================>]     336  --.-KB/s    in 0s      \n",
            "\n",
            "2018-12-22 06:18:58 (31.0 MB/s) - ‘init_env.sh’ saved [336/336]\n",
            "\n",
            "--2018-12-22 06:18:58--  https://d3kbcqa49mib13.cloudfront.net/spark-2.2.0-bin-hadoop2.7.tgz\n",
            "Resolving d3kbcqa49mib13.cloudfront.net (d3kbcqa49mib13.cloudfront.net)... 13.32.81.210, 13.32.81.43, 13.32.81.152, ...\n",
            "Connecting to d3kbcqa49mib13.cloudfront.net (d3kbcqa49mib13.cloudfront.net)|13.32.81.210|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 203728858 (194M) [application/x-tar]\n",
            "Saving to: ‘spark-2.2.0-bin-hadoop2.7.tgz’\n",
            "\n",
            "spark-2.2.0-bin-had 100%[===================>] 194.29M  94.6MB/s    in 2.1s    \n",
            "\n",
            "2018-12-22 06:19:00 (94.6 MB/s) - ‘spark-2.2.0-bin-hadoop2.7.tgz’ saved [203728858/203728858]\n",
            "\n",
            "spark-2.2.0-bin-hadoop2.7/\n",
            "spark-2.2.0-bin-hadoop2.7/NOTICE\n",
            "spark-2.2.0-bin-hadoop2.7/jars/\n",
            "spark-2.2.0-bin-hadoop2.7/jars/parquet-common-1.8.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/bonecp-0.8.0.RELEASE.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-net-2.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/javax.servlet-api-3.1.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-annotations-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-hdfs-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/oro-2.0.8.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/xercesImpl-2.9.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/antlr-runtime-3.4.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/machinist_2.11-0.6.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spire_2.11-0.13.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/parquet-hadoop-1.8.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-sketch_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/stream-2.7.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/kryo-shaded-3.0.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/metrics-jvm-3.1.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-mapreduce-client-common-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/RoaringBitmap-0.5.11.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-auth-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-common-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/activation-1.1.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jta-1.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/datanucleus-core-3.2.10.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jersey-container-servlet-2.22.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jersey-guava-2.22.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jets3t-0.9.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jetty-util-6.1.26.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-compress-1.4.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jersey-container-servlet-core-2.22.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-beanutils-1.7.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hk2-utils-2.4.0-b34.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/parquet-format-2.3.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/avro-1.7.7.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/datanucleus-api-jdo-3.2.6.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jline-2.12.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/metrics-core-3.1.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/java-xmlbuilder-1.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/stax-api-1.0-2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hk2-locator-2.4.0-b34.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/parquet-hadoop-bundle-1.6.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jsp-api-2.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/xmlenc-0.52.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/xbean-asm5-shaded-4.4.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jackson-core-asl-1.9.13.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/shapeless_2.11-2.3.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-collections-3.2.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/javax.inject-1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-sql_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/json4s-jackson_2.11-3.2.11.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/json4s-ast_2.11-3.2.11.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-codec-1.10.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/leveldbjni-all-1.8.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-httpclient-3.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/aopalliance-repackaged-2.4.0-b34.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-yarn-server-common-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/minlog-1.3.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/javolution-5.5.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/datanucleus-rdbms-3.2.9.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jersey-common-2.22.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-graphx_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/api-asn1-api-1.0.0-M20.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/apacheds-i18n-2.0.0-M15.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/validation-api-1.1.0.Final.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-dbcp-1.4.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/slf4j-log4j12-1.7.16.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-pool-1.5.4.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-network-common_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/pmml-schema-1.2.15.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-mapreduce-client-shuffle-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/joda-time-2.9.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-hive-thriftserver_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spire-macros_2.11-0.13.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/curator-recipes-2.6.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jersey-server-2.22.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/htrace-core-3.1.0-incubating.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/httpclient-4.5.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-mllib-local_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/snappy-0.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/breeze-macros_2.11-0.13.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jackson-jaxrs-1.9.13.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/eigenbase-properties-1.1.5.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-tags_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jackson-databind-2.6.5.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/curator-client-2.6.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-catalyst_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/paranamer-2.6.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/opencsv-2.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/json4s-core_2.11-3.2.11.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hive-metastore-1.2.1.spark2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-digester-1.8.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jsr305-1.3.9.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-repl_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jetty-6.1.26.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/pyrolite-4.13.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/log4j-1.2.17.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-yarn-server-web-proxy-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/calcite-avatica-1.2.0-incubating.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/scala-xml_2.11-1.0.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-network-shuffle_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/calcite-linq4j-1.2.0-incubating.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/compress-lzf-1.0.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jtransforms-2.4.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/apache-log4j-extras-1.2.17.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jcl-over-slf4j-1.7.16.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/guice-3.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/gson-2.2.4.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/httpcore-4.4.4.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/protobuf-java-2.5.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-mapreduce-client-core-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-yarn_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-hive_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-lang-2.6.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/stax-api-1.0.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/javax.annotation-api-1.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/netty-all-4.0.43.Final.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/curator-framework-2.6.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-io-2.4.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/avro-ipc-1.7.7.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hive-beeline-1.2.1.spark2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/mesos-1.0.0-shaded-protobuf.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jackson-annotations-2.6.5.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jersey-media-jaxb-2.22.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-yarn-common-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jackson-mapper-asl-1.9.13.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/parquet-jackson-1.8.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-client-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jackson-module-paranamer-2.6.5.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/aopalliance-1.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/super-csv-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/janino-3.0.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/antlr4-runtime-4.5.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jpam-1.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-lang3-3.5.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/javassist-3.18.1-GA.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/bcprov-jdk15on-1.51.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-launcher_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/javax.ws.rs-api-2.0.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-mapreduce-client-jobclient-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/metrics-graphite-3.1.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jackson-xc-1.9.13.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/lz4-1.3.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/core-1.1.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-mesos_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/antlr-2.7.7.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-yarn-client-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/mx4j-3.0.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-logging-1.1.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-beanutils-core-1.8.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/libfb303-0.9.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/libthrift-0.9.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jaxb-api-2.2.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hive-exec-1.2.1.spark2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/calcite-core-1.2.0-incubating.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/parquet-encoding-1.8.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-mllib_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/api-util-1.0.0-M20.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/apacheds-kerberos-codec-2.0.0-M15.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/mail-1.4.7.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/stringtemplate-3.2.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/guice-servlet-3.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-crypto-1.0.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/JavaEWAH-0.3.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/metrics-json-3.1.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jdo-api-3.0.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/scalap-2.11.8.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hive-cli-1.2.1.spark2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/zookeeper-3.4.6.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jackson-module-scala_2.11-2.6.5.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/ivy-2.4.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/py4j-0.10.4.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/arpack_combined_all-0.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-unsafe_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/macro-compat_2.11-1.1.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jul-to-slf4j-1.7.16.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/pmml-model-1.2.15.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/parquet-column-1.8.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/javax.inject-2.4.0-b34.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/breeze_2.11-0.13.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-cli-1.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/chill-java-0.8.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/avro-mapred-1.7.7-hadoop2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/snappy-java-1.1.2.6.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/base64-2.3.8.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-compiler-3.0.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/slf4j-api-1.7.16.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/derby-10.12.1.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-mapreduce-client-app-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/objenesis-2.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jodd-core-3.5.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jersey-client-2.22.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/ST4-4.0.4.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/univocity-parsers-2.2.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/scala-parser-combinators_2.11-1.0.4.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jackson-core-2.6.5.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-math3-3.4.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/xz-1.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/scala-reflect-2.11.8.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-configuration-1.6.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/scala-compiler-2.11.8.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-yarn-api-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hk2-api-2.4.0-b34.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-core_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/guava-14.0.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/netty-3.9.9.Final.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-streaming_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/chill_2.11-0.8.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/scala-library-2.11.8.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/osgi-resource-locator-1.0.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/python/\n",
            "spark-2.2.0-bin-hadoop2.7/python/run-tests.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/hello/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/hello/sub_hello/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/hello/sub_hello/sub_hello.txt\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/hello/hello.txt\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/userlibrary.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/userlib-0.1.zip\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/people.json\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/people1.json\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/c=1/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/c=1/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/c=1/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/_SUCCESS\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/c=0/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/c=0/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/c=0/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/streaming/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/streaming/text-test.txt\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/text-test.txt\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/people_array.json\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/.part-r-00008.gz.parquet.crc\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/part-r-00008.gz.parquet\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/_SUCCESS\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/part-r-00005.gz.parquet\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/.part-r-00005.gz.parquet.crc\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/part-r-00002.gz.parquet\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/.part-r-00002.gz.parquet.crc\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/part-r-00004.gz.parquet\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/.part-r-00004.gz.parquet.crc\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/.part-r-00007.gz.parquet.crc\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/part-r-00007.gz.parquet\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/_metadata\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/_common_metadata\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/ages_newlines.csv\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/ages.csv\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/SimpleHTTPServer.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pylintrc\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/pyspark.ml.rst\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/pyspark.streaming.rst\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/conf.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/_templates/\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/_templates/layout.html\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/pyspark.rst\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/make.bat\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/epytext.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/make2.bat\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/index.rst\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/_static/\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/_static/pyspark.js\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/_static/pyspark.css\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/pyspark.sql.rst\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/pyspark.mllib.rst\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/Makefile\n",
            "spark-2.2.0-bin-hadoop2.7/python/.gitignore\n",
            "spark-2.2.0-bin-hadoop2.7/python/MANIFEST.in\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/status.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/version.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/conf.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/base.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/evaluation.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/util.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/classification.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/regression.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/tests.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/tuning.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/common.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/stat.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/linalg/\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/linalg/__init__.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/pipeline.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/feature.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/clustering.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/recommendation.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/__init__.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/wrapper.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/param/\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/param/_shared_params_code_gen.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/param/shared.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/param/__init__.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/fpm.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/statcounter.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/profiler.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/serializers.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/traceback_utils.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/shell.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/conf.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/session.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/window.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/tests.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/utils.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/group.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/types.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/catalog.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/context.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/dataframe.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/column.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/streaming.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/__init__.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/readwriter.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/functions.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/taskcontext.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/util.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/python/\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/python/pyspark/\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/python/pyspark/shell.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/daemon.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/tests.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/resultiterable.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/heapq3.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/broadcast.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/shuffle.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/cloudpickle.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/accumulators.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/java_gateway.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/util.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/listener.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/tests.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/flume.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/kafka.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/kinesis.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/dstream.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/context.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/__init__.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/context.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/storagelevel.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/__init__.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/join.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/tree.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/evaluation.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/util.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/classification.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/regression.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/tests.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/common.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/linalg/\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/linalg/__init__.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/linalg/distributed.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/feature.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/clustering.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/recommendation.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/stat/\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/stat/__init__.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/stat/_statistics.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/stat/KernelDensity.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/stat/test.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/stat/distribution.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/random.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/__init__.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/fpm.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/rdd.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/rddsampler.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/worker.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/files.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/find_spark_home.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/setup.cfg\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark.egg-info/\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark.egg-info/SOURCES.txt\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark.egg-info/requires.txt\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark.egg-info/PKG-INFO\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark.egg-info/dependency_links.txt\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark.egg-info/top_level.txt\n",
            "spark-2.2.0-bin-hadoop2.7/python/run-tests\n",
            "spark-2.2.0-bin-hadoop2.7/python/dist/\n",
            "spark-2.2.0-bin-hadoop2.7/python/setup.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/lib/\n",
            "spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip\n",
            "spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip\n",
            "spark-2.2.0-bin-hadoop2.7/python/lib/PY4J_LICENSE.txt\n",
            "spark-2.2.0-bin-hadoop2.7/python/README.md\n",
            "spark-2.2.0-bin-hadoop2.7/RELEASE\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/start-mesos-shuffle-service.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/start-mesos-dispatcher.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/spark-daemon.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/stop-slaves.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/stop-thriftserver.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/stop-shuffle-service.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/stop-history-server.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/spark-config.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/start-history-server.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/start-thriftserver.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/start-shuffle-service.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/spark-daemons.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/start-all.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/stop-master.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/stop-mesos-dispatcher.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/stop-slave.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/start-slave.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/stop-mesos-shuffle-service.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/start-slaves.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/stop-all.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/slaves.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/start-master.sh\n",
            "spark-2.2.0-bin-hadoop2.7/examples/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/RSparkSQLExample.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/fpm.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/gbt.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/isoreg.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/randomForest.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/als.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/kstest.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/gaussianMixture.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/ml.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/survreg.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/glm.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/kmeans.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/naiveBayes.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/svmLinear.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/lda.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/bisectingKmeans.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/mlp.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/logit.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/data-manipulation.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/dataframe.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/streaming/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/streaming/structured_network_wordcount.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/status_api_demo.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/linearsvc.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/random_forest_regressor_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/stopwords_remover_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/min_hash_lsh_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/standard_scaler_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/multiclass_logistic_regression_with_elastic_net.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/fpgrowth_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/vector_assembler_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/max_abs_scaler_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/binarizer_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/imputer_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/bisecting_k_means_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/cross_validator.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/logistic_regression_summary_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/dataframe_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/naive_bayes_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/generalized_linear_regression_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/decision_tree_regression_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/count_vectorizer_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/one_vs_rest_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/linear_regression_with_elastic_net.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/index_to_string_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/sql_transformer.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/correlation_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/dct_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/gradient_boosted_tree_regressor_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/isotonic_regression_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/vector_slicer_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/multilayer_perceptron_classification.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/gradient_boosted_tree_classifier_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/kmeans_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/pca_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/lda_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/polynomial_expansion_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/onehot_encoder_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/aft_survival_regression.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/rformula_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/logistic_regression_with_elastic_net.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/decision_tree_classification_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/estimator_transformer_param_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/tokenizer_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/min_max_scaler_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/pipeline_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/random_forest_classifier_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/string_indexer_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/als_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/n_gram_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/vector_indexer_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/normalizer_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/quantile_discretizer_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/train_validation_split.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/chisq_selector_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/gaussian_mixture_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/bucketed_random_projection_lsh_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/bucketizer_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/elementwise_product_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/tf_idf_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/word2vec_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/chi_square_test_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/pagerank.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sql/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sql/hive.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sql/basic.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sql/datasource.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sql/streaming/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sql/streaming/structured_kafka_wordcount.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sql/streaming/structured_network_wordcount_windowed.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sql/streaming/structured_network_wordcount.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/wordcount.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/pi.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/logistic_regression.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/network_wordjoinsentiments.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/sql_network_wordcount.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/queue_stream.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/network_wordcount.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/kafka_wordcount.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/stateful_network_wordcount.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/direct_kafka_wordcount.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/hdfs_wordcount.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/recoverable_network_wordcount.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/flume_wordcount.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/transitive_closure.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/kmeans.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/avro_inputformat.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/linear_regression_with_sgd_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/svd_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/recommendation_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/regression_metrics_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/standard_scaler_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/kernel_density_estimation_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/random_forest_regression_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/sampled_rdds.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/fpgrowth_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/streaming_k_means_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/latent_dirichlet_allocation_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/bisecting_k_means_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/gradient_boosting_classification_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/multi_class_metrics_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/correlations_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/pca_rowmatrix_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/naive_bayes_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/decision_tree_regression_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/ranking_metrics_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/gaussian_mixture_model.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/logistic_regression_with_lbfgs_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/gradient_boosting_regression_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/power_iteration_clustering_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/binary_classification_metrics_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/logistic_regression.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/hypothesis_testing_kolmogorov_smirnov_test_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/isotonic_regression_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/word2vec.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/summary_statistics_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/multi_label_metrics_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/hypothesis_testing_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/kmeans.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/decision_tree_classification_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/random_rdd_generation.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/svm_with_sgd_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/streaming_linear_regression_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/stratified_sampling_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/normalizer_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/random_forest_classification_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/gaussian_mixture_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/elementwise_product_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/tf_idf_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/word2vec_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/correlations.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/k_means_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/parquet_inputformat.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/als.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sort.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPCAExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaCrossValidationExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaAFTSurvivalRegressionExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMaxAbsScalerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionSummaryExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionWithElasticNetExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearSVCExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorIndexerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaKMeansExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPipelineExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestRegressorExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaTfIdfExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorAssemblerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaInteractionExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketizerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeRegressionExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaSQLTransformerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDocument.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearRegressionWithElasticNetExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketedRandomProjectionLSHExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGeneralizedLinearRegressionExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLDAExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDCTExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaNaiveBayesExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLabeledDocument.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaIndexToStringExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaImputerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaOneVsRestExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaEstimatorTransformerParamExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBisectingKMeansExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGaussianMixtureExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaFPGrowthExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaStopWordsRemoverExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaCountVectorizerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRFormulaExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaOneHotEncoderExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaQuantileDiscretizerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestClassifierExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaCorrelationExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaIsotonicRegressionExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeClassifierExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaALSExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMultilayerPerceptronClassifierExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMulticlassLogisticRegressionWithElasticNetExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMinHashLSHExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaNGramExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaStandardScalerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeClassificationExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaNormalizerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPolynomialExpansionExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBinarizerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeRegressorExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaStringIndexerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaWord2VecExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorSlicerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSqSelectorExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMinMaxScalerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaElementwiseProductExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaTokenizerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSquareTestExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaTrainValidationSplitExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaSparkPi.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/hive/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/hive/JavaSparkHiveExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedTypedAggregation.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredKafkaWordCount.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCountWindowed.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCount.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredSessionization.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedUntypedAggregation.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaLogQuery.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaTC.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaStatusTrackerDemo.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecord.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaFlumeEventCount.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaDirectKafkaWordCount.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaNetworkWordCount.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaSqlNetworkWordCount.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecoverableNetworkWordCount.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaStatefulNetworkWordCount.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaCustomReceiver.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaQueueStream.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaKafkaWordCount.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaHdfsLR.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaPageRank.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaWordCount.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaPCAExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVDExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaKMeansExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLBFGSExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaKernelDensityEstimationExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaMultiLabelClassificationMetricsExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVMWithSGDExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaMulticlassClassificationMetricsExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaPrefixSpanExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLatentDirichletAllocationExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeRegressionExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaAssociationRulesExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaStreamingTestExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaBinaryClassificationMetricsExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestClassificationExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestRegressionExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaStratifiedSamplingExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaNaiveBayesExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingClassificationExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaPowerIterationClusteringExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRankingMetricsExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaALS.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRecommendationExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaBisectingKMeansExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaGaussianMixtureExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingRegressionExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaIsotonicRegressionExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSimpleFPGrowth.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRegressionMetricsExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeClassificationExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaCorrelationsExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingKolmogorovSmirnovTestExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLogisticRegressionWithLBFGSExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaChiSqSelectorExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaElementwiseProductExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSummaryStatisticsExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLinearRegressionWithSGDExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/DFSReadWriteTest.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/GroupByTest.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LinearSVCExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/InteractionExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MaxAbsScalerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/SQLTransformerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/StandardScalerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BisectingKMeansExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ElementwiseProductExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/IndexToStringExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionSummaryExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ALSExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/CorrelationExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BucketizerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorIndexerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MinHashLSHExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/StringIndexerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/TokenizerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/UnaryTransformerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PCAExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/StopWordsRemoverExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MultilayerPerceptronClassifierExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/IsotonicRegressionExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GBTExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ImputerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestRegressorExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ChiSquareTestExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DeveloperApiExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ChiSqSelectorExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/TfIdfExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BucketedRandomProjectionLSHExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BinarizerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeClassifierExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MinMaxScalerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/OneVsRestExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/OneHotEncoderExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorAssemblerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/Word2VecExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/NaiveBayesExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PipelineExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestClassifierExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DCTExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeRegressorExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionWithElasticNetExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DataFrameExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaCrossValidationExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MulticlassLogisticRegressionWithElasticNetExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeClassificationExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/KMeansExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/EstimatorTransformerParamExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaTrainValidationSplitExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/CountVectorizerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorSlicerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/NormalizerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/FPGrowthExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GaussianMixtureExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PolynomialExpansionExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionWithElasticNetExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeRegressionExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LDAExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GeneralizedLinearRegressionExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RFormulaExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/AFTSurvivalRegressionExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/QuantileDiscretizerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/NGramExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkKMeans.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/MultiBroadcastTest.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/hive/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/hive/SparkHiveExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedUntypedAggregation.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/RDDRelation.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedTypedAggregation.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCountWindowed.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredKafkaWordCount.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCount.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredSessionization.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/pythonconverters/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/pythonconverters/AvroConverters.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalLR.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkTC.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/BroadcastTest.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ExceptionHandlingTest.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalKMeans.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/AggregateMessagesExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/Analytics.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/SSSPExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/SynthBenchmark.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/ConnectedComponentsExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/ComprehensiveExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/PageRankExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/TriangleCountingExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/LiveJournalPageRank.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/HdfsTest.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SimpleSkewedGroupByTest.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkPageRank.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/StatefulNetworkWordCount.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/HdfsWordCount.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKafkaWordCount.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/QueueStream.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/FlumePollingEventCount.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/SqlNetworkWordCount.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/FlumeEventCount.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/RecoverableNetworkWordCount.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewStream.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewGenerator.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/StreamingExamples.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/NetworkWordCount.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/CustomReceiver.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/KafkaWordCount.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/RawNetworkGrep.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkPi.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkALS.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalFileLR.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/DriverSubmissionTest.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LogQuery.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SVMWithSGDExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LatentDirichletAllocationExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLinearRegressionExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MultiLabelMetricsExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RankingMetricsExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/AbstractParams.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StandardScalerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StratifiedSamplingExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/BisectingKMeansExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingKMeansExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/ElementwiseProductExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingClassificationExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestClassificationExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingTestExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/KernelDensityEstimationExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/TFIDFExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingRegressionExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnyPCA.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PMMLModelExportExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LogisticRegressionWithLBFGSExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PCAExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/AssociationRulesExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRunner.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnySVD.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/IsotonicRegressionExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassificationMetricsExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingKolmogorovSmirnovTestExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LBFGSExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PrefixSpanExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/ChiSqSelectorExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/Correlations.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RecommendationExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SVDExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MovieLensALS.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/CosineSimilarity.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MulticlassMetricsExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnSourceVectorExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/Word2VecExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLogisticRegression.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/NaiveBayesExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RegressionMetricsExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SampledRDDs.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LinearRegressionWithSGDExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeClassificationExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RandomRDDGeneration.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/KMeansExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SparseNaiveBayes.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassification.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PowerIterationClusteringExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DenseKMeans.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MultivariateSummarizer.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnRowMatrixExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/NormalizerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestRegressionExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/FPGrowthExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GaussianMixtureExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SummaryStatisticsExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/CorrelationsExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostedTreesRunner.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRegressionExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LinearRegression.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SimpleFPGrowth.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LDAExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkLR.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalPi.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkHdfsLR.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SkewedGroupByTest.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalALS.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/people.json\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/employees.json\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/people.txt\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/full_user.avsc\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/kv1.txt\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/users.parquet\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/user.avsc\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/users.avro\n",
            "spark-2.2.0-bin-hadoop2.7/examples/jars/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/jars/scopt_2.11-3.3.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/data/\n",
            "spark-2.2.0-bin-hadoop2.7/data/graphx/\n",
            "spark-2.2.0-bin-hadoop2.7/data/graphx/followers.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/graphx/users.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/streaming/\n",
            "spark-2.2.0-bin-hadoop2.7/data/streaming/AFINN-111.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/pagerank_data.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/kmeans_data.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/streaming_kmeans_data_test.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/sample_lda_libsvm_data.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/sample_kmeans_data.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/pic_data.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/sample_isotonic_regression_libsvm_data.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/als/\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/als/test.data\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/als/sample_movielens_ratings.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/sample_fpgrowth.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/sample_libsvm_data.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/ridge-data/\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/ridge-data/lpsa.data\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/sample_multiclass_classification_data.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/sample_linear_regression_data.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/sample_binary_classification_data.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/sample_lda_data.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/sample_movielens_data.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/sample_svm_data.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/gmm_data.txt\n",
            "spark-2.2.0-bin-hadoop2.7/R/\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/sparkr.zip\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/groupBy.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/covar_pop.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sampleBy.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sql.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/year.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/tan.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/GBTRegressionModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/last_day.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sign.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/hint.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/randn.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/orderBy.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/otherwise.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/AFTSurvivalRegressionModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/hashCode.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/bin.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dim.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.svmLinear.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/minute.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/createExternalTable-deprecated.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/distinct.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.conf.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/print.jobj.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/md5.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cbrt.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/write.ml.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dapplyCollect.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/acos.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/tables.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/NaiveBayesModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sum.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/structType.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/hex.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/isLocal.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/write.jdbc.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/from_utc_timestamp.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/tableNames.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/createDataFrame.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/isStreaming.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/toJSON.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.getSparkFiles.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/except.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/LDAModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/months_between.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark_partition_id.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/write.parquet.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sumDistinct.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/awaitTermination.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/BisectingKMeansModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sha2.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/abs.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/date_format.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/withColumn.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dayofyear.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sort_array.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/storageLevel.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/setCurrentDatabase.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/ceil.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/floor.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/stddev_pop.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sd.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/print.structType.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.survreg.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/predict.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/count.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/unhex.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/mean.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/instr.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/from_unixtime.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/saveAsTable.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/ltrim.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkRHive.init-deprecated.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/read.parquet.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/match.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/is.nan.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/read.ml.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/lag.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/write.json.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/unpersist.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/corr.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.callJStatic.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rint.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/LinearSVCModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.gbt.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/RandomForestClassificationModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/persist.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/var_pop.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/selectExpr.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/crc32.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/expr.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.callJMethod.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/with.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/generateAliasesForIntersectedCols.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/GeneralizedLinearRegressionModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/IsotonicRegressionModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/setLogLevel.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/shiftRightUnsigned.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/base64.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/array_contains.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/expm1.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/write.orc.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.version.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/insertInto.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/SparkDataFrame.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/merge.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dayofmonth.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/listDatabases.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/summarize.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/format_number.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/from_json.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dropDuplicates.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cache.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/write.text.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/approxCountDistinct.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkRSQL.init-deprecated.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/LogisticRegressionModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/stddev_samp.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/pivot.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/showDF.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/between.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/struct.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/subset.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/posexplode.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/lit.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/shiftLeft.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/glm.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/hypot.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/recoverPartitions.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.session.stop.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/translate.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/drop.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/GBTClassificationModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/shiftRight.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/regexp_replace.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/randomSplit.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/length.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rowsBetween.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/read.jdbc.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/schema.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/toRadians.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/filter.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/bround.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/createOrReplaceTempView.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cancelJobGroup.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/second.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/upper.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/head.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/limit.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/concat_ws.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/when.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/FPGrowthModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/install.spark.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.newJObject.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dropTempView.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/unbase64.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/soundex.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/structField.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.addFile.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.bisectingKmeans.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cacheTable.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cosh.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.mlp.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/ntile.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/atan2.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.kstest.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dtypes.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/reverse.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sinh.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.lda.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/createTable.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/negate.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/asin.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/hash.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rank.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/toDegrees.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/columns.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/columnfunctions.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/substring_index.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/to_timestamp.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.naiveBayes.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/atan.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.isoreg.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/factorial.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/countDistinct.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/quarter.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/setCheckpointDir.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/least.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/read.text.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/windowOrderBy.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dapply.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/coalesce.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/refreshByPath.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cume_dist.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dense_rank.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/freqItems.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/getNumPartitions.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/KMeansModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.session.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/arrange.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/write.stream.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/encode.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.glm.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/isActive.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/crossJoin.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rpad.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/uncacheTable.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/size.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/conv.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/log10.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/collect.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/read.stream.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/format_string.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/windowPartitionBy.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/union.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/stopQuery.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dropTempTable-deprecated.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/endsWith.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/startsWith.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/nanvl.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/mutate.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/explain.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/R.css\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cov.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/var_samp.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/log2.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/registerTempTable-deprecated.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/lastProgress.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/attach.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/min.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/ncol.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/month.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/window.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/partitionBy.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/percent_rank.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/listFunctions.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/to_utc_timestamp.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/crosstab.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/take.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/exp.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/to_json.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/column.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/ifelse.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/GaussianMixtureModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.logit.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/show.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/setJobGroup.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/KSTest-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/clearJobGroup.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/last.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/printSchema.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rename.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rbind.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/over.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/MultilayerPerceptronClassificationModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/coltypes.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/datediff.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/lead.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/summary.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/WindowSpec.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/unix_timestamp.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/tanh.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/listColumns.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/to_date.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/00Index.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.uiWebUrl.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/max.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/var.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/print.structField.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.als.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/alias.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sha1.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/status.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/read.orc.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/currentDatabase.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/write.df.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/round.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.init-deprecated.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/refreshTable.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/nrow.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/pmod.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/intersect.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rtrim.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/substr.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/log.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/levenshtein.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/monotonically_increasing_id.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/checkpoint.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/decode.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.lapply.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/read.json.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/RandomForestRegressionModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/trim.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/select.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/regexp_extract.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/as.data.frame.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rangeBetween.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/queryName.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sample.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/lower.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/repartition.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/concat.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cast.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/date_add.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.fpGrowth.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/hour.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/initcap.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/explode.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/add_months.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/bitwiseNOT.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/approxQuantile.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/kurtosis.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/greatest.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/first.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/read.df.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rand.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/next_day.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/clearCache.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/nafunctions.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/row_number.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/lpad.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/skewness.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/gapplyCollect.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/locate.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/avg.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sqrt.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cos.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/log1p.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/str.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/StreamingQuery.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/ALSModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/ascii.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/listTables.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sin.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/histogram.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/weekofyear.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/GroupedData.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/fitted.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/date_sub.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/tableToDF.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.getSparkFilesRootDirectory.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/join.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/gapply.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.randomForest.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.kmeans.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.gaussianMixture.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/INDEX\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/R/\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/R/SparkR.rdx\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/R/SparkR\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/R/SparkR.rdb\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/help/\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/help/aliases.rds\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/help/paths.rds\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/help/SparkR.rdx\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/help/AnIndex\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/help/SparkR.rdb\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/DESCRIPTION\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/Meta/\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/Meta/hsearch.rds\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/Meta/nsInfo.rds\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/Meta/package.rds\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/Meta/links.rds\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/Meta/Rd.rds\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/profile/\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/profile/general.R\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/profile/shell.R\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/worker/\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/worker/daemon.R\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/worker/worker.R\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/NAMESPACE\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/tests/\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/tests/testthat/\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/tests/testthat/test_basic.R\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-slf4j.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-scalacheck.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-py4j.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-paranamer.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-netlib.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-Mockito.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-minlog.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-graphlib-dot.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-d3.min.js.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-f2j.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-junit-interface.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-boto.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-spire.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-pyrolite.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-xmlenc.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-scala.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-SnapTree.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-AnchorJS.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-jline.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-heapq.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-cloudpickle.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-reflectasm.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-jpmml-model.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-modernizr.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-javolution.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-jbcrypt.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-sbt-launch-lib.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-postgresql.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-kryo.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-DPark.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-scopt.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-sorttable.js.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-dagre-d3.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-jquery.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-protobuf.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-antlr.txt\n",
            "spark-2.2.0-bin-hadoop2.7/conf/\n",
            "spark-2.2.0-bin-hadoop2.7/conf/fairscheduler.xml.template\n",
            "spark-2.2.0-bin-hadoop2.7/conf/metrics.properties.template\n",
            "spark-2.2.0-bin-hadoop2.7/conf/spark-env.sh.template\n",
            "spark-2.2.0-bin-hadoop2.7/conf/log4j.properties.template\n",
            "spark-2.2.0-bin-hadoop2.7/conf/docker.properties.template\n",
            "spark-2.2.0-bin-hadoop2.7/conf/slaves.template\n",
            "spark-2.2.0-bin-hadoop2.7/conf/spark-defaults.conf.template\n",
            "spark-2.2.0-bin-hadoop2.7/LICENSE\n",
            "spark-2.2.0-bin-hadoop2.7/bin/\n",
            "spark-2.2.0-bin-hadoop2.7/bin/spark-shell\n",
            "spark-2.2.0-bin-hadoop2.7/bin/spark-submit.cmd\n",
            "spark-2.2.0-bin-hadoop2.7/bin/spark-shell2.cmd\n",
            "spark-2.2.0-bin-hadoop2.7/bin/pyspark\n",
            "spark-2.2.0-bin-hadoop2.7/bin/sparkR.cmd\n",
            "spark-2.2.0-bin-hadoop2.7/bin/spark-class2.cmd\n",
            "spark-2.2.0-bin-hadoop2.7/bin/run-example.cmd\n",
            "spark-2.2.0-bin-hadoop2.7/bin/spark-submit2.cmd\n",
            "spark-2.2.0-bin-hadoop2.7/bin/spark-class\n",
            "spark-2.2.0-bin-hadoop2.7/bin/spark-submit\n",
            "spark-2.2.0-bin-hadoop2.7/bin/spark-sql\n",
            "spark-2.2.0-bin-hadoop2.7/bin/find-spark-home\n",
            "spark-2.2.0-bin-hadoop2.7/bin/run-example\n",
            "spark-2.2.0-bin-hadoop2.7/bin/beeline\n",
            "spark-2.2.0-bin-hadoop2.7/bin/pyspark2.cmd\n",
            "spark-2.2.0-bin-hadoop2.7/bin/spark-shell.cmd\n",
            "spark-2.2.0-bin-hadoop2.7/bin/spark-class.cmd\n",
            "spark-2.2.0-bin-hadoop2.7/bin/pyspark.cmd\n",
            "spark-2.2.0-bin-hadoop2.7/bin/sparkR\n",
            "spark-2.2.0-bin-hadoop2.7/bin/beeline.cmd\n",
            "spark-2.2.0-bin-hadoop2.7/bin/sparkR2.cmd\n",
            "spark-2.2.0-bin-hadoop2.7/bin/load-spark-env.sh\n",
            "spark-2.2.0-bin-hadoop2.7/bin/load-spark-env.cmd\n",
            "spark-2.2.0-bin-hadoop2.7/yarn/\n",
            "spark-2.2.0-bin-hadoop2.7/yarn/spark-2.2.0-yarn-shuffle.jar\n",
            "spark-2.2.0-bin-hadoop2.7/README.md\n",
            "環境初始化完畢\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "f1m9p6bAuwgY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "os.environ['SPARK_HOME'] = \"/usr/local/spark\"\n",
        "os.environ['PYSPARK_PYTHON'] = \"/usr/bin/python\"\n",
        "sys.path.append(\"/usr/local/spark/python/\")\n",
        "sys.path.append(\"/usr/local/spark/python/lib/pyspark.zip\")\n",
        "sys.path.append(\"/usr/local/spark/python/lib/py4j-0.10.4-src.zip\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Siw9OMRLu0uA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark import SparkConf\n",
        "sc = SparkContext()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RtUgrtwrt1Qa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Apache Spark 基本練習：\n",
        "\n",
        "是一個開源叢集運算框架，最初是由加州大學柏克萊分校AMPLab所開發。相對於Hadoop的MapReduce會在執行完工作後將中介資料存放到磁碟中，Spark使用了記憶體內運算技術，能在資料尚未寫入硬碟時即在記憶體內分析運算。Spark在記憶體內執行程式的運算速度能做到比Hadoop MapReduce的運算速度快上100倍。\n",
        "\n",
        "Some References :\n",
        "\n",
        "1. [http://www.mccarroll.net/blog/pyspark/index.html]\n",
        "2. [https://www.codementor.io/spark/tutorial/spark-python-rdd-basics]\n",
        "3. [http://backtobazics.com/big-data/spark/apache-spark-map-example/]\n",
        "4. [http://datascience-enthusiast.com/Python/Apache_Spark1.html]\n",
        "\n",
        "## RDD 基本操作練習：\n",
        "\n",
        "\n",
        "\n",
        "### 產生一個整數隨機陣列： python語法："
      ]
    },
    {
      "metadata": {
        "id": "I--b66Kzt1Qc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1055
        },
        "outputId": "574503b8-39d1-45fe-950f-7250d35fc327"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "random_array = np.random.randint(1000, size=1000)\n",
        "print (random_array)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[905 343 991 351 130 240 848 399 314 982 281 127 312 349 126 724 378  94\n",
            " 628 652 912  89 884 313 119 389 696 886 475 814 561 444 290 675 297 947\n",
            " 753  34 176  71 914 628 341 821 995 464 206 955 125 497 147 313  80 420\n",
            " 191  39 677 875 650 848  21 158 354  13 196 445 685 735 784 407 361 246\n",
            " 601 413 360 468 781 753 746 447 888 511  66 725  66 707 956 763 811 147\n",
            "  35  29 543 817  20 961 253 320 402 537 393 901 836 609 604 943 111 582\n",
            " 864  75  57  61 730 970 930 118 743 118 600  66  42  38 657  45 688 510\n",
            " 444 896 846  79 130 902 255 764 800 836 994  33 834 166 918 470 615  99\n",
            " 821 984 479 334 649 666 762 238 411 505 392  32 904 818 914 557 175 946\n",
            " 546 576 257 741  74 454 670 729 453 950 175 943 773 569 536 928 736 914\n",
            " 561 353 207 100 579 832 699 180  97 614 934 925 139 342 323 546 221 223\n",
            " 978  25 133 636 623  61 389 471 445  71 786  55 213 184  73 607 214 714\n",
            " 996 313 717 998 175 644 449  29 186 194 879 756 432 315 773 437 782 750\n",
            " 457 389 667 475 602 815 385 710 317 721 488 577 977 302 372 238 443 974\n",
            " 764 720 201 596 701 211 961 470 474 882 515 597 995 114 906 222 449  22\n",
            " 832 288 682 222  20 851 714   1 170 424 311 335 575 838 241 952 861 379\n",
            " 729 803 178 699  58 300   1 303 471 834 428  35 593 689 896 233 286 146\n",
            " 933 965 187 890 981 568 603 532 127 719 866 678 203 666 503 963 560 239\n",
            " 373 883 262 972 266 756  61 883 545 102 542  34  23 373 653 712 918 450\n",
            " 510 559 604 652 125 374 396 988 369 231 615  88 327 836 923 784 462 299\n",
            " 356 484  81 988 916 981 767 576 266 294 900 636 993 315 761 317  75 469\n",
            " 753 126  42 960 191 543 109 277 769 924 885  48 893 780 819 846  15 465\n",
            " 622 213  84 108 232 122 477 988 287 301 351 354 735 416 303  45 362 546\n",
            " 782 630 128 933  73 369  29 698 131 440 453 943 661 891 272 998 636 896\n",
            " 347 452 228 360 594 133 951 491 112 986 716 819 385  49 268 613 622 847\n",
            " 218 918 437 814 118 817 827 118 298 728  20 157 361 379 901 550 310 433\n",
            "  65 256 824 960 733 939 976 183 434 709 628 951 258 165 470 687 680 681\n",
            " 903 296  43 332 375 659 352 109 573 439 227 682 834 489 249 978 617 387\n",
            " 868 615 352 797  42  59 280 850 743 683 602 260 427 938 490 436 842 801\n",
            " 768 520 672 919 305 217 794 951 199 345 861 525 264 763 434 567 664 193\n",
            " 376 732   1 283 849 238   2 492 659 879 153 514 184 163 325 284 711  77\n",
            "  34 887 905 264 299 376 623 357 376 246 483  89 329 131 844 589 826 901\n",
            " 455 649 221  43 566 970 659 362 896 814 245 601 839 977 161 751 812 465\n",
            " 627 166 714 466 788 822 535 431  74 525 555 479 610 448 978 434 618 314\n",
            " 273 988 858 914 759 728 464 860 614 637 915  95   3 418  56 586 650 309\n",
            " 173 404 741 235  62 668 599 918 836 212 533 343 243 213 825 954 779 213\n",
            " 772 245 317 559 348 995  52 708 998 278 264 295 130 804  76 433 540  37\n",
            " 851 958 194 884 947 164 452 483 424 837 268 162 594 672  47 421 102 340\n",
            " 447 770 555 333 516 981 839 581 623 349 769 331 990 398 498 401 550 659\n",
            " 321 548 336 169 271 366 268 171 152 873  29 209 770 509 178 308 361 157\n",
            " 299 532 608 211 683 807 668 365 943 628 240 147 814 901 454 758 453  57\n",
            "  80 308 373 664 231 865 598 222 965 734 479 848 786 934 278 261 741 724\n",
            " 269  58 655 238 601  28  57 313 221 142 354 155 612 143 493 158 541 416\n",
            " 731 195 162 732 713 684 283 471 442 809 809 410 974 843 261 915 791  82\n",
            " 248 827 963 535 805 897 548 169 332   1 301 274 639  76 729 374 292 782\n",
            " 248 449 115  98 851 213 883 508 559 364 267 754 896 888 232 249 471 855\n",
            " 361 476 119 384 590 516 611  93 323 857  58 599 817 621 442 410 278 247\n",
            " 733 132  21 193 648 912 305 927 705 112 750 735 866 802 599 459 857 134\n",
            " 243 116  80 341 461 221 500 995 600  65 611 352 214 264 213 243 905 682\n",
            " 802 308 410 919 634 899 624 179 454  88 550 998 195 719 562 121 360 348\n",
            " 551 905 739 967 448 169 497 654 854 832 658 488 923 155 769 352 407 312\n",
            "  17 394 581 115 233 352 964 505 357 833 465 145 487 445 581  15 498 477\n",
            " 654 290 796 967 238 495 612 435 801 195 169 593 855 430 548 317 178 900\n",
            " 186 195 995 525 720 521 498 962 657  55 399 282 188 225 552 746 671 325\n",
            " 222 869 124  51 602 671 612 352 336 264 761 386 685 602 891 476 423 588\n",
            " 927 837 555 248 522 156 699 110 861 975]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "G_4UlzRamKOX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c574481f-164a-4a50-f0bc-43d5a145f013"
      },
      "cell_type": "code",
      "source": [
        "random_array[0:5]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([905, 343, 991, 351, 130])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "l6qyKmz4t1Qg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 將資料轉成RDD，分別擺放於各spark executors上\n",
        "\n",
        "<img src=\"https://www.dropbox.com/s/br94ete5q3rj9w3/spark%20data%20model.png?dl=1\" width=\"500\" align=\"left\">\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "___\n",
        "<img src=\"https://www.dropbox.com/s/l2gohpmn53jkv1b/%20spark%20system%20overview.png?dl=1\" width=\"500\" align=\"left\">"
      ]
    },
    {
      "metadata": {
        "id": "MeFSq7rbt1Qg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rdd = sc.parallelize(random_array)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FyfiIp_Amls1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 18199
        },
        "outputId": "6b9fb3a2-17fd-4d5c-ce8b-c7369032370d"
      },
      "cell_type": "code",
      "source": [
        "rdd.collect()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[905,\n",
              " 343,\n",
              " 991,\n",
              " 351,\n",
              " 130,\n",
              " 240,\n",
              " 848,\n",
              " 399,\n",
              " 314,\n",
              " 982,\n",
              " 281,\n",
              " 127,\n",
              " 312,\n",
              " 349,\n",
              " 126,\n",
              " 724,\n",
              " 378,\n",
              " 94,\n",
              " 628,\n",
              " 652,\n",
              " 912,\n",
              " 89,\n",
              " 884,\n",
              " 313,\n",
              " 119,\n",
              " 389,\n",
              " 696,\n",
              " 886,\n",
              " 475,\n",
              " 814,\n",
              " 561,\n",
              " 444,\n",
              " 290,\n",
              " 675,\n",
              " 297,\n",
              " 947,\n",
              " 753,\n",
              " 34,\n",
              " 176,\n",
              " 71,\n",
              " 914,\n",
              " 628,\n",
              " 341,\n",
              " 821,\n",
              " 995,\n",
              " 464,\n",
              " 206,\n",
              " 955,\n",
              " 125,\n",
              " 497,\n",
              " 147,\n",
              " 313,\n",
              " 80,\n",
              " 420,\n",
              " 191,\n",
              " 39,\n",
              " 677,\n",
              " 875,\n",
              " 650,\n",
              " 848,\n",
              " 21,\n",
              " 158,\n",
              " 354,\n",
              " 13,\n",
              " 196,\n",
              " 445,\n",
              " 685,\n",
              " 735,\n",
              " 784,\n",
              " 407,\n",
              " 361,\n",
              " 246,\n",
              " 601,\n",
              " 413,\n",
              " 360,\n",
              " 468,\n",
              " 781,\n",
              " 753,\n",
              " 746,\n",
              " 447,\n",
              " 888,\n",
              " 511,\n",
              " 66,\n",
              " 725,\n",
              " 66,\n",
              " 707,\n",
              " 956,\n",
              " 763,\n",
              " 811,\n",
              " 147,\n",
              " 35,\n",
              " 29,\n",
              " 543,\n",
              " 817,\n",
              " 20,\n",
              " 961,\n",
              " 253,\n",
              " 320,\n",
              " 402,\n",
              " 537,\n",
              " 393,\n",
              " 901,\n",
              " 836,\n",
              " 609,\n",
              " 604,\n",
              " 943,\n",
              " 111,\n",
              " 582,\n",
              " 864,\n",
              " 75,\n",
              " 57,\n",
              " 61,\n",
              " 730,\n",
              " 970,\n",
              " 930,\n",
              " 118,\n",
              " 743,\n",
              " 118,\n",
              " 600,\n",
              " 66,\n",
              " 42,\n",
              " 38,\n",
              " 657,\n",
              " 45,\n",
              " 688,\n",
              " 510,\n",
              " 444,\n",
              " 896,\n",
              " 846,\n",
              " 79,\n",
              " 130,\n",
              " 902,\n",
              " 255,\n",
              " 764,\n",
              " 800,\n",
              " 836,\n",
              " 994,\n",
              " 33,\n",
              " 834,\n",
              " 166,\n",
              " 918,\n",
              " 470,\n",
              " 615,\n",
              " 99,\n",
              " 821,\n",
              " 984,\n",
              " 479,\n",
              " 334,\n",
              " 649,\n",
              " 666,\n",
              " 762,\n",
              " 238,\n",
              " 411,\n",
              " 505,\n",
              " 392,\n",
              " 32,\n",
              " 904,\n",
              " 818,\n",
              " 914,\n",
              " 557,\n",
              " 175,\n",
              " 946,\n",
              " 546,\n",
              " 576,\n",
              " 257,\n",
              " 741,\n",
              " 74,\n",
              " 454,\n",
              " 670,\n",
              " 729,\n",
              " 453,\n",
              " 950,\n",
              " 175,\n",
              " 943,\n",
              " 773,\n",
              " 569,\n",
              " 536,\n",
              " 928,\n",
              " 736,\n",
              " 914,\n",
              " 561,\n",
              " 353,\n",
              " 207,\n",
              " 100,\n",
              " 579,\n",
              " 832,\n",
              " 699,\n",
              " 180,\n",
              " 97,\n",
              " 614,\n",
              " 934,\n",
              " 925,\n",
              " 139,\n",
              " 342,\n",
              " 323,\n",
              " 546,\n",
              " 221,\n",
              " 223,\n",
              " 978,\n",
              " 25,\n",
              " 133,\n",
              " 636,\n",
              " 623,\n",
              " 61,\n",
              " 389,\n",
              " 471,\n",
              " 445,\n",
              " 71,\n",
              " 786,\n",
              " 55,\n",
              " 213,\n",
              " 184,\n",
              " 73,\n",
              " 607,\n",
              " 214,\n",
              " 714,\n",
              " 996,\n",
              " 313,\n",
              " 717,\n",
              " 998,\n",
              " 175,\n",
              " 644,\n",
              " 449,\n",
              " 29,\n",
              " 186,\n",
              " 194,\n",
              " 879,\n",
              " 756,\n",
              " 432,\n",
              " 315,\n",
              " 773,\n",
              " 437,\n",
              " 782,\n",
              " 750,\n",
              " 457,\n",
              " 389,\n",
              " 667,\n",
              " 475,\n",
              " 602,\n",
              " 815,\n",
              " 385,\n",
              " 710,\n",
              " 317,\n",
              " 721,\n",
              " 488,\n",
              " 577,\n",
              " 977,\n",
              " 302,\n",
              " 372,\n",
              " 238,\n",
              " 443,\n",
              " 974,\n",
              " 764,\n",
              " 720,\n",
              " 201,\n",
              " 596,\n",
              " 701,\n",
              " 211,\n",
              " 961,\n",
              " 470,\n",
              " 474,\n",
              " 882,\n",
              " 515,\n",
              " 597,\n",
              " 995,\n",
              " 114,\n",
              " 906,\n",
              " 222,\n",
              " 449,\n",
              " 22,\n",
              " 832,\n",
              " 288,\n",
              " 682,\n",
              " 222,\n",
              " 20,\n",
              " 851,\n",
              " 714,\n",
              " 1,\n",
              " 170,\n",
              " 424,\n",
              " 311,\n",
              " 335,\n",
              " 575,\n",
              " 838,\n",
              " 241,\n",
              " 952,\n",
              " 861,\n",
              " 379,\n",
              " 729,\n",
              " 803,\n",
              " 178,\n",
              " 699,\n",
              " 58,\n",
              " 300,\n",
              " 1,\n",
              " 303,\n",
              " 471,\n",
              " 834,\n",
              " 428,\n",
              " 35,\n",
              " 593,\n",
              " 689,\n",
              " 896,\n",
              " 233,\n",
              " 286,\n",
              " 146,\n",
              " 933,\n",
              " 965,\n",
              " 187,\n",
              " 890,\n",
              " 981,\n",
              " 568,\n",
              " 603,\n",
              " 532,\n",
              " 127,\n",
              " 719,\n",
              " 866,\n",
              " 678,\n",
              " 203,\n",
              " 666,\n",
              " 503,\n",
              " 963,\n",
              " 560,\n",
              " 239,\n",
              " 373,\n",
              " 883,\n",
              " 262,\n",
              " 972,\n",
              " 266,\n",
              " 756,\n",
              " 61,\n",
              " 883,\n",
              " 545,\n",
              " 102,\n",
              " 542,\n",
              " 34,\n",
              " 23,\n",
              " 373,\n",
              " 653,\n",
              " 712,\n",
              " 918,\n",
              " 450,\n",
              " 510,\n",
              " 559,\n",
              " 604,\n",
              " 652,\n",
              " 125,\n",
              " 374,\n",
              " 396,\n",
              " 988,\n",
              " 369,\n",
              " 231,\n",
              " 615,\n",
              " 88,\n",
              " 327,\n",
              " 836,\n",
              " 923,\n",
              " 784,\n",
              " 462,\n",
              " 299,\n",
              " 356,\n",
              " 484,\n",
              " 81,\n",
              " 988,\n",
              " 916,\n",
              " 981,\n",
              " 767,\n",
              " 576,\n",
              " 266,\n",
              " 294,\n",
              " 900,\n",
              " 636,\n",
              " 993,\n",
              " 315,\n",
              " 761,\n",
              " 317,\n",
              " 75,\n",
              " 469,\n",
              " 753,\n",
              " 126,\n",
              " 42,\n",
              " 960,\n",
              " 191,\n",
              " 543,\n",
              " 109,\n",
              " 277,\n",
              " 769,\n",
              " 924,\n",
              " 885,\n",
              " 48,\n",
              " 893,\n",
              " 780,\n",
              " 819,\n",
              " 846,\n",
              " 15,\n",
              " 465,\n",
              " 622,\n",
              " 213,\n",
              " 84,\n",
              " 108,\n",
              " 232,\n",
              " 122,\n",
              " 477,\n",
              " 988,\n",
              " 287,\n",
              " 301,\n",
              " 351,\n",
              " 354,\n",
              " 735,\n",
              " 416,\n",
              " 303,\n",
              " 45,\n",
              " 362,\n",
              " 546,\n",
              " 782,\n",
              " 630,\n",
              " 128,\n",
              " 933,\n",
              " 73,\n",
              " 369,\n",
              " 29,\n",
              " 698,\n",
              " 131,\n",
              " 440,\n",
              " 453,\n",
              " 943,\n",
              " 661,\n",
              " 891,\n",
              " 272,\n",
              " 998,\n",
              " 636,\n",
              " 896,\n",
              " 347,\n",
              " 452,\n",
              " 228,\n",
              " 360,\n",
              " 594,\n",
              " 133,\n",
              " 951,\n",
              " 491,\n",
              " 112,\n",
              " 986,\n",
              " 716,\n",
              " 819,\n",
              " 385,\n",
              " 49,\n",
              " 268,\n",
              " 613,\n",
              " 622,\n",
              " 847,\n",
              " 218,\n",
              " 918,\n",
              " 437,\n",
              " 814,\n",
              " 118,\n",
              " 817,\n",
              " 827,\n",
              " 118,\n",
              " 298,\n",
              " 728,\n",
              " 20,\n",
              " 157,\n",
              " 361,\n",
              " 379,\n",
              " 901,\n",
              " 550,\n",
              " 310,\n",
              " 433,\n",
              " 65,\n",
              " 256,\n",
              " 824,\n",
              " 960,\n",
              " 733,\n",
              " 939,\n",
              " 976,\n",
              " 183,\n",
              " 434,\n",
              " 709,\n",
              " 628,\n",
              " 951,\n",
              " 258,\n",
              " 165,\n",
              " 470,\n",
              " 687,\n",
              " 680,\n",
              " 681,\n",
              " 903,\n",
              " 296,\n",
              " 43,\n",
              " 332,\n",
              " 375,\n",
              " 659,\n",
              " 352,\n",
              " 109,\n",
              " 573,\n",
              " 439,\n",
              " 227,\n",
              " 682,\n",
              " 834,\n",
              " 489,\n",
              " 249,\n",
              " 978,\n",
              " 617,\n",
              " 387,\n",
              " 868,\n",
              " 615,\n",
              " 352,\n",
              " 797,\n",
              " 42,\n",
              " 59,\n",
              " 280,\n",
              " 850,\n",
              " 743,\n",
              " 683,\n",
              " 602,\n",
              " 260,\n",
              " 427,\n",
              " 938,\n",
              " 490,\n",
              " 436,\n",
              " 842,\n",
              " 801,\n",
              " 768,\n",
              " 520,\n",
              " 672,\n",
              " 919,\n",
              " 305,\n",
              " 217,\n",
              " 794,\n",
              " 951,\n",
              " 199,\n",
              " 345,\n",
              " 861,\n",
              " 525,\n",
              " 264,\n",
              " 763,\n",
              " 434,\n",
              " 567,\n",
              " 664,\n",
              " 193,\n",
              " 376,\n",
              " 732,\n",
              " 1,\n",
              " 283,\n",
              " 849,\n",
              " 238,\n",
              " 2,\n",
              " 492,\n",
              " 659,\n",
              " 879,\n",
              " 153,\n",
              " 514,\n",
              " 184,\n",
              " 163,\n",
              " 325,\n",
              " 284,\n",
              " 711,\n",
              " 77,\n",
              " 34,\n",
              " 887,\n",
              " 905,\n",
              " 264,\n",
              " 299,\n",
              " 376,\n",
              " 623,\n",
              " 357,\n",
              " 376,\n",
              " 246,\n",
              " 483,\n",
              " 89,\n",
              " 329,\n",
              " 131,\n",
              " 844,\n",
              " 589,\n",
              " 826,\n",
              " 901,\n",
              " 455,\n",
              " 649,\n",
              " 221,\n",
              " 43,\n",
              " 566,\n",
              " 970,\n",
              " 659,\n",
              " 362,\n",
              " 896,\n",
              " 814,\n",
              " 245,\n",
              " 601,\n",
              " 839,\n",
              " 977,\n",
              " 161,\n",
              " 751,\n",
              " 812,\n",
              " 465,\n",
              " 627,\n",
              " 166,\n",
              " 714,\n",
              " 466,\n",
              " 788,\n",
              " 822,\n",
              " 535,\n",
              " 431,\n",
              " 74,\n",
              " 525,\n",
              " 555,\n",
              " 479,\n",
              " 610,\n",
              " 448,\n",
              " 978,\n",
              " 434,\n",
              " 618,\n",
              " 314,\n",
              " 273,\n",
              " 988,\n",
              " 858,\n",
              " 914,\n",
              " 759,\n",
              " 728,\n",
              " 464,\n",
              " 860,\n",
              " 614,\n",
              " 637,\n",
              " 915,\n",
              " 95,\n",
              " 3,\n",
              " 418,\n",
              " 56,\n",
              " 586,\n",
              " 650,\n",
              " 309,\n",
              " 173,\n",
              " 404,\n",
              " 741,\n",
              " 235,\n",
              " 62,\n",
              " 668,\n",
              " 599,\n",
              " 918,\n",
              " 836,\n",
              " 212,\n",
              " 533,\n",
              " 343,\n",
              " 243,\n",
              " 213,\n",
              " 825,\n",
              " 954,\n",
              " 779,\n",
              " 213,\n",
              " 772,\n",
              " 245,\n",
              " 317,\n",
              " 559,\n",
              " 348,\n",
              " 995,\n",
              " 52,\n",
              " 708,\n",
              " 998,\n",
              " 278,\n",
              " 264,\n",
              " 295,\n",
              " 130,\n",
              " 804,\n",
              " 76,\n",
              " 433,\n",
              " 540,\n",
              " 37,\n",
              " 851,\n",
              " 958,\n",
              " 194,\n",
              " 884,\n",
              " 947,\n",
              " 164,\n",
              " 452,\n",
              " 483,\n",
              " 424,\n",
              " 837,\n",
              " 268,\n",
              " 162,\n",
              " 594,\n",
              " 672,\n",
              " 47,\n",
              " 421,\n",
              " 102,\n",
              " 340,\n",
              " 447,\n",
              " 770,\n",
              " 555,\n",
              " 333,\n",
              " 516,\n",
              " 981,\n",
              " 839,\n",
              " 581,\n",
              " 623,\n",
              " 349,\n",
              " 769,\n",
              " 331,\n",
              " 990,\n",
              " 398,\n",
              " 498,\n",
              " 401,\n",
              " 550,\n",
              " 659,\n",
              " 321,\n",
              " 548,\n",
              " 336,\n",
              " 169,\n",
              " 271,\n",
              " 366,\n",
              " 268,\n",
              " 171,\n",
              " 152,\n",
              " 873,\n",
              " 29,\n",
              " 209,\n",
              " 770,\n",
              " 509,\n",
              " 178,\n",
              " 308,\n",
              " 361,\n",
              " 157,\n",
              " 299,\n",
              " 532,\n",
              " 608,\n",
              " 211,\n",
              " 683,\n",
              " 807,\n",
              " 668,\n",
              " 365,\n",
              " 943,\n",
              " 628,\n",
              " 240,\n",
              " 147,\n",
              " 814,\n",
              " 901,\n",
              " 454,\n",
              " 758,\n",
              " 453,\n",
              " 57,\n",
              " 80,\n",
              " 308,\n",
              " 373,\n",
              " 664,\n",
              " 231,\n",
              " 865,\n",
              " 598,\n",
              " 222,\n",
              " 965,\n",
              " 734,\n",
              " 479,\n",
              " 848,\n",
              " 786,\n",
              " 934,\n",
              " 278,\n",
              " 261,\n",
              " 741,\n",
              " 724,\n",
              " 269,\n",
              " 58,\n",
              " 655,\n",
              " 238,\n",
              " 601,\n",
              " 28,\n",
              " 57,\n",
              " 313,\n",
              " 221,\n",
              " 142,\n",
              " 354,\n",
              " 155,\n",
              " 612,\n",
              " 143,\n",
              " 493,\n",
              " 158,\n",
              " 541,\n",
              " 416,\n",
              " 731,\n",
              " 195,\n",
              " 162,\n",
              " 732,\n",
              " 713,\n",
              " 684,\n",
              " 283,\n",
              " 471,\n",
              " 442,\n",
              " 809,\n",
              " 809,\n",
              " 410,\n",
              " 974,\n",
              " 843,\n",
              " 261,\n",
              " 915,\n",
              " 791,\n",
              " 82,\n",
              " 248,\n",
              " 827,\n",
              " 963,\n",
              " 535,\n",
              " 805,\n",
              " 897,\n",
              " 548,\n",
              " 169,\n",
              " 332,\n",
              " 1,\n",
              " 301,\n",
              " 274,\n",
              " 639,\n",
              " 76,\n",
              " 729,\n",
              " 374,\n",
              " 292,\n",
              " 782,\n",
              " 248,\n",
              " 449,\n",
              " 115,\n",
              " 98,\n",
              " 851,\n",
              " 213,\n",
              " 883,\n",
              " 508,\n",
              " 559,\n",
              " 364,\n",
              " 267,\n",
              " 754,\n",
              " 896,\n",
              " 888,\n",
              " 232,\n",
              " 249,\n",
              " 471,\n",
              " 855,\n",
              " 361,\n",
              " 476,\n",
              " 119,\n",
              " 384,\n",
              " 590,\n",
              " 516,\n",
              " 611,\n",
              " 93,\n",
              " 323,\n",
              " 857,\n",
              " 58,\n",
              " 599,\n",
              " 817,\n",
              " 621,\n",
              " 442,\n",
              " 410,\n",
              " 278,\n",
              " 247,\n",
              " 733,\n",
              " 132,\n",
              " 21,\n",
              " 193,\n",
              " 648,\n",
              " 912,\n",
              " 305,\n",
              " 927,\n",
              " 705,\n",
              " 112,\n",
              " 750,\n",
              " 735,\n",
              " 866,\n",
              " 802,\n",
              " 599,\n",
              " 459,\n",
              " 857,\n",
              " 134,\n",
              " 243,\n",
              " 116,\n",
              " 80,\n",
              " 341,\n",
              " 461,\n",
              " 221,\n",
              " 500,\n",
              " 995,\n",
              " 600,\n",
              " 65,\n",
              " 611,\n",
              " 352,\n",
              " 214,\n",
              " 264,\n",
              " 213,\n",
              " 243,\n",
              " 905,\n",
              " 682,\n",
              " 802,\n",
              " 308,\n",
              " 410,\n",
              " 919,\n",
              " 634,\n",
              " 899,\n",
              " 624,\n",
              " 179,\n",
              " 454,\n",
              " 88,\n",
              " 550,\n",
              " 998,\n",
              " 195,\n",
              " 719,\n",
              " 562,\n",
              " 121,\n",
              " 360,\n",
              " 348,\n",
              " 551,\n",
              " 905,\n",
              " 739,\n",
              " 967,\n",
              " 448,\n",
              " 169,\n",
              " 497,\n",
              " 654,\n",
              " 854,\n",
              " 832,\n",
              " 658,\n",
              " 488,\n",
              " 923,\n",
              " 155,\n",
              " 769,\n",
              " 352,\n",
              " 407,\n",
              " 312,\n",
              " 17,\n",
              " 394,\n",
              " 581,\n",
              " 115,\n",
              " 233,\n",
              " 352,\n",
              " 964,\n",
              " 505,\n",
              " 357,\n",
              " 833,\n",
              " 465,\n",
              " 145,\n",
              " 487,\n",
              " 445,\n",
              " 581,\n",
              " 15,\n",
              " 498,\n",
              " 477,\n",
              " 654,\n",
              " 290,\n",
              " 796,\n",
              " 967,\n",
              " 238,\n",
              " 495,\n",
              " 612,\n",
              " 435,\n",
              " 801,\n",
              " 195,\n",
              " 169,\n",
              " 593,\n",
              " 855,\n",
              " 430,\n",
              " 548,\n",
              " 317,\n",
              " 178,\n",
              " 900,\n",
              " 186,\n",
              " 195,\n",
              " 995,\n",
              " 525,\n",
              " 720,\n",
              " 521,\n",
              " 498,\n",
              " 962,\n",
              " 657,\n",
              " 55,\n",
              " 399,\n",
              " 282,\n",
              " 188,\n",
              " 225,\n",
              " 552,\n",
              " 746,\n",
              " 671,\n",
              " 325,\n",
              " 222,\n",
              " 869,\n",
              " 124,\n",
              " 51,\n",
              " 602,\n",
              " 671,\n",
              " 612,\n",
              " 352,\n",
              " 336,\n",
              " 264,\n",
              " 761,\n",
              " 386,\n",
              " 685,\n",
              " 602,\n",
              " 891,\n",
              " 476,\n",
              " 423,\n",
              " 588,\n",
              " 927,\n",
              " 837,\n",
              " 555,\n",
              " 248,\n",
              " 522,\n",
              " 156,\n",
              " 699,\n",
              " 110,\n",
              " 861,\n",
              " 975]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "epWgiqQHnA5E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "729be2a8-6cc3-4d60-85ee-c7559e234a2e"
      },
      "cell_type": "code",
      "source": [
        "rdd.take(5)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[905, 343, 991, 351, 130]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "GiQqw6x0t1Qi",
        "colab_type": "code",
        "outputId": "53b87f7e-e791-4a1b-8c18-a2397bebea1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "print type(random_array)\n",
        "print type(rdd)\n",
        "print type(rdd.collect())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<type 'numpy.ndarray'>\n",
            "<class 'pyspark.rdd.RDD'>\n",
            "<type 'list'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IMGznHMkt1Qk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### RDD為Apache Spark最核心之概念，有別於MapRduce，僅提供Map()與Reduce()兩項操作。 RDD提供兩大類別Transformation與Action\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "mJUQZL9lt1Qm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "> <img src=\"https://www.dropbox.com/s/omfoi3uzcgapcm4/rdd%20transformation%20concept.png?dl=1\" width=\"500\" align=\"left\">  "
      ]
    },
    {
      "metadata": {
        "id": "kC-QgXwjt1Qm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "> <img src=\"https://www.dropbox.com/s/3u8gt5376qq5vjy/spark%20core.png?dl=1\" width=\"500\" align=\"left\">"
      ]
    },
    {
      "metadata": {
        "id": "TQtTc14Wt1Qm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 最基本之Action操作  \n",
        "### 使用 collect( ) 將分散於各機器之資料，收集成為單機資料集\n",
        "<img src=\"https://www.dropbox.com/s/pjv20pl5wkevjf6/collect.png?dl=1\" width=\"500\" align=\"left\">\n"
      ]
    },
    {
      "metadata": {
        "id": "mN3hI6UAt1Qn",
        "colab_type": "code",
        "outputId": "d932bcb2-f65e-45c1-ca86-0626b4300212",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "rdd.first()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "fa8Z0Zwht1Qp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Transformation 觀念介紹"
      ]
    },
    {
      "metadata": {
        "id": "XFThLRfCt1Qq",
        "colab_type": "code",
        "outputId": "874a72b1-3889-498c-ecdf-f5f622bcda45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "def doubling(x):\n",
        "    return x*2\n",
        "\n",
        "print doubling(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cVLdPhDNt1Qs",
        "colab_type": "code",
        "outputId": "39e38811-3850-48ad-dc93-fa09da99df24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "rdd.take(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[16, 55, 700, 749, 238]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "FqAxyXPqt1Qu",
        "colab_type": "code",
        "outputId": "f4844d11-3933-4ca0-fcc1-de53f48757fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "rdd.map(doubling).take(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[32, 110, 1400, 1498, 476]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "0FQiZbX3wFvf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def minusone(y):\n",
        "  return y-1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tGwRUarGwVio",
        "colab_type": "code",
        "outputId": "f340b723-6853-42cd-d9cf-9445e4149bf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "rdd.map(minusone).take(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[15, 54, 699, 748, 237]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "PCP1XQ-mt1Qx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 注意：匿名函式的使用  \n",
        "    rdd.map(doubling) = rdd.map(lambda x: x*2)"
      ]
    },
    {
      "metadata": {
        "id": "UbQ1fKJroYCK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "38e9b301-5383-4f8d-9a39-c8c841db314d"
      },
      "cell_type": "code",
      "source": [
        "rdd.map(lambda y:y*2).take(5)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1810, 686, 1982, 702, 260]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "F00w0ISRhPck",
        "colab_type": "code",
        "outputId": "afe3e5a6-2d69-4dc4-9a86-8a7ca39d6264",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "rdd.map(lambda x: x-1).take(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[15, 54, 699, 748, 237]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "4ZdpdcELsJvu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rdd.reduce(lambda x,y: x*x+y*y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dayDsTWWs4fK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rdd.map(lambda x:x*x).map(lambda x:x-1).reduce(lambda x,y:x+y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YvZ2AgidqUkA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ebc9573f-cca9-47e0-829a-43564724435b"
      },
      "cell_type": "code",
      "source": [
        "rdd.map(lambda x:1).reduce(lambda x,y:x+y)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "3ISApy-8o-Nt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b9b7a5d1-510d-48ca-f585-bb5dec3403c9"
      },
      "cell_type": "code",
      "source": [
        "rdd.reduce(lambda x,y:x+y)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500062"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "paFPu3J9yUng",
        "colab_type": "code",
        "outputId": "ac0bb5de-3551-4eda-ba32-b1700016f18b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "rdd.map(lambda yy:0).take(2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "4Anc1mSLzEmp",
        "colab_type": "code",
        "outputId": "33253c38-7c90-48eb-b954-f4ef53c224f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "rdd.map(lambda x: x-1).map(lambda x:x-2).map(lambda x:x-3).take(2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[10, 49]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "iJeJB9tNt1Qx",
        "colab_type": "code",
        "outputId": "188aa4cb-7018-4bfb-e47c-297cd93af3c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "464176"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "3rfUg7nrt1Q0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 練習：使用map()，將所有數字開平方根 \n",
        "    import math\n",
        "    print math.sqrt(5)"
      ]
    },
    {
      "metadata": {
        "id": "injnEU2Lt1Q1",
        "colab_type": "code",
        "outputId": "7123cbd9-c094-4e4b-fa75-7eab1e8b8ea9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "rdd.map(lambda x: math.sqrt(x)).take(2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[20.83266665599966, 19.467922333931785]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "n7PtJhL1t1Q4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 匿名函式的差異"
      ]
    },
    {
      "metadata": {
        "id": "LTFiSaMdt1Q5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "def sqrt(x):\n",
        "    return math.sqrt(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2Cz3ZPi1iL06",
        "colab_type": "code",
        "outputId": "04f4c589-ee67-42be-810e-1802499ce766",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "rdd.map(lambda y: math.sqrt(y)).take(2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[7.211102550927978, 26.814175355583846]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "zCnE4mS_iJ1m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "A75twst-t1Q7",
        "colab_type": "code",
        "outputId": "e9168d29-4409-421e-f3e8-e822c308a514",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "rdd.map(sqrt).take(2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[7.211102550927978, 26.814175355583846]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "olf5dYNO1JP8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Transformation Operators: map(), filter(), sample(), groupBy(), etc \n",
        "## (完整Transformation Operator請參考下列網址) http://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.RDD "
      ]
    },
    {
      "metadata": {
        "id": "m-dnb6Vb-RnK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "rdd資料中有多少筆?"
      ]
    },
    {
      "metadata": {
        "id": "qBhcOOPJ9sUa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rdd.map(lambda x:1).reduce(lambda x,y:x+y) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "952iKzm8-QqL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rdd.count()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9xKLWBJdvfAz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def checkeven(x):\n",
        "  if x%2 ==0:\n",
        "    return 1\n",
        "  return 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aZZp3EF8vvkW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rdd.map(checkeven).reduce(lambda x,y:x+y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HMYWs-X-1Usl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rdd.map(lambda x: x if x%2==0 else 0).reduce(lambda x,y:x+y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "elsAvKXNt1Q-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Transformation Operator 使用filter()，將所有偶數留下，奇數刪除。"
      ]
    },
    {
      "metadata": {
        "id": "raH2ldfBt1Q-",
        "colab_type": "code",
        "outputId": "2c9476d0-b028-41fd-e33d-d870b75a1d66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "rdd.filter(lambda x: x%2==0).count()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "482"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "uTLr4oJEt1RB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Transformation Operator 使用sample( ) 抽樣給定比例之RDD子集合"
      ]
    },
    {
      "metadata": {
        "id": "n7U8Q8G-t1RB",
        "colab_type": "code",
        "outputId": "e3bc7256-df65-4a53-8ae8-ec2400a33748",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "subsetrdd = rdd.sample(False, 0.01)\n",
        "subsetrdd.collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[379, 925, 307, 337, 532, 973, 408, 541, 12, 699, 550, 394, 456, 415]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "8jpIFRx3t1RE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Transformation Operator: 使用 groupBy( ) 將資料分組"
      ]
    },
    {
      "metadata": {
        "id": "jffxeQevt1RE",
        "colab_type": "code",
        "outputId": "614bedde-dd5e-4973-8942-dbb103289dc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "result = subsetrdd.groupBy(lambda x: x%2==0).collect()\n",
        "print result\n",
        "print [(x, list(y)) for (x, y) in result]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(False, <pyspark.resultiterable.ResultIterable object at 0x7f4286a82350>), (True, <pyspark.resultiterable.ResultIterable object at 0x7f4286a82f50>)]\n",
            "[(False, [379, 925, 307, 337, 973, 541, 699, 415]), (True, [532, 408, 12, 550, 394, 456])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fmcD3Rb41HDj",
        "colab_type": "code",
        "outputId": "f2f71b13-989e-436e-f126-3befbf20a871",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 0, 1, 0, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "id": "dhVPws5qt1RG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Transformation Operator 使用map ( ) 產生 key value pair"
      ]
    },
    {
      "metadata": {
        "id": "-eo0RYkqt1RH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "keyValueRdd = rdd.map(lambda x: ('偶數', x) if x%2==0 else ('基數',x))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HUVe61KU5iqQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v7s_mWe6_-ab",
        "colab_type": "code",
        "outputId": "5ad02ffd-fefd-49db-f404-074e7411b677",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "rdd.map(lambda x: ('even', x) if x%2==0 else ('odd',x))\\\n",
        "   .reduceByKey(lambda x,y: x+y)\\\n",
        "   .take(2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('even', 252194), ('odd', 243733)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "metadata": {
        "id": "6bMHQXhd7XOW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "outputId": "54d72006-7ec5-4d91-bc2d-ad6f680f52ca"
      },
      "cell_type": "code",
      "source": [
        "rdd.map(lambda x: (x, 1))\\\n",
        "   .map(lambda x: (x[1],x[0]))\\\n",
        "   .reduceByKey(lambda x,y: x+y)\\\n",
        "   .collect()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(768, 1),\n",
              " (2, 1),\n",
              " (516, 2),\n",
              " (854, 1),\n",
              " (520, 1),\n",
              " (522, 1),\n",
              " (514, 1),\n",
              " (658, 1),\n",
              " (600, 2),\n",
              " (20, 3),\n",
              " (22, 1),\n",
              " (536, 1),\n",
              " (772, 1),\n",
              " (802, 2),\n",
              " (28, 1),\n",
              " (542, 1),\n",
              " (32, 1),\n",
              " (688, 1),\n",
              " (34, 3),\n",
              " (548, 3)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "KjpBTvuY6kJr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evennumber(x):\n",
        "  if x%2==0:\n",
        "    return x\n",
        "  else:\n",
        "    return 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WP-RPBA36wYD",
        "colab_type": "code",
        "outputId": "36ddaff7-d3b6-4094-f38d-48cd9c3f53ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "evennumber(101)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "metadata": {
        "id": "w5hM0Gow7WOl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rdd.reduce(lambda x,y: x*x+y*y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TBWfbG8S60vF",
        "colab_type": "code",
        "outputId": "94280e76-1854-4fd5-88e5-11374c4f6562",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "rdd.map(evennumber).reduce(lambda x,y:x+y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "252194"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "id": "ipn1B9MJ3xND",
        "colab_type": "code",
        "outputId": "0c77ec89-3f74-4381-d366-a508c3a4cfeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "rdd.map(lambda x: x if x%2==0 else 0).reduce(lambda x,y:x+y)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "252194"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "ergjRAqbt1RI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Transformation Operator: 使用 groupbyKey ( ) 根據key將資料分組"
      ]
    },
    {
      "metadata": {
        "id": "0lU_2LrIt1RJ",
        "colab_type": "code",
        "outputId": "a41529e2-7ce3-4a2d-fd06-a06c4c4a848d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "keyValueRdd = rdd.map(lambda x: ('even', x) if x%2==0 else ('odd',x))\n",
        "\n",
        "for x in keyValueRdd.groupByKey().collect():\n",
        "    print x[0], list(x[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "even [386, 886, 606, 212, 374, 434, 464, 226, 528, 212, 220, 868, 730, 742, 506, 784, 834, 284, 620, 364, 652, 134, 482, 426, 92, 492, 180, 842, 894, 246, 970, 792, 150, 780, 10, 194, 44, 958, 588, 576, 878, 152, 150, 42, 412, 986, 798, 960, 950, 388, 550, 292, 738, 440, 552, 314, 0, 678, 200, 680, 528, 582, 774, 528, 22, 986, 88, 754, 294, 888, 30, 986, 496, 138, 724, 22, 242, 630, 120, 416, 6, 712, 756, 60, 670, 984, 944, 344, 674, 178, 650, 118, 700, 880, 154, 204, 860, 706, 564, 14, 226, 788, 664, 934, 110, 954, 114, 572, 760, 566, 402, 430, 782, 750, 416, 746, 304, 720, 802, 140, 726, 338, 834, 34, 516, 990, 348, 870, 970, 90, 452, 82, 60, 498, 710, 108, 74, 112, 6, 108, 176, 632, 498, 704, 756, 482, 344, 626, 744, 646, 864, 918, 664, 442, 224, 54, 850, 814, 672, 634, 768, 456, 636, 278, 234, 346, 710, 594, 566, 386, 352, 232, 600, 430, 390, 392, 600, 194, 636, 838, 960, 846, 214, 536, 562, 962, 700, 646, 390, 814, 198, 640, 330, 640, 60, 662, 388, 978, 108, 688, 918, 404, 574, 176, 334, 742, 104, 418, 974, 378, 660, 170, 992, 254, 414, 820, 38, 796, 858, 18, 876, 538, 84, 776, 772, 536, 364, 678, 638, 46, 948, 670, 768, 816, 476, 564, 312, 60, 214, 918, 564, 712, 670, 430, 396, 386, 282, 922, 618, 384, 88, 50, 992, 136, 472, 636, 962, 596, 958, 666, 892, 134, 546, 808, 634, 466, 448, 558, 774, 362, 676, 166, 582, 148, 374, 230, 804, 628, 958, 400, 134, 510, 22, 574, 826, 966, 566, 236, 302, 844, 262, 896, 734, 308, 228, 628, 618, 714, 320, 574, 602, 754, 734, 554, 344, 224, 578, 82, 836, 360, 152, 420, 890, 988, 742, 36, 86, 694, 270, 990, 898, 126, 282, 494, 94, 394, 200, 790, 774, 738, 826, 798, 684, 934, 894, 724, 962, 712, 818, 8, 472, 592, 428, 800, 766, 452, 158, 362, 284, 558, 264, 298, 730, 92, 388, 26, 710, 92, 410, 108, 164, 986, 352, 452, 458, 348, 916, 974, 960, 900, 556, 168, 236, 746, 560, 6, 644, 938, 476, 494, 854, 134, 880, 822, 700, 42, 984, 136, 964, 270, 654, 746, 94, 804, 180, 554, 476, 924, 686, 848, 578, 528, 920, 572, 760, 460, 88, 724, 834, 890, 148, 812, 924, 244, 462, 102, 890, 158, 524, 446, 364, 328, 252, 754, 52, 564, 878, 184, 152, 78, 736, 478, 824, 68, 374, 996, 728, 928, 486, 236, 750, 748, 242, 142, 30, 854, 374, 474, 874, 458, 206, 684, 298, 982, 946, 792, 468, 110, 166, 556, 612, 296, 720, 782, 456, 630, 302, 302, 534, 408, 934, 768, 68, 384, 160, 260, 772, 216, 814, 202, 176, 320, 826, 898, 764, 582, 638, 796, 438, 652, 68, 482, 736, 356, 252, 260, 4, 978, 946, 152, 214, 712, 290, 280]\n",
            "odd [335, 659, 35, 619, 427, 505, 65, 483, 275, 249, 627, 453, 85, 533, 665, 3, 31, 239, 397, 607, 133, 681, 49, 917, 79, 315, 879, 349, 33, 383, 419, 113, 391, 307, 499, 645, 17, 221, 409, 593, 871, 787, 929, 587, 579, 585, 139, 257, 637, 251, 679, 447, 609, 301, 513, 679, 469, 387, 613, 193, 461, 693, 325, 839, 37, 713, 131, 841, 349, 23, 219, 339, 509, 69, 43, 977, 943, 253, 247, 691, 551, 423, 721, 167, 59, 65, 5, 81, 99, 421, 185, 555, 43, 855, 395, 999, 145, 755, 207, 373, 903, 299, 641, 377, 75, 527, 633, 865, 453, 67, 483, 779, 969, 161, 135, 759, 225, 279, 43, 85, 11, 567, 987, 35, 377, 371, 975, 771, 311, 605, 857, 153, 665, 629, 141, 371, 317, 77, 11, 77, 411, 783, 147, 829, 99, 205, 777, 105, 901, 153, 127, 47, 241, 487, 847, 87, 785, 549, 755, 319, 313, 189, 745, 337, 517, 153, 671, 439, 107, 819, 399, 715, 567, 323, 583, 439, 533, 31, 927, 451, 799, 785, 583, 557, 851, 419, 123, 353, 785, 191, 15, 89, 677, 589, 71, 347, 633, 919, 33, 343, 901, 425, 369, 555, 875, 241, 323, 201, 365, 175, 481, 539, 1, 789, 873, 803, 495, 739, 785, 815, 1, 109, 595, 451, 651, 69, 13, 679, 629, 811, 481, 415, 961, 935, 619, 141, 265, 149, 233, 465, 619, 813, 301, 569, 39, 241, 763, 479, 1, 443, 951, 955, 671, 839, 957, 115, 919, 515, 813, 527, 95, 819, 731, 697, 305, 73, 157, 411, 427, 629, 915, 23, 387, 29, 231, 43, 891, 33, 839, 69, 287, 759, 113, 173, 905, 101, 741, 31, 571, 829, 813, 993, 123, 177, 529, 891, 669, 137, 25, 873, 1, 959, 465, 341, 729, 103, 965, 561, 491, 833, 563, 887, 449, 99, 295, 367, 817, 243, 161, 147, 687, 857, 63, 405, 851, 261, 255, 635, 51, 183, 695, 281, 299, 721, 137, 463, 433, 213, 525, 761, 119, 407, 347, 307, 73, 15, 169, 721, 107, 239, 235, 625, 819, 161, 277, 701, 853, 889, 7, 879, 631, 663, 325, 421, 561, 609, 359, 33, 163, 363, 37, 207, 771, 539, 735, 343, 691, 893, 387, 455, 53, 337, 543, 919, 535, 885, 641, 691, 701, 115, 37, 695, 873, 901, 561, 269, 963, 35, 185, 681, 731, 55, 831, 785, 767, 845, 833, 195, 391, 737, 377, 71, 669, 549, 335, 13, 483, 627, 707, 633, 479, 587, 527, 293, 349, 95, 239, 961, 299, 43, 221, 831, 777, 965, 447, 501, 807, 415, 779, 681, 207, 865, 631, 613, 737, 757, 809, 487, 415, 721, 329, 299, 643, 477, 977, 543, 561, 545, 189, 551, 665, 697, 269, 11, 553, 269, 573, 447, 659, 385, 423, 521, 69, 851, 199, 619, 469, 117, 639, 387, 929, 839, 587, 497, 711, 77, 191, 987, 425, 441, 871, 171, 105, 923, 791, 677]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2cF4eO2ht1RL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "###練習 計算奇數加總值與偶數加總值\n",
        "#### groupbyKey(), reduceByKey()"
      ]
    },
    {
      "metadata": {
        "id": "JvI54vHQt1RL",
        "colab_type": "code",
        "outputId": "4ab6d42f-2f68-4a0b-ec58-95846be98ef4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "keyValueRdd.reduceByKey(lambda x,y: x+y).take(2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('even', 240850), ('odd', 229326)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "metadata": {
        "id": "2zt-atriGcY1",
        "colab_type": "code",
        "outputId": "559557cc-d178-41b3-d25f-582ae6632927",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "rdd.map(lambda x: (x,1)).reduceByKey(lambda a,b:a+b).take(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 1), (512, 1), (514, 1), (4, 1), (518, 3)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "metadata": {
        "id": "GeI84meUt1Re",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "___\n",
        "___\n",
        "___\n",
        "\n",
        "\n",
        "# RDD Action Action Operation\n",
        "#### 使用reduce()，計算所有數之加總值。"
      ]
    },
    {
      "metadata": {
        "id": "YML9Ihvet1Re",
        "colab_type": "code",
        "outputId": "c5b19b1c-6787-46c4-cf83-655d78323567",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "rdd.reduce(lambda x,y: x+y )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "508189"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "K7VhWlCUfIrn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rdd.reduce(lambda x,y: x*x+y*y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GYMr5QHb0Isc",
        "colab_type": "code",
        "outputId": "2146c730-a543-423c-da05-7126d803a878",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "rdd.map(lambda x:1).reduce(lambda a,b:a+b)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "vjiNSGoD0p1r",
        "colab_type": "code",
        "outputId": "d850579d-80b7-41fc-b8e2-6c4912c5653e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 1, 1, 1, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "metadata": {
        "id": "OQWCsi3Nt1Rq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "______\n",
        "\n",
        "## 觀念\n",
        "### 練習：計算所有數字平方和。"
      ]
    },
    {
      "metadata": {
        "id": "4cFvuXYX2iA7",
        "colab_type": "code",
        "outputId": "7f81a8a5-5e95-4d3d-cc51-1c650b0ad31b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "rdd.map(lambda x: x*x).reduce(lambda x,y: x+y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "306397002"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "id": "6rJDXbsZt1Rr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 練習：使用map(), reduceByKey()，計算所有數字出現頻率。"
      ]
    },
    {
      "metadata": {
        "id": "z13lFWqgt1Rr",
        "colab_type": "code",
        "outputId": "405f2f61-d52e-44fe-cf27-be783a282d37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "rdd.map(lambda x: (x,1)).reduceByKey(lambda a,b:a+b)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PythonRDD[37] at RDD at PythonRDD.scala:48"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "metadata": {
        "id": "0N__bRU5t1Rv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "_____\n",
        "_____\n",
        "_____\n",
        "\n",
        "# WordCount"
      ]
    },
    {
      "metadata": {
        "id": "R_cMDKMet1Rx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "#f=urllib.urlretrieve(\"https://www.ccel.org/ccel/bible/kjv.txt\",\"bible\")\n",
        "f=urllib.urlretrieve(\"https://www.dropbox.com/s/28ljfwb1aeuyi37/speech.txt?dl=1\",\"speech\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vap6A2cVFRZy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1603
        },
        "outputId": "92973c7c-6106-4dc1-8bff-93a9f396fb80"
      },
      "cell_type": "code",
      "source": [
        "!cat speech"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "各位 友邦 的 元首 與 貴賓 、 各國 駐台 使節 及 代表 、 現場 的 好 朋友 ， 全體 國人 同胞 ， 大家 好 。   \n",
            "感謝 與 承擔   就 在 剛剛 ， 我 和 陳建仁 已經 在 總統府 裡面 ， 正式 宣誓 就任 中華民國 第十四 任 \n",
            "總統 與 副 總統 。 我們 要 感謝 這塊 土地 對 我們 的 栽培 ， 感謝 人民 對 我們 的 信任 ， \n",
            "以及 ， 最 重要 的 ， 感謝 這個 國家 的 民主 機制 ， 讓 我們 透過 和平 的 選舉 過程 ， 實現 第三次 政黨 輪替 ，\n",
            "並且 克服 種種 不 確定 因素 ， 順利 渡過 長達 四個 月 的 交接 期 ， 完成 政權 和平 移轉 。   台灣 ， \n",
            "再 一次 用 行動 告訴 世界 ， 作為 一群 民主 人 與 自由人 ， 我們 有 堅定 的 信念 ， 去 捍衛 民主自由 的 生活 方式 。 \n",
            "這段 旅程 ， 我們 每 一個 人 都 參與 其中 。 親愛 的 台灣 人民 ， 我們 做到 了 。   我要 告訴 大家 ， \n",
            "對於 一月 十六日 的 選舉 結果 ， 我 從來 沒有 其他 的 解讀 方式 。 人民 選擇 了 新 總統 、 新政府 ， 所 期待 的 就是 四個 字 \n",
            "： 解決問題 。 此時此刻 ， 台灣 的 處境 很 困難 ， 迫切需要 執政者 義無反顧 的 承擔 。 這 一點 ， 我 不會 忘記 。 \n",
            "我 也 要 告訴 大家 ， 眼前 的 種種 難關 ， 需要 我們 誠實 面對 ， 需要 我們 共同 承擔 。 所以 ， 這個 演說 是 一個 邀請 ， \n",
            "我要 邀請 全體 國人 同胞 一 起來 ， 扛起 這個 國家 的 未來 。   國家 不會 因為 領導人 而 偉大 ； 全體 國民 的 共同奮鬥 ， \n",
            "才 讓 這個 國家 偉大 。 總統 該 團結 的 不 只是 支持者 ， 總統 該 團結 的 是 整個 國家 。 團結 是 為 了 改變 ， \n",
            "這是 我 對 這個 國家 最 深切 的 期待 。 在 這裡 ， 我要 誠懇 地 呼籲 ， 請給 這個 國家 一個 機會 ， 讓 我們 拋下 成見 ，\n",
            "拋下 過去 的 對立 ， 我們 一 起來 完成 新 時代 交給 我們 的 使命 。   在 我們 共同奮鬥 的 過程 中 ， 身為 總統 ， \n",
            "我要 向 全國 人民 宣示 ， 未來 我 和 新政府 ， 將 領導 這個 國家 的 改革 ， 展現 決心 ， 絕不 退縮 。   \n",
            "為 年輕人 打造 一個 更好 的 國家   未來 的 路並 不好 走 ， 台灣 需要 一個 正面 迎向 一切 挑戰 的 新政府 ， \n",
            "我 的 責任 就是 領導 這個 新政府 。   我們 的 年金 制度 ， 如果 不改 ， 就 會 破產 。   \n",
            "我們 僵化 的 教育 制度 ， 已經 逐漸 與 社會 脈動 脫節 。   我們 的 能源 與 資源 十分 有限 ， \n",
            "我們 的 經濟 缺乏 動能 ， 舊 的 代工 模式 已經 面臨 瓶頸 ， 整個 國家 極 需要 新 的 經濟 發展 模式 。   \n",
            "我們 的 人口 結構 急速 老化 ， 長 照 體系 卻 尚未 健全 。   我們 的 人口 出生率 持續 低落 ， \n",
            "完善 的 托育 制度 卻 始終 遙遙無期 。   我們 環境 汙染 問題 仍然 嚴重 。   我們 國家 的 財政 並不 樂觀 。  \n",
            "我們 的 司法 已經 失去 人民 的 信任 。   我們 的 食品安全 問題 ， 困擾 著 所有 家庭 。   我們 的 貧富差距 越來越 嚴重 。  \n",
            "我們 的 社會 安全網 還是 有 很多 破洞 。   最 重要 的 ， 我要 特別 強調 ， 我們 的 年輕人 處於 低薪 的 處境 ， 他們 的 人生 ， \n",
            "動彈不得 ， 對於 未來 ， 充滿 無奈 與 茫然 。   年輕人 的 未來 是 政府 的 責任 。 如果 不 友善 的 結構 沒有 改變 ， \n",
            "再 多 個人 菁英 的 出現 ， 都 不足以 讓 整體 年輕人 的 處境 變好 。 我 期許 自己 ， 在 未來 的 任期 之內 ， 要 一步 一步 ， \n",
            "從 根本 的 結構 來 解決 這個 國家 的 問題 。   這 就是 我 想 為 台灣 的 年輕人 做 的 事 。 \n",
            "雖然 我 沒有 辦法 立刻 幫 所有 的 年輕人 加薪 ， 但是 我 願意 承諾 ， 新政府 會 立刻 展開 行動 。 \n",
            "請給 我們 一點 時間 ， 也 請 跟 我們 一起 走上 改革 的 這 一條 路 。   改變 年輕人 的 處境 ， 就是 改變 國家 的 處境 。 \n",
            "一個 國家 的 年輕人 沒有 未來 ， 這個 國家 必定 沒有 未來 。 幫助 年輕人 突破 困境 ， 實現 世代 正義 ， \n",
            "把 一個 更好 的 國家 交到 下一代 手上 ， 就是 新政府 重大 的 責任 。   \n",
            "第一 、   經濟 結構 的 轉型   要 打造 一個 更好 的 國家 ， 未來 ， 新政府 要 做到 以下 幾件 事情 。  \n",
            "首先 ， 就是 讓 台灣 的 經濟 結構 轉型 。 這是 新政府 所 必須 承擔 的 最 艱鉅 使命 。 我們 不要 妄自菲薄 ， \n",
            "更 不要 失去 信心 。 台灣 有 很多 別的 國家 沒有 的 優勢 ， 我們 有 海洋 經濟 的 活力 和 靭 性 ， 高素質 的 人力資源 、 \n",
            "務實 可靠 的 工程師 文化 、 完整 的 產業鏈 、 敏捷 靈活 的 中小企業 ， 以及 ， 永不 屈服 的 創業精神 。   \n",
            "我們 要 讓 台灣 經濟 脫胎換骨 ， 就 必須 從現在起 就 下定決心 ， 勇敢 地 走出 另外 一條 路 。 這 一條 路 ， \n",
            "就是 打造 台灣 經濟 發展 的 新 模式 。   新政府 將 打造 一個 以 創新 、 就業 、 分配 為 核心 價值 ， \n",
            "追求 永續 發展 的 新 經濟模式 。 改革 的 第一步 ， 就是 強化 經濟 的 活力 與 自主性 ， 加強 和 全球 及 區域 的 連結 ， \n",
            "積極 參與 多邊 及 雙邊 經濟 合作 及 自由 貿易談判 ， 包括 TPP 、 RCEP 等 ， 並且 ， 推動 新 南向 政策 ， \n",
            "提升 對外 經濟 的 格局 及 多元性 ， 告別 以往 過於 依賴 單一 市場 的 現象 。   除此之外 ， 新政府 相信 ， \n",
            "唯有 激發 新 的 成長 動能 ， 我們 才能 突破 當前 經濟 的 停滯不前 。 我們 會以 出口 和 內需 作為 雙引擎 ， \n",
            "讓 企業 生產 和 人民 生活 互為 表裡 ， 讓 對外貿易 和 在 地 經濟 緊密 連結 。   我們 會 優先 推動 五大 創新 研發 計畫 ， \n",
            "藉 著 這些 產業 來 重新 塑造 台灣 的 全球 競爭力 。 我們 也 要 積極 提升 勞動 生產力 ， 保障 勞工 權益 ，\n",
            "讓 薪資 和 經濟 成長 能 同步 提升 。   這是 台灣 經濟 發展 的 關鍵時刻 。 我們 有 決心 ， 也 有 溝通 能力 。 \n",
            "我們 已經 有 系統性 的 規劃 ， 未來 ， 會以 跨部會 聯手 的 模式 ， 把 整個 國家 的 力量 集結 起來 ， \n",
            "一 起來 催生 這個 新 模式 。   在 經濟 發展 的 同時 ， 我們 不要 忘記 對 環境 的 責任 。 經濟 發展 的 新 模式 會 和 國土規劃 、 \n",
            "區域 發展 及 環境 永續 ， 相互 結合 。 產業 的 佈局 和 國土 的 利用 ， 應該 拋棄 零碎 的 規畫 ， 和 短視 近利 的 眼光 。 \n",
            "我們 必須 追求 區域 的 均衡 發展 ， 這 需要 中央 來 規畫 、 整合 ， 也 需要 地方 政府 充分發揮 區域 聯合 治理 的 精神 。   \n",
            "我們 也 不能 再 像 過去 ， 無止盡 地 揮霍 自然資源 及 國民 健康 。 所以 ， 對 各種 汙染 的 控制 ， 我們 會 嚴格把關 ， \n",
            "更要 讓 台灣 走向 循環 經濟 的 時代 ， 把 廢棄物 轉換 為 再生資源 。 對於 能源 的 選擇 ， 我們 會以 永續 的 觀念 去 逐步 調整 。\n",
            "新政府 會 嚴肅 看待 氣候變遷 、 國土 保育 、 災害 防治 的 相關 議題 ， 因為 ， 我們 只有 一個 地球 ， 我們 也 只有 一個 台灣 。  \n",
            "第二 、   強化 社會 安全網   新政府 必須 要 承擔 的 第二件 事情 ， 就是 強化 台灣 的 社會 安全網 。 這些 年 ， \n",
            "幾件 關於 兒少 安全 及 隨機 殺人 的 事件 ， 都 讓 整個 社會 震驚 。 不過 ， 一個 政府 不能 永遠 在 震驚 ， \n",
            "它 必須 要 有 同理 心 。 沒有 人 可以 替 受害者 家屬 承受 傷痛 ， 但是 ， 一個 政府 ， 尤其 是 第一線 處理 問題 的 人 ， \n",
            "必須 要 讓 受害者 以及 家屬 覺得 ， 不幸 事件 發生 的 時候 ， 政府 是 站 在 他們 這 一邊 。   除了 同理 心 之外 ， \n",
            "政府 更 應該 要 提出 解決 的 方法 。 全力 防止 悲劇 一再 發生 ， 從 治安 、 教育 、 心理健康 、 社會工作 等 各個 面向 ， \n",
            "積極 把 破洞 補 起來 。 尤其 是 治安 與 反毒 的 工作 ， 這些 事情 ， 新政府 會用 最 嚴肅 的 態度 和 行動 來 面對 。  \n",
            "在 年金 的 改革 方面 ， 這是 攸關 台灣 生存 發展 的 關鍵 改革 ， 我們 不 應該 遲疑 ， 也 不 可以 躁進 。 \n",
            "由 陳建仁 副 總統 擔任 召集人 的 年金 改革 委員會 ， 已經 緊鑼密鼓 在 籌備 之中 。 過去 的 政府 在 這個 議題 上 ，\n",
            "曾經 有過 一些 努力 。 但是 ， 缺乏 社會 的 參與 。 新政府 的 做法 ， 是 發動 一個 集體 協商 ， 因為 年金 改革 必須 是 \n",
            "一個 透過 協商 來 團結 所有人 的 過程 。   這 就是 為 什麼 ， 我們 要 召開 年金 改革 國是會議 ， 由 不同 階層 、 \n",
            "不同 職業 代表 ， 在 社會 團結 的 基礎 上 ， 共同 協商 。 一年 之內 ， 我們 會 提出 可行 的 改革方案 。 無論是 勞工 還是 \n",
            "公務員 ， 每 一個 國民 的 退休 生活 都 應該 得到 公平 的 保障 。   另外 ， 在 長期 照顧 的 議題 上 ， 我們 將會 把 優質 、\n",
            "平價 、 普及 的 長期 照顧 系統 建立 起來 。 和 年金 改革 一樣 ， 長 照 體系 也 是 一個 社會 總動員 的 過程 。 \n",
            "新政府 的 做法 是 由 政府 主導 和 規劃 ， 鼓勵 民間 發揮 社區 主義 的 精神 ， 透過 社會 集體 互助 的 力量 ， \n",
            "來 建立 一套 妥善 而 完整 的 體系 。 每 一個 老年人 都 可以 在 自己 熟悉 的 社區 ， 安心 享受 老年 生活 ，\n",
            "每 一個 家庭 的 照顧 壓力 將會 減輕 。 照顧 老人 的 工作 不能 完全 讓 它 變成 自由市場 。 我們 會 把 責任 扛起來 ， \n",
            "按部就班 來 規劃 與 執行 ， 為 超 高齡 社會 的 來臨 ， 做好 準備 。   第三 、   社會 的 公平 與 正義  \n",
            "新政府 要 承擔 的 第三件 事情 ， 就是 社會 的 公平 與 正義 。 在 這個 議題 上 ， 新政府 會 持續 和 公民 社會 一起 合作 ， \n",
            "讓 台灣 的 政策 更 符合 多元 、 平等 、 開放 、 透明 、 人權 的 價值 ， 讓 台灣 的 民主 機制 更加 深化 與 進化 。   \n",
            "新 的 民主制度 要 能夠 上路 ， 我們 必須 先 找出 面對 過去 的 共同 方法 。 未來 ， 我會 在 總統府 成立 真 相與 和解 委員會 ， \n",
            "用 最 誠懇 與 謹慎 的 態度 ， 來 處理 過去 的 歷史 。 追求 轉型 正義 的 目標 是 在 追求 社會 的 真正 和解 ， \n",
            "讓 所有 台灣 人 都 記取 那個 時代 的 錯誤 。   我們 將從 真相 的 調查 與 整理 出發 ， 預計 在 三年 之內 ， \n",
            "完成 台灣 自己 的 轉型 正義 調查 報告書 。 我們 將會 依據 調查報告 所 揭示 的 真相 ， 來 進行 後續 的 轉型 正義 工作 。 \n",
            "挖掘 真相 、 彌平 傷痕 、 釐清 責任 。 從此以後 ， 過去 的 歷史 不再 是 台灣 分裂 的 原因 ， 而是 台灣 一起 往前走 的 動力 。  \n",
            "同樣 在 公平正義 的 議題 上 ， 我會 秉持 相同 的 原則 ， 來 面對 原住民 族 的 議題 。 今天 的 就職典禮 ， \n",
            "原住民 族 的 小朋友 在 唱國歌 之前 ， 先唱 了 他們 部落 傳統 的 古調 。 這 象徵 了 ， 我們 不敢 忘記 ，\n",
            "這個 島上 先來後到 的 順序 。   新政府 會 用 道歉 的 態度 ， 來 面對 原住民 族 相關 議題 ， 重建 原民 史觀 ， \n",
            "逐步 推動 自治 ， 復 育 語言 文化 ， 提升 生活 照顧 ， 這 就是 我要 領導 新政府 推動 的 改變 。   \n",
            "接下來 ， 新政府 也 會 積極 推動 司法 改革 。 這是 現階段 台灣 人民 最 關心 的 議題 。 \n",
            "司法 無法 親近 人民 、 不 被 人民 信任 、 司法 無法 有效 打擊犯罪 ， 以及 ， 司法 失去 作為 正義 最後 一道 防線 的 功能 ，\n",
            "是 人民 普遍 的 感受 。   為 了 展現 新政府 的 決心 ， 我們 會 在 今年 十月 召開 司法 國是會議 ， 透過 人民 實際 的 參與 ， \n",
            "讓 社會 力 進來 ， 一起 推動 司法 改革 。 司法 必須 回應 人民 的 需求 ， 不再 只是 法律 人 的 司法 ， 而是 全民 的 司法 。\n",
            "司法 改革 也 不 只是 司法 人 的 家務事 ， 而是 全民 參與 的 改革 。 這 就是 我 對 司法 改革 的 期待 。   \n",
            "第四 、 區域 的 和平 穩定 發展 及 兩岸關係   新政府 要 承擔 的 第四件 事情 ， 是 區域 的 和平 穩定 與 發展 ，\n",
            "以及 妥善處理 兩岸關係 。 過去 三十年 ， 無論是 對 亞洲 或是 全球 ， 都 是 變動 最 劇烈 的 時期 ； 而 全球 及 區域 的 \n",
            "經濟 穩定 和 集體 安全 ， 也 是 各國 政府 越來越 關切 的 課題 。   台灣 在 區域 發展 當中 ， 一直 是 不可或缺 的 關鍵 角色 。 \n",
            "但是 近年來 ， 區域 的 情勢 快速 變動 ， 如果 台灣 不 善用 自己 的 實力 和 籌碼 ， 積極 參與 區域 事務 ， \n",
            "不但 將會 變得 無足輕重 ， 甚至 可能 被 邊緣化 ， 喪失 對於 未來 的 自主權 。   我們 有 危機 ， 但 也 有 轉機 。 \n",
            "台灣 現階段 的 經濟 發展 ， 和 區域 中 許多 國家 高度 關聯 和 互補 。 如果 將 打造 經濟 發展 新 模式 的 努力 ， 透過 和 亞洲 、 乃至 亞太 區域 的 國家 合作 ， 共同 形塑 未來 的 發展 策略 ， 不但 可以 為 區域 的 經濟 創新 、 結構調整 和 永續 發展 ， 做出 積極 的 貢獻 ， 更 可以 和 區域 內 的 成員 ， 建立 緊密 的 「 經濟 共同體 」 意識 。   我們 要 和 其他 國家 共享資源 、 人才 與 市場 ， 擴大 經濟 規模 ， 讓 資源 有效 利用 。 「 新 南向 政策 」 就是 基於 這樣 的 精神 。 我們 會 在 科技 、 文化 與 經貿 等 各 層面 ， 和 區域 成員 廣泛 交流 合作 ， 尤其 是 增進 與 東協 、 印度 的 多元 關係 。 為 此 ， 我們 也 願意 和 對岸 ， 就 共同 參與 區域 發展 的 相關 議題 ， 坦誠 交換意見 ， 尋求 各種 合作 與 協力 的 可能性 。   在 積極 發展 經濟 的 同時 ， 亞太地區 的 安全 情勢 也 變得 越來越 複雜 ， 而 兩岸關係 ， 也 成為 建構 區域 和平 與 集體 安全 的 重要一環 。 這個 建構 的 進程 ， 台灣會 做 一個 「 和平 的 堅定 維護者 」 ， 積極 參與 ， 絕不 缺席 ； 我們 也將 致力 維持 兩岸關係 的 和平 穩定 ； 我們 更會 努力 促成 內部 和解 ， 強化 民主 機制 ， 凝聚 共識 ， 形成 一致 對外 的 立場 。   對話 和 溝通 ， 是 我們 達成 目標 最 重要 的 關鍵 。 台灣 也 要 成為 一個 「 和平 的 積極 溝通 者 」 ， 我們 將和 相關 的 各方 ， 建立 常態 、 緊密 的 溝通 機制 ， 隨時 交換意見 ， 防止 誤判 ， 建立 互信 ， 有效 解決 爭議 。 我們 將 謹守 和平 原則 、 利益 共享 原則 ， 來 處理 相關 的 爭議 。   我 依照 中華民國 憲法 當選 總統 ， 我 有 責任 捍衛 中華民國 的 主權 和 領土 ； 對 於東海 及 南海 問題 ， 我們 主張 應 擱置 爭議 ， 共同開發 。   兩岸 之間 的 對話 與 溝通 ， 我們 也將 努力 維持 現有 的 機制 。 1992 年 兩岸 兩會 秉持 相互 諒解 、 求同存異 的 政治 思維 ， 進行 溝通 協商 ， 達成 若干 的 共同 認知 與 諒解 ， 我 尊重 這個 歷史事實 。 92 年 之後 ， 20 多 年來 雙方 交流 、 協商 所 累積 形成 的 現狀 與 成果 ， 兩岸 都 應該 共同 珍惜 與 維護 ， 並在 這個 既有 的 事實 與 政治 基礎 上 ， 持續 推動 兩岸關係 和平 穩定 發展 ； 新政府 會 依據 中華民國 憲法 、 兩岸人民 關係 條例 及其 他 相關 法律 ， 處理 兩岸 事務 。 兩岸 的 兩個 執政黨 應該 要 放下 歷史 包袱 ， 展開 良性 對話 ， 造福 兩岸人民 。   我 所講 的 既有 政治 基礎 ， 包含 幾個 關鍵 元素 ， 第一 ， 1992 年 兩岸 兩會 會談 的 歷史事實 與 求同存異 的 共同 認知 ， 這是 歷史事實 ； 第二 ， 中華民國 現行 憲政 體制 ； 第三 ， 兩岸 過去 20 多 年來 協商 和 交流 互動 的 成果 ； 第四 ， 台灣 民主 原則 及 普遍 民意 。   第五 、   外 交與 全球性 議題   新政府 要 承擔 的 第五件 事情 ， 是 善 盡 地球 公民 的 責任 ， 在外 交與 全球性 的 議題 上 做出 貢獻 。 讓 台灣 走向世界 ， 也 要 讓 世界 走進 台灣 。   現場 有 許多 來自 各國 的 元首 與 使節團 ， 我要 特別 謝謝 他們 ， 長久以來 一直 幫助 台灣 ， 讓 我們 有 機會 參與 國際 社會 。 未來 ， 我們 會 持續 透過 官方 互動 、 企業 投資 與 民間 合作 各種 方式 ， 分享 台灣 發展 的 經驗 ， 與 友邦 建立 永續 的 夥伴關係 。   台灣 是 全球 公民 社會 的 模範生 ， 民主化 以來 ， 我們 始終 堅持 和平 、 自由 、 民主 及 人權 的 普世 價值 。 我們 會 秉持 這個 精神 ， 加入 全球 議題 的 價值 同盟 。 我們 會 繼續 深化 與 包括 美國 、 日本 、 歐洲 在內 的 友好 民主 國家 的 關係 ， 在 共同 的 價值 基礎 上 ， 推動 全方位 的 合作 。   我們 會 積極 參與 國際 經貿合作 及 規則 制定 ， 堅定 維護 全球 的 經濟秩序 ， 並且 融入 重要 的 區域 經貿 體系 。 我們 也 不會 在 防制 全球 暖化 、 氣候變遷 的 議題 上 缺席 。 我們 將會 在 行政院 設立 專責 的 能源 和 減碳 辦公室 ， 並且 根據 COP21 巴黎 協議 的 規定 ， 定期 檢討 溫室 氣體 的 減量 目標 ， 與 友好 國家 攜手 ， 共同 維護 永續 的 地球 。   同時 ， 新政府 會 支持 並 參與 ， 全球性 新興 議題 的 國際 合作 ， 包括 人 道 救援 、 醫療 援助 、 疾病 的 防治 與 研究 、 反恐 合作 ， 以及 共同 打擊 跨國 犯罪 ， 讓 台灣成 為 國際 社會 不可或缺 的 夥伴 。   結語   1996 年 台灣 第一次 總統 直選 ， 到 今天 剛好 20 年 。 過去 20 年 ， 在 幾任 政府 以及 公民 社會 的 努力 之下 ， 我們 成功 渡過 了 許多 新興 民主 國家 必須 面對 的 難關 。 在 這個 過程 中 ， 我們 曾經 有過 許多 感動 人心 的 時刻 和 故事 ， 不過 ， 正 如同 世界 上 其他 國家 一樣 ， 我們 也 曾經 有過 焦慮 、 不安 、 矛盾 、 與 對立 。   我們 看到 了 社會 的 對立 ， 進步 與 保守 的 對立 ， 環境 與 開發 的 對立 ， 以及 ， 政治 意識 之間 的 對立 。 這些 對立 ， 曾經 激發 出 選舉 時 的 動員 能量 ， 不過 也 因為 這些 對立 ， 我們 的 民主 逐漸 失去 了 解決問題 的 能力 。   民主 是 一個 進程 ， 每 一個 時代 的 政治工作者 ， 都 要 清楚 認識 他 身上 所 肩負 的 責任 。 民主 會 前進 ， 民主 也 有 可能 倒退 。 今天 ， 我 站 在 這裡 ， 就是 要 告訴 大家 ， 倒退 不會 是 我們 的 選項 。 新政府 的 責任 就是 把 台灣 的 民主 推向 下 一個 階段 ： 以前 的 民主 是 選舉 的 輸贏 ， 現在 的 民主 則是 關於 人民 的 幸福 ； 以前 的 民主 是 兩個 價值觀 的 對決 ， 現在 的 民主 則是 不同 價值觀 的 對話 。   打造 一個 沒有 被 意識形態 綁架 的 「 團結 的 民主 」 ， 打造 一個 可以 回應 社會 與 經濟 問題 的 「 有效率 的 民主 」 ， 打造 一個 能夠 實質 照料 人民 的 「 務實 的 民主 」 ， 這 就是 新 時代 的 意義 。   只要 我們 相信 ， 新 時代 就 會 來臨 。 只要 這個 國家 的 主人 ， 有 堅定 的 信念 ， 新 時代 一定 會 在 我們 這 一代人 的 手上 誕生 。   各位 親愛 的 台灣 人民 ， 演講 要 結束 了 ， 改革 要 開始 了 。 從 這 一刻起 ， 這個 國家 的 擔子 交在 新政府 身上 。 我會 讓 大家 看見 這個 國家 的 改變 。   歷史 會 記得 我們 這個 勇敢 的 世代 ， 這個 國家 的 繁榮 、 尊嚴 、 團結 、 自信 和 公義 ， 都 有 我們 努力 的 痕跡 。 歷史 會 記得 我們 的 勇敢 ， 我們 在 2016 年 一起 把 國家 帶 向 新 的 方向 。 這塊 土地 上 的 每 一個 人 ， 都 因為 參與 台灣 的 改變 ， 而 感到 驕傲 。   剛才 表演 節目 中 的 一首 歌曲 當中 ， 有 一句 讓 我 很 感動 的 歌詞 ：   （ 台語 ） 現在 是 彼 一天 ， 勇敢 ㄟ 台灣 人 。   各位 國人 同胞 ， 兩千 三百萬 的 台灣 人民 ， 等待 已經 結束 ， 現在 就是 那 一天 。 今天 ， 明天 ， 未來 的 每 一天 ， 我們 都 要 做 一個 守護 民主 、 守護 自由 、 守護 這個 國家 的 台灣 人 。   謝謝 大家 。\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xsHULr-xt1Ry",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_file = \"./speech\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qiBihzxpakdB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wmkX3FKlt1R0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "text_file = sc.textFile(data_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SzhZOO8B54_d",
        "colab_type": "code",
        "outputId": "95472dbd-d72e-4c4f-d58b-7dfa4e6dc1d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(type(text_file))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pyspark.rdd.RDD'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eeZrFbLMGHHZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "70886488-323d-45b9-c3a6-b04676241565"
      },
      "cell_type": "code",
      "source": [
        "text_file.map(lambda x:1).reduce(lambda x,y:x+y)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "89"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "metadata": {
        "id": "cs1lrKXVGA3D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "79570888-67b6-4b5f-a780-6d7c0a0e5927"
      },
      "cell_type": "code",
      "source": [
        "text_file.count()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "89"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "metadata": {
        "id": "6-yGTmTCGhRF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "line = \"政府 更 應該 要 提出 解決 的 方法\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1sQ0OxQ3GxzD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "a8e99ef0-29bb-4d2d-adb1-7c5b6d9fade1"
      },
      "cell_type": "code",
      "source": [
        "line.split(\" \")"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\xe6\\x94\\xbf\\xe5\\xba\\x9c',\n",
              " '\\xe6\\x9b\\xb4',\n",
              " '\\xe6\\x87\\x89\\xe8\\xa9\\xb2',\n",
              " '\\xe8\\xa6\\x81',\n",
              " '\\xe6\\x8f\\x90\\xe5\\x87\\xba',\n",
              " '\\xe8\\xa7\\xa3\\xe6\\xb1\\xba',\n",
              " '\\xe7\\x9a\\x84',\n",
              " '\\xe6\\x96\\xb9\\xe6\\xb3\\x95']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "metadata": {
        "id": "20-xKOtsL5kH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "850492bb-dc15-4a2b-d037-b34b94b5cd04"
      },
      "cell_type": "code",
      "source": [
        "print(line.split(\" \")[0])"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "政府\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mck2GTk1Goke",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "c0ace92e-4411-4599-9a83-a29303422aa5"
      },
      "cell_type": "code",
      "source": [
        "for i in line.split(\" \"):\n",
        "  print(i)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "政府\n",
            "更\n",
            "應該\n",
            "要\n",
            "提出\n",
            "解決\n",
            "的\n",
            "方法\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ucnc2gbbdKuG",
        "colab_type": "code",
        "outputId": "3774bd00-a35e-4b89-cb9b-054140c5b198",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "text_file.map(lambda line: line.split(\" \"))\\\n",
        "         .map(lambda x:(x,1))\\\n",
        "         .reduceByKey(lambda x,y:x+y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "89"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "metadata": {
        "id": "sDfnyGI9Iby0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OPzvMPHQK076",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e2fc10c5-6297-4e47-e668-0f5cef91dd31"
      },
      "cell_type": "code",
      "source": [
        "text_file.map(lambda line: line.split(\" \")).count()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "89"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "metadata": {
        "id": "JlZxjR_sda8D",
        "colab_type": "code",
        "outputId": "849c2fc7-23a5-48f1-daa0-8c3e73e98caf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "text_file.flatMap(lambda line: line.split(\" \")).count()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3776"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "metadata": {
        "id": "NAIpNQIFt1R2",
        "colab_type": "code",
        "outputId": "86248425-793e-4b9a-e681-a6fc66b95e61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "cell_type": "code",
      "source": [
        "wordcountsRDD = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
        "             .map(lambda word: (word, 1)) \\\n",
        "             .reduceByKey(lambda a, b: a + b)\n",
        "wordcountsRDD.take(10)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(u'', 228),\n",
              " (u'\\u8981', 25),\n",
              " (u'\\u627e\\u51fa', 1),\n",
              " (u'\\u65c5\\u7a0b', 1),\n",
              " (u'\\u8b1d\\u8b1d', 2),\n",
              " (u'\\u5341\\u516d\\u65e5', 1),\n",
              " (u'\\u7b2c\\u4e8c', 2),\n",
              " (u'\\u4ee3\\u5de5', 1),\n",
              " (u'\\u7b2c\\u4e09\\u4ef6', 1),\n",
              " (u'\\u6b4c\\u66f2', 1)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "metadata": {
        "id": "tdcUiiBL58sD",
        "colab_type": "code",
        "outputId": "03330777-5fb2-45d1-edcb-1c8201f814bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "cell_type": "code",
      "source": [
        "for i in wordcountsRDD.take(10):\n",
        "  print i[0], i[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 228\n",
            "要 25\n",
            "找出 1\n",
            "旅程 1\n",
            "謝謝 2\n",
            "十六日 1\n",
            "第二 2\n",
            "代工 1\n",
            "第三件 1\n",
            "歌曲 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wW3nMPLN6kGw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kLRha3Eet1R4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 根據字元符號順序做排序"
      ]
    },
    {
      "metadata": {
        "id": "rngFUG_0t1R6",
        "colab_type": "code",
        "outputId": "85fef0c8-5ad2-4b5a-8036-ff8145a5f741",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "text_file = sc.textFile(data_file)\n",
        "counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
        "             .map(lambda word: (word, 1)) \\\n",
        "             .reduceByKey(lambda a, b: a + b).sortByKey()\n",
        "counts.take(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(u'', 228), (u'1992', 2), (u'1996', 1), (u'20', 4), (u'2016', 1)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "metadata": {
        "id": "62tc3XbHt1R7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 出現頻率做排序"
      ]
    },
    {
      "metadata": {
        "id": "KeUmGmNJt1R8",
        "colab_type": "code",
        "outputId": "9784fce2-1a5a-468a-a18d-2ebe287bfefc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "cell_type": "code",
      "source": [
        "text_file = sc.textFile(data_file)\n",
        "\n",
        "counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
        "             .map(lambda word: (word, 1)) \\\n",
        "             .reduceByKey(lambda a, b: a + b)\\\n",
        "             .sortBy(lambda x: x[1], ascending=False)\n",
        "\n",
        "s = counts.take(20)\n",
        "\n",
        "for i in s:\n",
        "  print i[0],i[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "， 336\n",
            "的 292\n",
            " 228\n",
            "。 159\n",
            "我們 86\n",
            "、 59\n",
            "台灣 39\n",
            "與 37\n",
            "在 33\n",
            "國家 32\n",
            "和 31\n",
            "一個 29\n",
            "是 27\n",
            "新政府 27\n",
            "要 25\n",
            "經濟 25\n",
            "這個 25\n",
            "會 24\n",
            "讓 24\n",
            "也 22\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sQasOGYAt1R_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### map 與 flatMap的差異"
      ]
    },
    {
      "metadata": {
        "id": "TKZtTRUXt1R_",
        "colab_type": "code",
        "outputId": "6db30991-3974-4b1f-8d36-b1edd90b5677",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 526
        }
      },
      "cell_type": "code",
      "source": [
        "text_file = sc.textFile(data_file)\n",
        "counts = text_file.map(lambda line: line.split(\" \"))\n",
        "counts.take(1)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[u'\\u5404\\u4f4d',\n",
              "  u'\\u53cb\\u90a6',\n",
              "  u'\\u7684',\n",
              "  u'\\u5143\\u9996',\n",
              "  u'\\u8207',\n",
              "  u'\\u8cb4\\u8cd3',\n",
              "  u'\\u3001',\n",
              "  u'\\u5404\\u570b',\n",
              "  u'\\u99d0\\u53f0',\n",
              "  u'\\u4f7f\\u7bc0',\n",
              "  u'\\u53ca',\n",
              "  u'\\u4ee3\\u8868',\n",
              "  u'\\u3001',\n",
              "  u'\\u73fe\\u5834',\n",
              "  u'\\u7684',\n",
              "  u'\\u597d',\n",
              "  u'\\u670b\\u53cb',\n",
              "  u'\\uff0c',\n",
              "  u'\\u5168\\u9ad4',\n",
              "  u'\\u570b\\u4eba',\n",
              "  u'\\u540c\\u80de',\n",
              "  u'\\uff0c',\n",
              "  u'\\u5927\\u5bb6',\n",
              "  u'\\u597d',\n",
              "  u'\\u3002',\n",
              "  u'',\n",
              "  u'',\n",
              "  u'']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "metadata": {
        "id": "tDvjL-8Tt1SC",
        "colab_type": "code",
        "outputId": "4f224b32-1234-4181-a18e-5300dd1fc018",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "text_file = sc.textFile(data_file)\n",
        "counts = text_file.flatMap(lambda line: line.split(\" \"))\n",
        "counts.take(1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[u'\\u5404\\u4f4d']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "metadata": {
        "id": "sKvDFz9rKk94",
        "colab_type": "code",
        "outputId": "a8c33534-f8ac-4ddc-cc21-4eab69493381",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "rdd.map(lambda x: x if x>500 else 0).reduce(lambda x,y:x+y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "366120"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "metadata": {
        "id": "QVX1K9rvt1SE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "____\n",
        "____\n",
        "____\n",
        "# pi-estimation "
      ]
    },
    {
      "metadata": {
        "id": "v8gkxPbt-7g0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "b869bf9f-d8d2-4253-8036-c9381d4a313f"
      },
      "cell_type": "code",
      "source": [
        "list(xrange(0,20))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "metadata": {
        "id": "7qJCgsskt1SE",
        "colab_type": "code",
        "outputId": "0f51f78e-c8af-4de2-8594-d9984fb240e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def sample(p):\n",
        "    x, y = random.random(), random.random()\n",
        "    return 1 if x*x + y*y < 1 else 0\n",
        "\n",
        "count = sc.parallelize(xrange(0, 1000000000)).map(sample) \\\n",
        "             .reduce(lambda a, b: a + b)\n",
        "print \"Pi is roughly %f\" % (4.0 * count / 1000000000)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pi is roughly 3.141636\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OAQNfVent1SF",
        "colab_type": "code",
        "outputId": "963d721a-32a2-4467-8d03-cab9650617f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "count = sc.parallelize(xrange(0, 1000000))\\\n",
        "        .map(lambda p: 1 if (random.random()**2 + random.random()**2)<1 else 0) \\\n",
        "        .reduce(lambda a, b: a + b)\n",
        "print \"Pi is roughly %f\" % (4.0 * count / 1000000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pi is roughly 3.146328\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WXI8RPNEt1SH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "___\n",
        "___\n",
        "___\n",
        "\n",
        "# Text Search Example"
      ]
    },
    {
      "metadata": {
        "id": "ym_ec0yDt1SI",
        "colab_type": "code",
        "outputId": "21fe5c90-987d-45d4-9fa8-8248c702a40e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        }
      },
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "f=urllib.urlretrieve(\"https://www.ccel.org/ccel/bible/kjv.txt\",\"bible\")\n",
        "text_file = sc.textFile(data_file)\n",
        "lines = text_file.map(lambda line: line) \n",
        "lines.take(20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[u'     __________________________________________________________________',\n",
              " u'',\n",
              " u'           Title: The King James Version of the Holy Bible',\n",
              " u'      Creator(s): Anonymous',\n",
              " u'          Rights: Public Domain',\n",
              " u'   CCEL Subjects: All; Bible; Old Testament; New Testament; Apocrypha',\n",
              " u'      LC Call no: BS185',\n",
              " u'     LC Subjects:',\n",
              " u'',\n",
              " u'                  The Bible',\n",
              " u'',\n",
              " u'                  Modern texts and versions',\n",
              " u'',\n",
              " u'                  English',\n",
              " u'     __________________________________________________________________',\n",
              " u'',\n",
              " u'Holy Bible',\n",
              " u'',\n",
              " u'                               King James Version',\n",
              " u'     __________________________________________________________________']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "metadata": {
        "id": "W8dnzpSUt1SJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "___\n",
        "___\n",
        "___\n",
        "\n",
        "# Filter Example"
      ]
    },
    {
      "metadata": {
        "id": "aVpHViqwt1SK",
        "colab_type": "code",
        "outputId": "d52e843f-2601-40de-ec7a-79bf906dcf17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        }
      },
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "urllib.urlretrieve(\"https://www.ccel.org/ccel/bible/kjv.txt\",\"bible\")\n",
        "data_file = \"./bible\"\n",
        "text_file = sc.textFile(data_file)\n",
        "lines = text_file.filter(lambda line: 'and' in line) \n",
        "lines.take(20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[u'                  Modern texts and versions',\n",
              " u'   Great and manifold were the blessings, most dread Sovereign, which',\n",
              " u\"   England, when first he sent Your Majesty's Royal Person to rule and\",\n",
              " u'   Occindental Star, Queen Elizabeth, of most happy memory, some thick and',\n",
              " u'   palpable clouds of darkness would so have overshadowed this land, that',\n",
              " u'   men should have been in doubt which way they were to walk, and that it',\n",
              " u'   dispelled those supposed and surmised mists, and gave unto all that',\n",
              " u'   beheld the Government established in Your Highness, and Your hopeful',\n",
              " u'   Seed, by an undoubted Title; and this also accompanied with peace and',\n",
              " u'   tranquillity at home and abroad.',\n",
              " u'   only to the time spent in this transitory world, but directeth and',\n",
              " u'   and to continue it in that state wherein the famous Predecessor of Your',\n",
              " u'   Highness did leave it; nay, to go forward with the confidence and',\n",
              " u'   resolution of a man, in maintaining the truth of Christ, and',\n",
              " u'   propagating it far and near is that which hath so bound and firmly knit',\n",
              " u\"   the hearts of all Your Majesty's loyal and religious people unto You,\",\n",
              " u'   with comfort, and they bless You in their hearts, as that sanctified',\n",
              " u'   every day increaseth and taketh strength, when they observe that the',\n",
              " u'   backward, but is more and more kindled, manifesting itself abroad in',\n",
              " u'   healed,) and every day at home, by religious and learned discourse, by']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "metadata": {
        "id": "_fErZ1SNt1SO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6VL_arA54uEZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qtJTOTZ644l-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dz1EqZwB459t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nAfeeKml5AVL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q9bY61jYJNZN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}